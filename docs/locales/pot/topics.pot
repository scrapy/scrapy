# SOME DESCRIPTIVE TITLE.
# Copyright (C) 2008â€“2020, Scrapy developers
# This file is distributed under the same license as the Scrapy package.
# FIRST AUTHOR <EMAIL@ADDRESS>, YEAR.
#
#, fuzzy
msgid ""
msgstr ""
"Project-Id-Version: Scrapy 2.1\n"
"Report-Msgid-Bugs-To: \n"
"POT-Creation-Date: 2020-05-06 20:10+0200\n"
"PO-Revision-Date: YEAR-MO-DA HO:MI+ZONE\n"
"Last-Translator: FULL NAME <EMAIL@ADDRESS>\n"
"Language-Team: LANGUAGE <LL@li.org>\n"
"MIME-Version: 1.0\n"
"Content-Type: text/plain; charset=UTF-8\n"
"Content-Transfer-Encoding: 8bit\n"

#: ../../topics/api.rst:5
msgid "Core API"
msgstr ""

#: ../../topics/api.rst:9
msgid "This section documents the Scrapy core API, and it's intended for developers of extensions and middlewares."
msgstr ""

#: ../../topics/api.rst:15
msgid "Crawler API"
msgstr ""

#: ../../topics/api.rst:17
msgid "The main entry point to Scrapy API is the :class:`~scrapy.crawler.Crawler` object, passed to extensions through the ``from_crawler`` class method. This object provides access to all Scrapy core components, and it's the only way for extensions to access them and hook their functionality into Scrapy."
msgstr ""

#: ../../topics/api.rst:25
msgid "The Extension Manager is responsible for loading and keeping track of installed extensions and it's configured through the :setting:`EXTENSIONS` setting which contains a dictionary of all available extensions and their order similar to how you :ref:`configure the downloader middlewares <topics-downloader-middleware-setting>`."
msgstr ""

#: ../../topics/api.rst:33
msgid "The Crawler object must be instantiated with a :class:`scrapy.spiders.Spider` subclass and a :class:`scrapy.settings.Settings` object."
msgstr ""

#: ../../topics/api.rst:39
msgid "The settings manager of this crawler."
msgstr ""

#: ../../topics/api.rst:41
msgid "This is used by extensions & middlewares to access the Scrapy settings of this crawler."
msgstr ""

#: ../../topics/api.rst:44
msgid "For an introduction on Scrapy settings see :ref:`topics-settings`."
msgstr ""

#: ../../topics/api.rst:46
msgid "For the API see :class:`~scrapy.settings.Settings` class."
msgstr ""

#: ../../topics/api.rst:50
msgid "The signals manager of this crawler."
msgstr ""

#: ../../topics/api.rst:52
msgid "This is used by extensions & middlewares to hook themselves into Scrapy functionality."
msgstr ""

#: ../../topics/api.rst:55
msgid "For an introduction on signals see :ref:`topics-signals`."
msgstr ""

#: ../../topics/api.rst:57
msgid "For the API see :class:`~scrapy.signalmanager.SignalManager` class."
msgstr ""

#: ../../topics/api.rst:61
msgid "The stats collector of this crawler."
msgstr ""

#: ../../topics/api.rst:63
msgid "This is used from extensions & middlewares to record stats of their behaviour, or access stats collected by other extensions."
msgstr ""

#: ../../topics/api.rst:66
msgid "For an introduction on stats collection see :ref:`topics-stats`."
msgstr ""

#: ../../topics/api.rst:68
msgid "For the API see :class:`~scrapy.statscollectors.StatsCollector` class."
msgstr ""

#: ../../topics/api.rst:72
msgid "The extension manager that keeps track of enabled extensions."
msgstr ""

#: ../../topics/api.rst:74
msgid "Most extensions won't need to access this attribute."
msgstr ""

#: ../../topics/api.rst:76
msgid "For an introduction on extensions and a list of available extensions on Scrapy see :ref:`topics-extensions`."
msgstr ""

#: ../../topics/api.rst:81
msgid "The execution engine, which coordinates the core crawling logic between the scheduler, downloader and spiders."
msgstr ""

#: ../../topics/api.rst:84
msgid "Some extension may want to access the Scrapy engine, to inspect  or modify the downloader and scheduler behaviour, although this is an advanced use and this API is not yet stable."
msgstr ""

#: ../../topics/api.rst:90
msgid "Spider currently being crawled. This is an instance of the spider class provided while constructing the crawler, and it is created after the arguments given in the :meth:`crawl` method."
msgstr ""

#: ../../topics/api.rst:96
msgid "Starts the crawler by instantiating its spider class with the given ``args`` and ``kwargs`` arguments, while setting the execution engine in motion."
msgstr ""

#: ../../topics/api.rst:100
msgid "Returns a deferred that is fired when the crawl is finished."
msgstr ""

#: ../../../scrapy/crawler.py:docstring of scrapy.crawler.Crawler.stop:1
msgid "Starts a graceful stop of the crawler and returns a deferred that is fired when the crawler is stopped."
msgstr ""

#: ../../../scrapy/crawler.py:docstring of scrapy.crawler.CrawlerRunner:1
msgid "This is a convenient helper class that keeps track of, manages and runs crawlers inside an already setup :mod:`~twisted.internet.reactor`."
msgstr ""

#: ../../../scrapy/crawler.py:docstring of scrapy.crawler.CrawlerRunner:4
msgid "The CrawlerRunner object must be instantiated with a :class:`~scrapy.settings.Settings` object."
msgstr ""

#: ../../../scrapy/crawler.py:docstring of scrapy.crawler.CrawlerRunner:7
#: ../../../scrapy/crawler.py:docstring of scrapy.crawler.CrawlerProcess:18
msgid "This class shouldn't be needed (since Scrapy is responsible of using it accordingly) unless writing scripts that manually handle the crawling process. See :ref:`run-from-script` for an example."
msgstr ""

#: ../../../scrapy/crawler.py:docstring of scrapy.crawler.CrawlerRunner.crawl:1
#: ../../../scrapy/crawler.py:docstring of scrapy.crawler.CrawlerProcess.crawl:1
msgid "Run a crawler with the provided arguments."
msgstr ""

#: ../../../scrapy/crawler.py:docstring of scrapy.crawler.CrawlerRunner.crawl:3
#: ../../../scrapy/crawler.py:docstring of scrapy.crawler.CrawlerProcess.crawl:3
msgid "It will call the given Crawler's :meth:`~Crawler.crawl` method, while keeping track of it so it can be stopped later."
msgstr ""

#: ../../../scrapy/crawler.py:docstring of scrapy.crawler.CrawlerRunner.crawl:6
#: ../../../scrapy/crawler.py:docstring of scrapy.crawler.CrawlerProcess.crawl:6
msgid "If ``crawler_or_spidercls`` isn't a :class:`~scrapy.crawler.Crawler` instance, this method will try to create one using this parameter as the spider class given to it."
msgstr ""

#: ../../../scrapy/crawler.py:docstring of scrapy.crawler.CrawlerRunner.crawl:10
#: ../../../scrapy/crawler.py:docstring of scrapy.crawler.CrawlerProcess.crawl:10
msgid "Returns a deferred that is fired when the crawling is finished."
msgstr ""

#: ../../../scrapy/crawler.py:docstring of scrapy.crawler.CrawlerRunner.crawl:0
#: ../../../scrapy/crawler.py:docstring of scrapy.crawler.CrawlerProcess:0
#: ../../../scrapy/crawler.py:docstring of scrapy.crawler.CrawlerProcess.crawl:0
#: ../../../scrapy/crawler.py:docstring of scrapy.crawler.CrawlerProcess.start:0
#: ../../../scrapy/settings/__init__.py:docstring of scrapy.settings.BaseSettings.get:0
#: ../../../scrapy/settings/__init__.py:docstring of scrapy.settings.BaseSettings.getbool:0
#: ../../../scrapy/settings/__init__.py:docstring of scrapy.settings.BaseSettings.getdict:0
#: ../../../scrapy/settings/__init__.py:docstring of scrapy.settings.BaseSettings.getfloat:0
#: ../../../scrapy/settings/__init__.py:docstring of scrapy.settings.BaseSettings.getint:0
#: ../../../scrapy/settings/__init__.py:docstring of scrapy.settings.BaseSettings.getlist:0
#: ../../../scrapy/settings/__init__.py:docstring of scrapy.settings.BaseSettings.getpriority:0
#: ../../../scrapy/settings/__init__.py:docstring of scrapy.settings.BaseSettings.getwithbase:0
#: ../../../scrapy/settings/__init__.py:docstring of scrapy.settings.BaseSettings.set:0
#: ../../../scrapy/settings/__init__.py:docstring of scrapy.settings.BaseSettings.setmodule:0
#: ../../../scrapy/settings/__init__.py:docstring of scrapy.settings.BaseSettings.update:0
#: ../../topics/api.rst:0
#: ../../topics/api.rst:0
#: ../../topics/api.rst:0
#: ../../../scrapy/signalmanager.py:docstring of scrapy.signalmanager.SignalManager.connect:0
#: ../../../scrapy/signalmanager.py:docstring of scrapy.signalmanager.SignalManager.disconnect_all:0
#: ../../topics/contracts.rst:0
#: ../../topics/downloader-middleware.rst:0
#: ../../topics/downloader-middleware.rst:0
#: ../../topics/downloader-middleware.rst:0
#: ../../topics/downloader-middleware.rst:0
#: ../../topics/downloader-middleware.rst:0
#: ../../topics/downloader-middleware.rst:0
#: ../../topics/downloader-middleware.rst:0
#: ../../topics/downloader-middleware.rst:0
#: ../../../scrapy/robotstxt.py:docstring of scrapy.robotstxt.RobotParser.allowed:0
#: ../../../scrapy/robotstxt.py:docstring of scrapy.robotstxt.RobotParser.from_crawler:0
#: ../../topics/email.rst:0
#: ../../topics/email.rst:0
#: ../../topics/email.rst:0
#: ../../topics/exceptions.rst:0
#: ../../topics/exporters.rst:0
#: ../../topics/exporters.rst:0
#: ../../topics/exporters.rst:0
#: ../../topics/exporters.rst:0
#: ../../topics/exporters.rst:0
#: ../../topics/exporters.rst:0
#: ../../topics/exporters.rst:0
#: ../../../scrapy/exporters.py:docstring of scrapy.exporters.MarshalItemExporter:0
#: ../../topics/item-pipeline.rst:0
#: ../../topics/item-pipeline.rst:0
#: ../../topics/item-pipeline.rst:0
#: ../../topics/item-pipeline.rst:0
#: ../../topics/leaks.rst:0
#: ../../topics/link-extractors.rst:0
#: ../../topics/loaders.rst:0
#: ../../topics/loaders.rst:0
#: ../../topics/loaders.rst:0
#: ../../topics/loaders.rst:0
#: ../../topics/loaders.rst:0
#: ../../topics/loaders.rst:0
#: ../../topics/logging.rst:0
#: ../../../scrapy/http/request/__init__.py:docstring of scrapy.http.Request:0
#: ../../topics/request-response.rst:0
#: ../../topics/request-response.rst:0
#: ../../topics/request-response.rst:0
#: ../../../scrapy/http/response/__init__.py:docstring of scrapy.http.Response:0
#: ../../topics/request-response.rst:0
#: ../../topics/signals.rst:0
#: ../../topics/signals.rst:0
#: ../../topics/signals.rst:0
#: ../../topics/signals.rst:0
#: ../../topics/signals.rst:0
#: ../../topics/signals.rst:0
#: ../../topics/signals.rst:0
#: ../../topics/signals.rst:0
#: ../../topics/signals.rst:0
#: ../../topics/signals.rst:0
#: ../../topics/signals.rst:0
#: ../../topics/signals.rst:0
#: ../../topics/signals.rst:0
#: ../../topics/spider-middleware.rst:0
#: ../../topics/spider-middleware.rst:0
#: ../../topics/spider-middleware.rst:0
#: ../../topics/spider-middleware.rst:0
#: ../../topics/spider-middleware.rst:0
#: ../../topics/spiders.rst:0
#: ../../topics/spiders.rst:0
#: ../../topics/telnetconsole.rst:0
msgid "Parameters"
msgstr ""

#: ../../../scrapy/crawler.py:docstring of scrapy.crawler.CrawlerRunner.crawl:12
#: ../../../scrapy/crawler.py:docstring of scrapy.crawler.CrawlerProcess.crawl:12
msgid "already created crawler, or a spider class or spider's name inside the project to create it"
msgstr ""

#: ../../../scrapy/crawler.py:docstring of scrapy.crawler.CrawlerRunner.crawl:17
#: ../../../scrapy/crawler.py:docstring of scrapy.crawler.CrawlerProcess.crawl:17
msgid "arguments to initialize the spider"
msgstr ""

#: ../../../scrapy/crawler.py:docstring of scrapy.crawler.CrawlerRunner.crawl:19
#: ../../../scrapy/crawler.py:docstring of scrapy.crawler.CrawlerProcess.crawl:19
msgid "keyword arguments to initialize the spider"
msgstr ""

#: ../../../scrapy/crawler.py:docstring of scrapy.crawler.CrawlerRunner.crawlers:1
#: ../../../scrapy/crawler.py:docstring of scrapy.crawler.CrawlerProcess.crawlers:1
msgid "Set of :class:`crawlers <scrapy.crawler.Crawler>` started by :meth:`crawl` and managed by this class."
msgstr ""

#: ../../../scrapy/crawler.py:docstring of scrapy.crawler.CrawlerRunner.create_crawler:1
#: ../../../scrapy/crawler.py:docstring of scrapy.crawler.CrawlerProcess.create_crawler:1
msgid "Return a :class:`~scrapy.crawler.Crawler` object."
msgstr ""

#: ../../../scrapy/crawler.py:docstring of scrapy.crawler.CrawlerRunner.create_crawler:3
#: ../../../scrapy/crawler.py:docstring of scrapy.crawler.CrawlerProcess.create_crawler:3
msgid "If ``crawler_or_spidercls`` is a Crawler, it is returned as-is."
msgstr ""

#: ../../../scrapy/crawler.py:docstring of scrapy.crawler.CrawlerRunner.create_crawler:4
#: ../../../scrapy/crawler.py:docstring of scrapy.crawler.CrawlerProcess.create_crawler:4
msgid "If ``crawler_or_spidercls`` is a Spider subclass, a new Crawler is constructed for it."
msgstr ""

#: ../../../scrapy/crawler.py:docstring of scrapy.crawler.CrawlerRunner.create_crawler:6
#: ../../../scrapy/crawler.py:docstring of scrapy.crawler.CrawlerProcess.create_crawler:6
msgid "If ``crawler_or_spidercls`` is a string, this function finds a spider with this name in a Scrapy project (using spider loader), then creates a Crawler instance for it."
msgstr ""

#: ../../../scrapy/crawler.py:docstring of scrapy.crawler.CrawlerRunner.join:1
#: ../../../scrapy/crawler.py:docstring of scrapy.crawler.CrawlerProcess.join:1
msgid "Returns a deferred that is fired when all managed :attr:`crawlers` have completed their executions."
msgstr ""

#: ../../../scrapy/crawler.py:docstring of scrapy.crawler.CrawlerRunner.stop:1
#: ../../../scrapy/crawler.py:docstring of scrapy.crawler.CrawlerProcess.stop:1
msgid "Stops simultaneously all the crawling jobs taking place."
msgstr ""

#: ../../../scrapy/crawler.py:docstring of scrapy.crawler.CrawlerRunner.stop:3
#: ../../../scrapy/crawler.py:docstring of scrapy.crawler.CrawlerProcess.stop:3
msgid "Returns a deferred that is fired when they all have ended."
msgstr ""

#: ../../../scrapy/crawler.py:docstring of scrapy.crawler.CrawlerProcess:1
msgid "Bases: :class:`scrapy.crawler.CrawlerRunner`"
msgstr ""

#: ../../../scrapy/crawler.py:docstring of scrapy.crawler.CrawlerProcess:1
msgid "A class to run multiple scrapy crawlers in a process simultaneously."
msgstr ""

#: ../../../scrapy/crawler.py:docstring of scrapy.crawler.CrawlerProcess:3
msgid "This class extends :class:`~scrapy.crawler.CrawlerRunner` by adding support for starting a :mod:`~twisted.internet.reactor` and handling shutdown signals, like the keyboard interrupt command Ctrl-C. It also configures top-level logging."
msgstr ""

#: ../../../scrapy/crawler.py:docstring of scrapy.crawler.CrawlerProcess:8
msgid "This utility should be a better fit than :class:`~scrapy.crawler.CrawlerRunner` if you aren't running another :mod:`~twisted.internet.reactor` within your application."
msgstr ""

#: ../../../scrapy/crawler.py:docstring of scrapy.crawler.CrawlerProcess:12
msgid "The CrawlerProcess object must be instantiated with a :class:`~scrapy.settings.Settings` object."
msgstr ""

#: ../../../scrapy/crawler.py:docstring of scrapy.crawler.CrawlerProcess:15
#: ../../../scrapy/utils/log.py:docstring of scrapy.utils.log.configure_logging:7
msgid "whether to install root logging handler (default: True)"
msgstr ""

#: ../../../scrapy/crawler.py:docstring of scrapy.crawler.CrawlerProcess.start:1
msgid "This method starts a :mod:`~twisted.internet.reactor`, adjusts its pool size to :setting:`REACTOR_THREADPOOL_MAXSIZE`, and installs a DNS cache based on :setting:`DNSCACHE_ENABLED` and :setting:`DNSCACHE_SIZE`."
msgstr ""

#: ../../../scrapy/crawler.py:docstring of scrapy.crawler.CrawlerProcess.start:5
msgid "If ``stop_after_crawl`` is True, the reactor will be stopped after all crawlers have finished, using :meth:`join`."
msgstr ""

#: ../../../scrapy/crawler.py:docstring of scrapy.crawler.CrawlerProcess.start:8
msgid "stop or not the reactor when all crawlers have finished"
msgstr ""

#: ../../topics/api.rst:115
msgid "Settings API"
msgstr ""

#: ../../topics/api.rst:122
msgid "Dictionary that sets the key name and priority level of the default settings priorities used in Scrapy."
msgstr ""

#: ../../topics/api.rst:125
msgid "Each item defines a settings entry point, giving it a code name for identification and an integer priority. Greater priorities take more precedence over lesser ones when setting and retrieving values in the :class:`~scrapy.settings.Settings` class."
msgstr ""

#: ../../topics/api.rst:142
msgid "For a detailed explanation on each settings sources, see: :ref:`topics-settings`."
msgstr ""

#: ../../../scrapy/settings/__init__.py:docstring of scrapy.settings.get_settings_priority:1
msgid "Small helper function that looks up a given string priority in the :attr:`~scrapy.settings.SETTINGS_PRIORITIES` dictionary and returns its numerical value, or directly returns a given numerical priority."
msgstr ""

#: ../../../scrapy/settings/__init__.py:docstring of scrapy.settings.Settings:1
msgid "Bases: :class:`scrapy.settings.BaseSettings`"
msgstr ""

#: ../../../scrapy/settings/__init__.py:docstring of scrapy.settings.Settings:1
msgid "This object stores Scrapy settings for the configuration of internal components, and can be used for any further customization."
msgstr ""

#: ../../../scrapy/settings/__init__.py:docstring of scrapy.settings.Settings:4
msgid "It is a direct subclass and supports all methods of :class:`~scrapy.settings.BaseSettings`. Additionally, after instantiation of this class, the new object will have the global default settings described on :ref:`topics-settings-ref` already populated."
msgstr ""

#: ../../../scrapy/settings/__init__.py:docstring of scrapy.settings.BaseSettings:1
msgid "Instances of this class behave like dictionaries, but store priorities along with their ``(key, value)`` pairs, and can be frozen (i.e. marked immutable)."
msgstr ""

#: ../../../scrapy/settings/__init__.py:docstring of scrapy.settings.BaseSettings:5
msgid "Key-value entries can be passed on initialization with the ``values`` argument, and they would take the ``priority`` level (unless ``values`` is already an instance of :class:`~scrapy.settings.BaseSettings`, in which case the existing priority levels will be kept).  If the ``priority`` argument is a string, the priority name will be looked up in :attr:`~scrapy.settings.SETTINGS_PRIORITIES`. Otherwise, a specific integer should be provided."
msgstr ""

#: ../../../scrapy/settings/__init__.py:docstring of scrapy.settings.BaseSettings:13
msgid "Once the object is created, new settings can be loaded or updated with the :meth:`~scrapy.settings.BaseSettings.set` method, and can be accessed with the square bracket notation of dictionaries, or with the :meth:`~scrapy.settings.BaseSettings.get` method of the instance and its value conversion variants. When requesting a stored key, the value with the highest priority will be retrieved."
msgstr ""

#: ../../../scrapy/settings/__init__.py:docstring of scrapy.settings.BaseSettings.copy:1
msgid "Make a deep copy of current settings."
msgstr ""

#: ../../../scrapy/settings/__init__.py:docstring of scrapy.settings.BaseSettings.copy:3
msgid "This method returns a new instance of the :class:`Settings` class, populated with the same values and their priorities."
msgstr ""

#: ../../../scrapy/settings/__init__.py:docstring of scrapy.settings.BaseSettings.copy:6
msgid "Modifications to the new object won't be reflected on the original settings."
msgstr ""

#: ../../../scrapy/settings/__init__.py:docstring of scrapy.settings.BaseSettings.copy_to_dict:1
msgid "Make a copy of current settings and convert to a dict."
msgstr ""

#: ../../../scrapy/settings/__init__.py:docstring of scrapy.settings.BaseSettings.copy_to_dict:3
msgid "This method returns a new dict populated with the same values and their priorities as the current settings."
msgstr ""

#: ../../../scrapy/settings/__init__.py:docstring of scrapy.settings.BaseSettings.copy_to_dict:6
msgid "Modifications to the returned dict won't be reflected on the original settings."
msgstr ""

#: ../../../scrapy/settings/__init__.py:docstring of scrapy.settings.BaseSettings.copy_to_dict:9
msgid "This method can be useful for example for printing settings in Scrapy shell."
msgstr ""

#: ../../../scrapy/settings/__init__.py:docstring of scrapy.settings.BaseSettings.freeze:1
msgid "Disable further changes to the current settings."
msgstr ""

#: ../../../scrapy/settings/__init__.py:docstring of scrapy.settings.BaseSettings.freeze:3
msgid "After calling this method, the present state of the settings will become immutable. Trying to change values through the :meth:`~set` method and its variants won't be possible and will be alerted."
msgstr ""

#: ../../../scrapy/settings/__init__.py:docstring of scrapy.settings.BaseSettings.frozencopy:1
msgid "Return an immutable copy of the current settings."
msgstr ""

#: ../../../scrapy/settings/__init__.py:docstring of scrapy.settings.BaseSettings.frozencopy:3
msgid "Alias for a :meth:`~freeze` call in the object returned by :meth:`copy`."
msgstr ""

#: ../../../scrapy/settings/__init__.py:docstring of scrapy.settings.BaseSettings.get:1
msgid "Get a setting value without affecting its original type."
msgstr ""

#: ../../../scrapy/settings/__init__.py:docstring of scrapy.settings.BaseSettings.get:3
#: ../../../scrapy/settings/__init__.py:docstring of scrapy.settings.BaseSettings.getbool:9
#: ../../../scrapy/settings/__init__.py:docstring of scrapy.settings.BaseSettings.getdict:9
#: ../../../scrapy/settings/__init__.py:docstring of scrapy.settings.BaseSettings.getfloat:3
#: ../../../scrapy/settings/__init__.py:docstring of scrapy.settings.BaseSettings.getint:3
#: ../../../scrapy/settings/__init__.py:docstring of scrapy.settings.BaseSettings.getlist:7
#: ../../../scrapy/settings/__init__.py:docstring of scrapy.settings.BaseSettings.getpriority:4
#: ../../../scrapy/settings/__init__.py:docstring of scrapy.settings.BaseSettings.set:7
msgid "the setting name"
msgstr ""

#: ../../../scrapy/settings/__init__.py:docstring of scrapy.settings.BaseSettings.get:6
#: ../../../scrapy/settings/__init__.py:docstring of scrapy.settings.BaseSettings.getbool:12
#: ../../../scrapy/settings/__init__.py:docstring of scrapy.settings.BaseSettings.getdict:12
#: ../../../scrapy/settings/__init__.py:docstring of scrapy.settings.BaseSettings.getfloat:6
#: ../../../scrapy/settings/__init__.py:docstring of scrapy.settings.BaseSettings.getint:6
#: ../../../scrapy/settings/__init__.py:docstring of scrapy.settings.BaseSettings.getlist:10
msgid "the value to return if no setting is found"
msgstr ""

#: ../../../scrapy/settings/__init__.py:docstring of scrapy.settings.BaseSettings.getbool:1
msgid "Get a setting value as a boolean."
msgstr ""

#: ../../../scrapy/settings/__init__.py:docstring of scrapy.settings.BaseSettings.getbool:3
msgid "``1``, ``'1'``, `True`` and ``'True'`` return ``True``, while ``0``, ``'0'``, ``False``, ``'False'`` and ``None`` return ``False``."
msgstr ""

#: ../../../scrapy/settings/__init__.py:docstring of scrapy.settings.BaseSettings.getbool:6
msgid "For example, settings populated through environment variables set to ``'0'`` will return ``False`` when using this method."
msgstr ""

#: ../../../scrapy/settings/__init__.py:docstring of scrapy.settings.BaseSettings.getdict:1
msgid "Get a setting value as a dictionary. If the setting original type is a dictionary, a copy of it will be returned. If it is a string it will be evaluated as a JSON dictionary. In the case that it is a :class:`~scrapy.settings.BaseSettings` instance itself, it will be converted to a dictionary, containing all its current settings values as they would be returned by :meth:`~scrapy.settings.BaseSettings.get`, and losing all information about priority and mutability."
msgstr ""

#: ../../../scrapy/settings/__init__.py:docstring of scrapy.settings.BaseSettings.getfloat:1
msgid "Get a setting value as a float."
msgstr ""

#: ../../../scrapy/settings/__init__.py:docstring of scrapy.settings.BaseSettings.getint:1
msgid "Get a setting value as an int."
msgstr ""

#: ../../../scrapy/settings/__init__.py:docstring of scrapy.settings.BaseSettings.getlist:1
msgid "Get a setting value as a list. If the setting original type is a list, a copy of it will be returned. If it's a string it will be split by \",\"."
msgstr ""

#: ../../../scrapy/settings/__init__.py:docstring of scrapy.settings.BaseSettings.getlist:4
msgid "For example, settings populated through environment variables set to ``'one,two'`` will return a list ['one', 'two'] when using this method."
msgstr ""

#: ../../../scrapy/settings/__init__.py:docstring of scrapy.settings.BaseSettings.getpriority:1
msgid "Return the current numerical priority value of a setting, or ``None`` if the given ``name`` does not exist."
msgstr ""

#: ../../../scrapy/settings/__init__.py:docstring of scrapy.settings.BaseSettings.getwithbase:1
msgid "Get a composition of a dictionary-like setting and its `_BASE` counterpart."
msgstr ""

#: ../../../scrapy/settings/__init__.py:docstring of scrapy.settings.BaseSettings.getwithbase:4
msgid "name of the dictionary-like setting"
msgstr ""

#: ../../../scrapy/settings/__init__.py:docstring of scrapy.settings.BaseSettings.maxpriority:1
msgid "Return the numerical value of the highest priority present throughout all settings, or the numerical value for ``default`` from :attr:`~scrapy.settings.SETTINGS_PRIORITIES` if there are no settings stored."
msgstr ""

#: ../../../scrapy/settings/__init__.py:docstring of scrapy.settings.BaseSettings.set:1
msgid "Store a key/value attribute with a given priority."
msgstr ""

#: ../../../scrapy/settings/__init__.py:docstring of scrapy.settings.BaseSettings.set:3
msgid "Settings should be populated *before* configuring the Crawler object (through the :meth:`~scrapy.crawler.Crawler.configure` method), otherwise they won't have any effect."
msgstr ""

#: ../../../scrapy/settings/__init__.py:docstring of scrapy.settings.BaseSettings.set:10
msgid "the value to associate with the setting"
msgstr ""

#: ../../../scrapy/settings/__init__.py:docstring of scrapy.settings.BaseSettings.set:13
msgid "the priority of the setting. Should be a key of :attr:`~scrapy.settings.SETTINGS_PRIORITIES` or an integer"
msgstr ""

#: ../../../scrapy/settings/__init__.py:docstring of scrapy.settings.BaseSettings.setmodule:1
msgid "Store settings from a module with a given priority."
msgstr ""

#: ../../../scrapy/settings/__init__.py:docstring of scrapy.settings.BaseSettings.setmodule:3
msgid "This is a helper function that calls :meth:`~scrapy.settings.BaseSettings.set` for every globally declared uppercase variable of ``module`` with the provided ``priority``."
msgstr ""

#: ../../../scrapy/settings/__init__.py:docstring of scrapy.settings.BaseSettings.setmodule:7
msgid "the module or the path of the module"
msgstr ""

#: ../../../scrapy/settings/__init__.py:docstring of scrapy.settings.BaseSettings.setmodule:10
#: ../../../scrapy/settings/__init__.py:docstring of scrapy.settings.BaseSettings.update:17
msgid "the priority of the settings. Should be a key of :attr:`~scrapy.settings.SETTINGS_PRIORITIES` or an integer"
msgstr ""

#: ../../../scrapy/settings/__init__.py:docstring of scrapy.settings.BaseSettings.update:1
msgid "Store key/value pairs with a given priority."
msgstr ""

#: ../../../scrapy/settings/__init__.py:docstring of scrapy.settings.BaseSettings.update:3
msgid "This is a helper function that calls :meth:`~scrapy.settings.BaseSettings.set` for every item of ``values`` with the provided ``priority``."
msgstr ""

#: ../../../scrapy/settings/__init__.py:docstring of scrapy.settings.BaseSettings.update:7
msgid "If ``values`` is a string, it is assumed to be JSON-encoded and parsed into a dict with ``json.loads()`` first. If it is a :class:`~scrapy.settings.BaseSettings` instance, the per-key priorities will be used and the ``priority`` parameter ignored. This allows inserting/updating settings with different priorities with a single command."
msgstr ""

#: ../../../scrapy/settings/__init__.py:docstring of scrapy.settings.BaseSettings.update:14
msgid "the settings names and values"
msgstr ""

#: ../../topics/api.rst:157
msgid "SpiderLoader API"
msgstr ""

#: ../../topics/api.rst:164
msgid "This class is in charge of retrieving and handling the spider classes defined across the project."
msgstr ""

#: ../../topics/api.rst:167
msgid "Custom spider loaders can be employed by specifying their path in the :setting:`SPIDER_LOADER_CLASS` project setting. They must fully implement the :class:`scrapy.interfaces.ISpiderLoader` interface to guarantee an errorless execution."
msgstr ""

#: ../../topics/api.rst:174
msgid "This class method is used by Scrapy to create an instance of the class. It's called with the current project settings, and it loads the spiders found recursively in the modules of the :setting:`SPIDER_MODULES` setting."
msgstr ""

#: ../../topics/api.rst:179
msgid "project settings"
msgstr ""

#: ../../topics/api.rst:184
msgid "Get the Spider class with the given name. It'll look into the previously loaded spiders for a spider class with name ``spider_name`` and will raise a KeyError if not found."
msgstr ""

#: ../../topics/api.rst:188
msgid "spider class name"
msgstr ""

#: ../../topics/api.rst:193
msgid "Get the names of the available spiders in the project."
msgstr ""

#: ../../topics/api.rst:197
msgid "List the spiders' names that can handle the given request. Will try to match the request's url against the domains of the spiders."
msgstr ""

#: ../../topics/api.rst:200
msgid "queried request"
msgstr ""

#: ../../topics/api.rst:206
msgid "Signals API"
msgstr ""

#: ../../../scrapy/signalmanager.py:docstring of scrapy.signalmanager.SignalManager.connect:1
msgid "Connect a receiver function to a signal."
msgstr ""

#: ../../../scrapy/signalmanager.py:docstring of scrapy.signalmanager.SignalManager.connect:3
msgid "The signal can be any object, although Scrapy comes with some predefined signals that are documented in the :ref:`topics-signals` section."
msgstr ""

#: ../../../scrapy/signalmanager.py:docstring of scrapy.signalmanager.SignalManager.connect:7
msgid "the function to be connected"
msgstr ""

#: ../../../scrapy/signalmanager.py:docstring of scrapy.signalmanager.SignalManager.connect:10
msgid "the signal to connect to"
msgstr ""

#: ../../../scrapy/signalmanager.py:docstring of scrapy.signalmanager.SignalManager.disconnect:1
msgid "Disconnect a receiver function from a signal. This has the opposite effect of the :meth:`connect` method, and the arguments are the same."
msgstr ""

#: ../../../scrapy/signalmanager.py:docstring of scrapy.signalmanager.SignalManager.disconnect_all:1
msgid "Disconnect all receivers from the given signal."
msgstr ""

#: ../../../scrapy/signalmanager.py:docstring of scrapy.signalmanager.SignalManager.disconnect_all:3
msgid "the signal to disconnect from"
msgstr ""

#: ../../../scrapy/signalmanager.py:docstring of scrapy.signalmanager.SignalManager.send_catch_log:1
msgid "Send a signal, catch exceptions and log them."
msgstr ""

#: ../../../scrapy/signalmanager.py:docstring of scrapy.signalmanager.SignalManager.send_catch_log:3
#: ../../../scrapy/signalmanager.py:docstring of scrapy.signalmanager.SignalManager.send_catch_log_deferred:7
msgid "The keyword arguments are passed to the signal handlers (connected through the :meth:`connect` method)."
msgstr ""

#: ../../../scrapy/signalmanager.py:docstring of scrapy.signalmanager.SignalManager.send_catch_log_deferred:1
msgid "Like :meth:`send_catch_log` but supports returning :class:`~twisted.internet.defer.Deferred` objects from signal handlers."
msgstr ""

#: ../../../scrapy/signalmanager.py:docstring of scrapy.signalmanager.SignalManager.send_catch_log_deferred:4
msgid "Returns a Deferred that gets fired once all signal handlers deferreds were fired. Send a signal, catch exceptions and log them."
msgstr ""

#: ../../topics/api.rst:216
msgid "Stats Collector API"
msgstr ""

#: ../../topics/api.rst:218
msgid "There are several Stats Collectors available under the :mod:`scrapy.statscollectors` module and they all implement the Stats Collector API defined by the :class:`~scrapy.statscollectors.StatsCollector` class (which they all inherit from)."
msgstr ""

#: ../../topics/api.rst:230
msgid "Return the value for the given stats key or default if it doesn't exist."
msgstr ""

#: ../../topics/api.rst:234
msgid "Get all stats from the currently running spider as a dict."
msgstr ""

#: ../../topics/api.rst:238
msgid "Set the given value for the given stats key."
msgstr ""

#: ../../topics/api.rst:242
msgid "Override the current stats with the dict passed in ``stats`` argument."
msgstr ""

#: ../../topics/api.rst:246
msgid "Increment the value of the given stats key, by the given count, assuming the start value given (when it's not set)."
msgstr ""

#: ../../topics/api.rst:251
msgid "Set the given value for the given key only if current value for the same key is lower than value. If there is no current value for the given key, the value is always set."
msgstr ""

#: ../../topics/api.rst:257
msgid "Set the given value for the given key only if current value for the same key is greater than value. If there is no current value for the given key, the value is always set."
msgstr ""

#: ../../topics/api.rst:263
msgid "Clear all stats."
msgstr ""

#: ../../topics/api.rst:265
msgid "The following methods are not part of the stats collection api but instead used when implementing custom stats collectors:"
msgstr ""

#: ../../topics/api.rst:270
msgid "Open the given spider for stats collection."
msgstr ""

#: ../../topics/api.rst:274
msgid "Close the given spider. After this is called, no more specific stats can be accessed or collected."
msgstr ""

#: ../../topics/architecture.rst:5
msgid "Architecture overview"
msgstr ""

#: ../../topics/architecture.rst:7
msgid "This document describes the architecture of Scrapy and how its components interact."
msgstr ""

#: ../../topics/architecture.rst:11
msgid "Overview"
msgstr ""

#: ../../topics/architecture.rst:13
msgid "The following diagram shows an overview of the Scrapy architecture with its components and an outline of the data flow that takes place inside the system (shown by the red arrows). A brief description of the components is included below with links for more detailed information about them. The data flow is also described below."
msgstr ""

#: ../../topics/architecture.rst:22
msgid "Data flow"
msgstr ""

#: ../../topics/architecture.rst:29
msgid "The data flow in Scrapy is controlled by the execution engine, and goes like this:"
msgstr ""

#: ../../topics/architecture.rst:32
msgid "The :ref:`Engine <component-engine>` gets the initial Requests to crawl from the :ref:`Spider <component-spiders>`."
msgstr ""

#: ../../topics/architecture.rst:35
msgid "The :ref:`Engine <component-engine>` schedules the Requests in the :ref:`Scheduler <component-scheduler>` and asks for the next Requests to crawl."
msgstr ""

#: ../../topics/architecture.rst:39
msgid "The :ref:`Scheduler <component-scheduler>` returns the next Requests to the :ref:`Engine <component-engine>`."
msgstr ""

#: ../../topics/architecture.rst:42
msgid "The :ref:`Engine <component-engine>` sends the Requests to the :ref:`Downloader <component-downloader>`, passing through the :ref:`Downloader Middlewares <component-downloader-middleware>` (see :meth:`~scrapy.downloadermiddlewares.DownloaderMiddleware.process_request`)."
msgstr ""

#: ../../topics/architecture.rst:47
msgid "Once the page finishes downloading the :ref:`Downloader <component-downloader>` generates a Response (with that page) and sends it to the Engine, passing through the :ref:`Downloader Middlewares <component-downloader-middleware>` (see :meth:`~scrapy.downloadermiddlewares.DownloaderMiddleware.process_response`)."
msgstr ""

#: ../../topics/architecture.rst:53
msgid "The :ref:`Engine <component-engine>` receives the Response from the :ref:`Downloader <component-downloader>` and sends it to the :ref:`Spider <component-spiders>` for processing, passing through the :ref:`Spider Middleware <component-spider-middleware>` (see :meth:`~scrapy.spidermiddlewares.SpiderMiddleware.process_spider_input`)."
msgstr ""

#: ../../topics/architecture.rst:59
msgid "The :ref:`Spider <component-spiders>` processes the Response and returns scraped items and new Requests (to follow) to the :ref:`Engine <component-engine>`, passing through the :ref:`Spider Middleware <component-spider-middleware>` (see :meth:`~scrapy.spidermiddlewares.SpiderMiddleware.process_spider_output`)."
msgstr ""

#: ../../topics/architecture.rst:65
msgid "The :ref:`Engine <component-engine>` sends processed items to :ref:`Item Pipelines <component-pipelines>`, then send processed Requests to the :ref:`Scheduler <component-scheduler>` and asks for possible next Requests to crawl."
msgstr ""

#: ../../topics/architecture.rst:70
msgid "The process repeats (from step 1) until there are no more requests from the :ref:`Scheduler <component-scheduler>`."
msgstr ""

#: ../../topics/architecture.rst:74
msgid "Components"
msgstr ""

#: ../../topics/architecture.rst:79
msgid "Scrapy Engine"
msgstr ""

#: ../../topics/architecture.rst:81
msgid "The engine is responsible for controlling the data flow between all components of the system, and triggering events when certain actions occur. See the :ref:`Data Flow <data-flow>` section above for more details."
msgstr ""

#: ../../topics/architecture.rst:88
msgid "Scheduler"
msgstr ""

#: ../../topics/architecture.rst:90
msgid "The Scheduler receives requests from the engine and enqueues them for feeding them later (also to the engine) when the engine requests them."
msgstr ""

#: ../../topics/architecture.rst:96
msgid "Downloader"
msgstr ""

#: ../../topics/architecture.rst:98
msgid "The Downloader is responsible for fetching web pages and feeding them to the engine which, in turn, feeds them to the spiders."
msgstr ""

#: ../../topics/architecture.rst:104
#: ../../topics/spiders.rst:5
msgid "Spiders"
msgstr ""

#: ../../topics/architecture.rst:106
msgid "Spiders are custom classes written by Scrapy users to parse responses and extract items (aka scraped items) from them or additional requests to follow. For more information see :ref:`topics-spiders`."
msgstr ""

#: ../../topics/architecture.rst:113
#: ../../topics/item-pipeline.rst:5
msgid "Item Pipeline"
msgstr ""

#: ../../topics/architecture.rst:115
msgid "The Item Pipeline is responsible for processing the items once they have been extracted (or scraped) by the spiders. Typical tasks include cleansing, validation and persistence (like storing the item in a database). For more information see :ref:`topics-item-pipeline`."
msgstr ""

#: ../../topics/architecture.rst:123
#: ../../topics/exceptions.rst:68
msgid "Downloader middlewares"
msgstr ""

#: ../../topics/architecture.rst:125
msgid "Downloader middlewares are specific hooks that sit between the Engine and the Downloader and process requests when they pass from the Engine to the Downloader, and responses that pass from Downloader to the Engine."
msgstr ""

#: ../../topics/architecture.rst:129
msgid "Use a Downloader middleware if you need to do one of the following:"
msgstr ""

#: ../../topics/architecture.rst:131
msgid "process a request just before it is sent to the Downloader (i.e. right before Scrapy sends the request to the website);"
msgstr ""

#: ../../topics/architecture.rst:133
msgid "change received response before passing it to a spider;"
msgstr ""

#: ../../topics/architecture.rst:134
msgid "send a new Request instead of passing received response to a spider;"
msgstr ""

#: ../../topics/architecture.rst:135
msgid "pass response to a spider without fetching a web page;"
msgstr ""

#: ../../topics/architecture.rst:136
msgid "silently drop some requests."
msgstr ""

#: ../../topics/architecture.rst:138
msgid "For more information see :ref:`topics-downloader-middleware`."
msgstr ""

#: ../../topics/architecture.rst:143
#: ../../topics/exceptions.rst:69
msgid "Spider middlewares"
msgstr ""

#: ../../topics/architecture.rst:145
msgid "Spider middlewares are specific hooks that sit between the Engine and the Spiders and are able to process spider input (responses) and output (items and requests)."
msgstr ""

#: ../../topics/architecture.rst:149
msgid "Use a Spider middleware if you need to"
msgstr ""

#: ../../topics/architecture.rst:151
msgid "post-process output of spider callbacks - change/add/remove requests or items;"
msgstr ""

#: ../../topics/architecture.rst:152
msgid "post-process start_requests;"
msgstr ""

#: ../../topics/architecture.rst:153
msgid "handle spider exceptions;"
msgstr ""

#: ../../topics/architecture.rst:154
msgid "call errback instead of callback for some of the requests based on response content."
msgstr ""

#: ../../topics/architecture.rst:157
msgid "For more information see :ref:`topics-spider-middleware`."
msgstr ""

#: ../../topics/architecture.rst:160
msgid "Event-driven networking"
msgstr ""

#: ../../topics/architecture.rst:162
msgid "Scrapy is written with `Twisted`_, a popular event-driven networking framework for Python. Thus, it's implemented using a non-blocking (aka asynchronous) code for concurrency."
msgstr ""

#: ../../topics/architecture.rst:166
msgid "For more information about asynchronous programming and Twisted see these links:"
msgstr ""

#: ../../topics/architecture.rst:169
msgid ":doc:`twisted:core/howto/defer-intro`"
msgstr ""

#: ../../topics/architecture.rst:170
msgid "`Twisted - hello, asynchronous programming`_"
msgstr ""

#: ../../topics/architecture.rst:171
msgid "`Twisted Introduction - Krondo`_"
msgstr ""

#: ../../topics/asyncio.rst:3
msgid "asyncio"
msgstr ""

#: ../../topics/asyncio.rst:7
msgid "Scrapy has partial support :mod:`asyncio`. After you :ref:`install the asyncio reactor <install-asyncio>`, you may use :mod:`asyncio` and :mod:`asyncio`-powered libraries in any :doc:`coroutine <coroutines>`."
msgstr ""

#: ../../topics/asyncio.rst:11
msgid ":mod:`asyncio` support in Scrapy is experimental. Future Scrapy versions may introduce related changes without a deprecation period or warning."
msgstr ""

#: ../../topics/asyncio.rst:18
msgid "Installing the asyncio reactor"
msgstr ""

#: ../../topics/asyncio.rst:20
msgid "To enable :mod:`asyncio` support, set the :setting:`TWISTED_REACTOR` setting to ``'twisted.internet.asyncioreactor.AsyncioSelectorReactor'``."
msgstr ""

#: ../../topics/asyncio.rst:23
msgid "If you are using :class:`~scrapy.crawler.CrawlerRunner`, you also need to install the :class:`~twisted.internet.asyncioreactor.AsyncioSelectorReactor` reactor manually. You can do that using :func:`~scrapy.utils.reactor.install_reactor`::"
msgstr ""

#: ../../topics/autothrottle.rst:5
msgid "AutoThrottle extension"
msgstr ""

#: ../../topics/autothrottle.rst:7
msgid "This is an extension for automatically throttling crawling speed based on load of both the Scrapy server and the website you are crawling."
msgstr ""

#: ../../topics/autothrottle.rst:11
msgid "Design goals"
msgstr ""

#: ../../topics/autothrottle.rst:13
msgid "be nicer to sites instead of using default download delay of zero"
msgstr ""

#: ../../topics/autothrottle.rst:14
msgid "automatically adjust Scrapy to the optimum crawling speed, so the user doesn't have to tune the download delays to find the optimum one. The user only needs to specify the maximum concurrent requests it allows, and the extension does the rest."
msgstr ""

#: ../../topics/autothrottle.rst:22
msgid "How it works"
msgstr ""

#: ../../topics/autothrottle.rst:24
msgid "AutoThrottle extension adjusts download delays dynamically to make spider send :setting:`AUTOTHROTTLE_TARGET_CONCURRENCY` concurrent requests on average to each remote website."
msgstr ""

#: ../../topics/autothrottle.rst:28
msgid "It uses download latency to compute the delays. The main idea is the following: if a server needs ``latency`` seconds to respond, a client should send a request each ``latency/N`` seconds to have ``N`` requests processed in parallel."
msgstr ""

#: ../../topics/autothrottle.rst:33
msgid "Instead of adjusting the delays one can just set a small fixed download delay and impose hard limits on concurrency using :setting:`CONCURRENT_REQUESTS_PER_DOMAIN` or :setting:`CONCURRENT_REQUESTS_PER_IP` options. It will provide a similar effect, but there are some important differences:"
msgstr ""

#: ../../topics/autothrottle.rst:39
msgid "because the download delay is small there will be occasional bursts of requests;"
msgstr ""

#: ../../topics/autothrottle.rst:41
msgid "often non-200 (error) responses can be returned faster than regular responses, so with a small download delay and a hard concurrency limit crawler will be sending requests to server faster when server starts to return errors. But this is an opposite of what crawler should do - in case of errors it makes more sense to slow down: these errors may be caused by the high request rate."
msgstr ""

#: ../../topics/autothrottle.rst:48
msgid "AutoThrottle doesn't have these issues."
msgstr ""

#: ../../topics/autothrottle.rst:51
msgid "Throttling algorithm"
msgstr ""

#: ../../topics/autothrottle.rst:53
msgid "AutoThrottle algorithm adjusts download delays based on the following rules:"
msgstr ""

#: ../../topics/autothrottle.rst:55
msgid "spiders always start with a download delay of :setting:`AUTOTHROTTLE_START_DELAY`;"
msgstr ""

#: ../../topics/autothrottle.rst:57
msgid "when a response is received, the target download delay is calculated as ``latency / N`` where ``latency`` is a latency of the response, and ``N`` is :setting:`AUTOTHROTTLE_TARGET_CONCURRENCY`."
msgstr ""

#: ../../topics/autothrottle.rst:60
msgid "download delay for next requests is set to the average of previous download delay and the target download delay;"
msgstr ""

#: ../../topics/autothrottle.rst:62
msgid "latencies of non-200 responses are not allowed to decrease the delay;"
msgstr ""

#: ../../topics/autothrottle.rst:63
msgid "download delay can't become less than :setting:`DOWNLOAD_DELAY` or greater than :setting:`AUTOTHROTTLE_MAX_DELAY`"
msgstr ""

#: ../../topics/autothrottle.rst:66
msgid "The AutoThrottle extension honours the standard Scrapy settings for concurrency and delay. This means that it will respect :setting:`CONCURRENT_REQUESTS_PER_DOMAIN` and :setting:`CONCURRENT_REQUESTS_PER_IP` options and never set a download delay lower than :setting:`DOWNLOAD_DELAY`."
msgstr ""

#: ../../topics/autothrottle.rst:74
msgid "In Scrapy, the download latency is measured as the time elapsed between establishing the TCP connection and receiving the HTTP headers."
msgstr ""

#: ../../topics/autothrottle.rst:77
msgid "Note that these latencies are very hard to measure accurately in a cooperative multitasking environment because Scrapy may be busy processing a spider callback, for example, and unable to attend downloads. However, these latencies should still give a reasonable estimate of how busy Scrapy (and ultimately, the server) is, and this extension builds on that premise."
msgstr ""

#: ../../topics/autothrottle.rst:84
#: ../../topics/feed-exports.rst:210
#: ../../topics/settings.rst:5
msgid "Settings"
msgstr ""

#: ../../topics/autothrottle.rst:86
msgid "The settings used to control the AutoThrottle extension are:"
msgstr ""

#: ../../topics/autothrottle.rst:88
msgid ":setting:`AUTOTHROTTLE_ENABLED`"
msgstr ""

#: ../../topics/autothrottle.rst:89
msgid ":setting:`AUTOTHROTTLE_START_DELAY`"
msgstr ""

#: ../../topics/autothrottle.rst:90
msgid ":setting:`AUTOTHROTTLE_MAX_DELAY`"
msgstr ""

#: ../../topics/autothrottle.rst:91
msgid ":setting:`AUTOTHROTTLE_TARGET_CONCURRENCY`"
msgstr ""

#: ../../topics/autothrottle.rst:92
msgid ":setting:`AUTOTHROTTLE_DEBUG`"
msgstr ""

#: ../../topics/autothrottle.rst:93
msgid ":setting:`CONCURRENT_REQUESTS_PER_DOMAIN`"
msgstr ""

#: ../../topics/autothrottle.rst:94
msgid ":setting:`CONCURRENT_REQUESTS_PER_IP`"
msgstr ""

#: ../../topics/autothrottle.rst:95
msgid ":setting:`DOWNLOAD_DELAY`"
msgstr ""

#: ../../topics/autothrottle.rst:97
msgid "For more information see :ref:`autothrottle-algorithm`."
msgstr ""

#: ../../topics/autothrottle.rst:102
msgid "AUTOTHROTTLE_ENABLED"
msgstr ""

#: ../../topics/autothrottle.rst:104
#: ../../topics/autothrottle.rst:161
#: ../../topics/downloader-middleware.rst:260
#: ../../topics/downloader-middleware.rst:549
#: ../../topics/downloader-middleware.rst:598
#: ../../topics/downloader-middleware.rst:652
#: ../../topics/downloader-middleware.rst:664
#: ../../topics/downloader-middleware.rst:1179
#: ../../topics/email.rst:168
#: ../../topics/email.rst:177
#: ../../topics/feed-exports.rst:330
#: ../../topics/feed-exports.rst:349
#: ../../topics/settings.rst:352
#: ../../topics/settings.rst:501
#: ../../topics/settings.rst:728
#: ../../topics/settings.rst:930
#: ../../topics/settings.rst:941
#: ../../topics/settings.rst:961
#: ../../topics/settings.rst:1036
#: ../../topics/settings.rst:1138
#: ../../topics/settings.rst:1187
#: ../../topics/settings.rst:1300
#: ../../topics/spider-middleware.rst:285
msgid "Default: ``False``"
msgstr ""

#: ../../topics/autothrottle.rst:106
msgid "Enables the AutoThrottle extension."
msgstr ""

#: ../../topics/autothrottle.rst:111
msgid "AUTOTHROTTLE_START_DELAY"
msgstr ""

#: ../../topics/autothrottle.rst:113
msgid "Default: ``5.0``"
msgstr ""

#: ../../topics/autothrottle.rst:115
msgid "The initial download delay (in seconds)."
msgstr ""

#: ../../topics/autothrottle.rst:120
msgid "AUTOTHROTTLE_MAX_DELAY"
msgstr ""

#: ../../topics/autothrottle.rst:122
#: ../../topics/settings.rst:951
#: ../../topics/settings.rst:1018
msgid "Default: ``60.0``"
msgstr ""

#: ../../topics/autothrottle.rst:124
msgid "The maximum download delay (in seconds) to be set in case of high latencies."
msgstr ""

#: ../../topics/autothrottle.rst:129
msgid "AUTOTHROTTLE_TARGET_CONCURRENCY"
msgstr ""

#: ../../topics/autothrottle.rst:133
msgid "Default: ``1.0``"
msgstr ""

#: ../../topics/autothrottle.rst:135
msgid "Average number of requests Scrapy should be sending in parallel to remote websites."
msgstr ""

#: ../../topics/autothrottle.rst:138
msgid "By default, AutoThrottle adjusts the delay to send a single concurrent request to each of the remote websites. Set this option to a higher value (e.g. ``2.0``) to increase the throughput and the load on remote servers. A lower ``AUTOTHROTTLE_TARGET_CONCURRENCY`` value (e.g. ``0.5``) makes the crawler more conservative and polite."
msgstr ""

#: ../../topics/autothrottle.rst:144
msgid "Note that :setting:`CONCURRENT_REQUESTS_PER_DOMAIN` and :setting:`CONCURRENT_REQUESTS_PER_IP` options are still respected when AutoThrottle extension is enabled. This means that if ``AUTOTHROTTLE_TARGET_CONCURRENCY`` is set to a value higher than :setting:`CONCURRENT_REQUESTS_PER_DOMAIN` or :setting:`CONCURRENT_REQUESTS_PER_IP`, the crawler won't reach this number of concurrent requests."
msgstr ""

#: ../../topics/autothrottle.rst:152
msgid "At every given time point Scrapy can be sending more or less concurrent requests than ``AUTOTHROTTLE_TARGET_CONCURRENCY``; it is a suggested value the crawler tries to approach, not a hard limit."
msgstr ""

#: ../../topics/autothrottle.rst:159
msgid "AUTOTHROTTLE_DEBUG"
msgstr ""

#: ../../topics/autothrottle.rst:163
msgid "Enable AutoThrottle debug mode which will display stats on every response received, so you can see how the throttling parameters are being adjusted in real time."
msgstr ""

#: ../../topics/benchmarking.rst:5
msgid "Benchmarking"
msgstr ""

#: ../../topics/benchmarking.rst:9
msgid "Scrapy comes with a simple benchmarking suite that spawns a local HTTP server and crawls it at the maximum possible speed. The goal of this benchmarking is to get an idea of how Scrapy performs in your hardware, in order to have a common baseline for comparisons. It uses a simple spider that does nothing and just follows links."
msgstr ""

#: ../../topics/benchmarking.rst:15
msgid "To run it use::"
msgstr ""

#: ../../topics/benchmarking.rst:19
msgid "You should see an output like this::"
msgstr ""

#: ../../topics/benchmarking.rst:80
msgid "That tells you that Scrapy is able to crawl about 3000 pages per minute in the hardware where you run it. Note that this is a very simple spider intended to follow links, any custom spider you write will probably do more stuff which results in slower crawl rates. How slower depends on how much your spider does and how well it's written."
msgstr ""

#: ../../topics/benchmarking.rst:86
msgid "In the future, more cases will be added to the benchmarking suite to cover other common scenarios."
msgstr ""

#: ../../topics/broad-crawls.rst:5
msgid "Broad Crawls"
msgstr ""

#: ../../topics/broad-crawls.rst:7
msgid "Scrapy defaults are optimized for crawling specific sites. These sites are often handled by a single Scrapy spider, although this is not necessary or required (for example, there are generic spiders that handle any given site thrown at them)."
msgstr ""

#: ../../topics/broad-crawls.rst:12
msgid "In addition to this \"focused crawl\", there is another common type of crawling which covers a large (potentially unlimited) number of domains, and is only limited by time or other arbitrary constraint, rather than stopping when the domain was crawled to completion or when there are no more requests to perform. These are called \"broad crawls\" and is the typical crawlers employed by search engines."
msgstr ""

#: ../../topics/broad-crawls.rst:19
msgid "These are some common properties often found in broad crawls:"
msgstr ""

#: ../../topics/broad-crawls.rst:21
msgid "they crawl many domains (often, unbounded) instead of a specific set of sites"
msgstr ""

#: ../../topics/broad-crawls.rst:23
msgid "they don't necessarily crawl domains to completion, because it would be impractical (or impossible) to do so, and instead limit the crawl by time or number of pages crawled"
msgstr ""

#: ../../topics/broad-crawls.rst:27
msgid "they are simpler in logic (as opposed to very complex spiders with many extraction rules) because data is often post-processed in a separate stage"
msgstr ""

#: ../../topics/broad-crawls.rst:30
msgid "they crawl many domains concurrently, which allows them to achieve faster crawl speeds by not being limited by any particular site constraint (each site is crawled slowly to respect politeness, but many sites are crawled in parallel)"
msgstr ""

#: ../../topics/broad-crawls.rst:35
msgid "As said above, Scrapy default settings are optimized for focused crawls, not broad crawls. However, due to its asynchronous architecture, Scrapy is very well suited for performing fast broad crawls. This page summarizes some things you need to keep in mind when using Scrapy for doing broad crawls, along with concrete suggestions of Scrapy settings to tune in order to achieve an efficient broad crawl."
msgstr ""

#: ../../topics/broad-crawls.rst:45
msgid "Use the right :setting:`SCHEDULER_PRIORITY_QUEUE`"
msgstr ""

#: ../../topics/broad-crawls.rst:47
msgid "Scrapyâ€™s default scheduler priority queue is ``'scrapy.pqueues.ScrapyPriorityQueue'``. It works best during single-domain crawl. It does not work well with crawling many different domains in parallel"
msgstr ""

#: ../../topics/broad-crawls.rst:51
msgid "To apply the recommended priority queue use::"
msgstr ""

#: ../../topics/broad-crawls.rst:58
msgid "Increase concurrency"
msgstr ""

#: ../../topics/broad-crawls.rst:60
msgid "Concurrency is the number of requests that are processed in parallel. There is a global limit (:setting:`CONCURRENT_REQUESTS`) and an additional limit that can be set either per domain (:setting:`CONCURRENT_REQUESTS_PER_DOMAIN`) or per IP (:setting:`CONCURRENT_REQUESTS_PER_IP`)."
msgstr ""

#: ../../topics/broad-crawls.rst:65
msgid "The scheduler priority queue :ref:`recommended for broad crawls <broad-crawls-scheduler-priority-queue>` does not support :setting:`CONCURRENT_REQUESTS_PER_IP`."
msgstr ""

#: ../../topics/broad-crawls.rst:69
msgid "The default global concurrency limit in Scrapy is not suitable for crawling many different domains in parallel, so you will want to increase it. How much to increase it will depend on how much CPU and memory you crawler will have available."
msgstr ""

#: ../../topics/broad-crawls.rst:74
msgid "A good starting point is ``100``::"
msgstr ""

#: ../../topics/broad-crawls.rst:78
msgid "But the best way to find out is by doing some trials and identifying at what concurrency your Scrapy process gets CPU bounded. For optimum performance, you should pick a concurrency where CPU usage is at 80-90%."
msgstr ""

#: ../../topics/broad-crawls.rst:82
msgid "Increasing concurrency also increases memory usage. If memory usage is a concern, you might need to lower your global concurrency limit accordingly."
msgstr ""

#: ../../topics/broad-crawls.rst:87
msgid "Increase Twisted IO thread pool maximum size"
msgstr ""

#: ../../topics/broad-crawls.rst:89
msgid "Currently Scrapy does DNS resolution in a blocking way with usage of thread pool. With higher concurrency levels the crawling could be slow or even fail hitting DNS resolver timeouts. Possible solution to increase the number of threads handling DNS queries. The DNS queue will be processed faster speeding up establishing of connection and crawling overall."
msgstr ""

#: ../../topics/broad-crawls.rst:95
msgid "To increase maximum thread pool size use::"
msgstr ""

#: ../../topics/broad-crawls.rst:100
msgid "Setup your own DNS"
msgstr ""

#: ../../topics/broad-crawls.rst:102
msgid "If you have multiple crawling processes and single central DNS, it can act like DoS attack on the DNS server resulting to slow down of entire network or even blocking your machines. To avoid this setup your own DNS server with local cache and upstream to some large DNS like OpenDNS or Verizon."
msgstr ""

#: ../../topics/broad-crawls.rst:108
msgid "Reduce log level"
msgstr ""

#: ../../topics/broad-crawls.rst:110
msgid "When doing broad crawls you are often only interested in the crawl rates you get and any errors found. These stats are reported by Scrapy when using the ``INFO`` log level. In order to save CPU (and log storage requirements) you should not use ``DEBUG`` log level when preforming large broad crawls in production. Using ``DEBUG`` level when developing your (broad) crawler may be fine though."
msgstr ""

#: ../../topics/broad-crawls.rst:117
msgid "To set the log level use::"
msgstr ""

#: ../../topics/broad-crawls.rst:122
msgid "Disable cookies"
msgstr ""

#: ../../topics/broad-crawls.rst:124
msgid "Disable cookies unless you *really* need. Cookies are often not needed when doing broad crawls (search engine crawlers ignore them), and they improve performance by saving some CPU cycles and reducing the memory footprint of your Scrapy crawler."
msgstr ""

#: ../../topics/broad-crawls.rst:129
msgid "To disable cookies use::"
msgstr ""

#: ../../topics/broad-crawls.rst:134
msgid "Disable retries"
msgstr ""

#: ../../topics/broad-crawls.rst:136
msgid "Retrying failed HTTP requests can slow down the crawls substantially, specially when sites causes are very slow (or fail) to respond, thus causing a timeout error which gets retried many times, unnecessarily, preventing crawler capacity to be reused for other domains."
msgstr ""

#: ../../topics/broad-crawls.rst:141
msgid "To disable retries use::"
msgstr ""

#: ../../topics/broad-crawls.rst:146
msgid "Reduce download timeout"
msgstr ""

#: ../../topics/broad-crawls.rst:148
msgid "Unless you are crawling from a very slow connection (which shouldn't be the case for broad crawls) reduce the download timeout so that stuck requests are discarded quickly and free up capacity to process the next ones."
msgstr ""

#: ../../topics/broad-crawls.rst:152
msgid "To reduce the download timeout use::"
msgstr ""

#: ../../topics/broad-crawls.rst:157
msgid "Disable redirects"
msgstr ""

#: ../../topics/broad-crawls.rst:159
msgid "Consider disabling redirects, unless you are interested in following them. When doing broad crawls it's common to save redirects and resolve them when revisiting the site at a later crawl. This also help to keep the number of request constant per crawl batch, otherwise redirect loops may cause the crawler to dedicate too many resources on any specific domain."
msgstr ""

#: ../../topics/broad-crawls.rst:165
msgid "To disable redirects use::"
msgstr ""

#: ../../topics/broad-crawls.rst:170
msgid "Enable crawling of \"Ajax Crawlable Pages\""
msgstr ""

#: ../../topics/broad-crawls.rst:172
msgid "Some pages (up to 1%, based on empirical data from year 2013) declare themselves as `ajax crawlable`_. This means they provide plain HTML version of content that is usually available only via AJAX. Pages can indicate it in two ways:"
msgstr ""

#: ../../topics/broad-crawls.rst:177
msgid "by using ``#!`` in URL - this is the default way;"
msgstr ""

#: ../../topics/broad-crawls.rst:178
msgid "by using a special meta tag - this way is used on \"main\", \"index\" website pages."
msgstr ""

#: ../../topics/broad-crawls.rst:181
msgid "Scrapy handles (1) automatically; to handle (2) enable :ref:`AjaxCrawlMiddleware <ajaxcrawl-middleware>`::"
msgstr ""

#: ../../topics/broad-crawls.rst:186
msgid "When doing broad crawls it's common to crawl a lot of \"index\" web pages; AjaxCrawlMiddleware helps to crawl them correctly. It is turned OFF by default because it has some performance overhead, and enabling it for focused crawls doesn't make much sense."
msgstr ""

#: ../../topics/broad-crawls.rst:196
msgid "Crawl in BFO order"
msgstr ""

#: ../../topics/broad-crawls.rst:198
msgid ":ref:`Scrapy crawls in DFO order by default <faq-bfo-dfo>`."
msgstr ""

#: ../../topics/broad-crawls.rst:200
msgid "In broad crawls, however, page crawling tends to be faster than page processing. As a result, unprocessed early requests stay in memory until the final depth is reached, which can significantly increase memory usage."
msgstr ""

#: ../../topics/broad-crawls.rst:204
msgid ":ref:`Crawl in BFO order <faq-bfo-dfo>` instead to save memory."
msgstr ""

#: ../../topics/broad-crawls.rst:208
msgid "Be mindful of memory leaks"
msgstr ""

#: ../../topics/broad-crawls.rst:210
msgid "If your broad crawl shows a high memory usage, in addition to :ref:`crawling in BFO order <broad-crawls-bfo>` and :ref:`lowering concurrency <broad-crawls-concurrency>` you should :ref:`debug your memory leaks <topics-leaks>`."
msgstr ""

#: ../../topics/broad-crawls.rst:217
msgid "Install a specific Twisted reactor"
msgstr ""

#: ../../topics/broad-crawls.rst:219
msgid "If the crawl is exceeding the system's capabilities, you might want to try installing a specific Twisted reactor, via the :setting:`TWISTED_REACTOR` setting."
msgstr ""

#: ../../topics/commands.rst:7
msgid "Command line tool"
msgstr ""

#: ../../topics/commands.rst:11
msgid "Scrapy is controlled through the ``scrapy`` command-line tool, to be referred here as the \"Scrapy tool\" to differentiate it from the sub-commands, which we just call \"commands\" or \"Scrapy commands\"."
msgstr ""

#: ../../topics/commands.rst:15
msgid "The Scrapy tool provides several commands, for multiple purposes, and each one accepts a different set of arguments and options."
msgstr ""

#: ../../topics/commands.rst:18
msgid "(The ``scrapy deploy`` command has been removed in 1.0 in favor of the standalone ``scrapyd-deploy``. See `Deploying your project`_.)"
msgstr ""

#: ../../topics/commands.rst:24
msgid "Configuration settings"
msgstr ""

#: ../../topics/commands.rst:26
msgid "Scrapy will look for configuration parameters in ini-style ``scrapy.cfg`` files in standard locations:"
msgstr ""

#: ../../topics/commands.rst:29
msgid "``/etc/scrapy.cfg`` or ``c:\\scrapy\\scrapy.cfg`` (system-wide),"
msgstr ""

#: ../../topics/commands.rst:30
msgid "``~/.config/scrapy.cfg`` (``$XDG_CONFIG_HOME``) and ``~/.scrapy.cfg`` (``$HOME``) for global (user-wide) settings, and"
msgstr ""

#: ../../topics/commands.rst:32
msgid "``scrapy.cfg`` inside a Scrapy project's root (see next section)."
msgstr ""

#: ../../topics/commands.rst:34
msgid "Settings from these files are merged in the listed order of preference: user-defined values have higher priority than system-wide defaults and project-wide settings will override all others, when defined."
msgstr ""

#: ../../topics/commands.rst:38
msgid "Scrapy also understands, and can be configured through, a number of environment variables. Currently these are:"
msgstr ""

#: ../../topics/commands.rst:41
msgid "``SCRAPY_SETTINGS_MODULE`` (see :ref:`topics-settings-module-envvar`)"
msgstr ""

#: ../../topics/commands.rst:42
msgid "``SCRAPY_PROJECT`` (see :ref:`topics-project-envvar`)"
msgstr ""

#: ../../topics/commands.rst:43
msgid "``SCRAPY_PYTHON_SHELL`` (see :ref:`topics-shell`)"
msgstr ""

#: ../../topics/commands.rst:48
msgid "Default structure of Scrapy projects"
msgstr ""

#: ../../topics/commands.rst:50
msgid "Before delving into the command-line tool and its sub-commands, let's first understand the directory structure of a Scrapy project."
msgstr ""

#: ../../topics/commands.rst:53
msgid "Though it can be modified, all Scrapy projects have the same file structure by default, similar to this::"
msgstr ""

#: ../../topics/commands.rst:69
msgid "The directory where the ``scrapy.cfg`` file resides is known as the *project root directory*. That file contains the name of the python module that defines the project settings. Here is an example:"
msgstr ""

#: ../../topics/commands.rst:81
msgid "Sharing the root directory between projects"
msgstr ""

#: ../../topics/commands.rst:83
msgid "A project root directory, the one that contains the ``scrapy.cfg``, may be shared by multiple Scrapy projects, each with its own settings module."
msgstr ""

#: ../../topics/commands.rst:86
msgid "In that case, you must define one or more aliases for those settings modules under ``[settings]`` in your ``scrapy.cfg`` file:"
msgstr ""

#: ../../topics/commands.rst:96
msgid "By default, the ``scrapy`` command-line tool will use the ``default`` settings. Use the ``SCRAPY_PROJECT`` environment variable to specify a different project for ``scrapy`` to use::"
msgstr ""

#: ../../topics/commands.rst:108
msgid "Using the ``scrapy`` tool"
msgstr ""

#: ../../topics/commands.rst:110
msgid "You can start by running the Scrapy tool with no arguments and it will print some usage help and the available commands::"
msgstr ""

#: ../../topics/commands.rst:123
msgid "The first line will print the currently active project if you're inside a Scrapy project. In this example it was run from outside a project. If run from inside a project it would have printed something like this::"
msgstr ""

#: ../../topics/commands.rst:135
msgid "Creating projects"
msgstr ""

#: ../../topics/commands.rst:137
msgid "The first thing you typically do with the ``scrapy`` tool is create your Scrapy project::"
msgstr ""

#: ../../topics/commands.rst:142
msgid "That will create a Scrapy project under the ``project_dir`` directory. If ``project_dir`` wasn't specified, ``project_dir`` will be the same as ``myproject``."
msgstr ""

#: ../../topics/commands.rst:145
msgid "Next, you go inside the new project directory::"
msgstr ""

#: ../../topics/commands.rst:149
msgid "And you're ready to use the ``scrapy`` command to manage and control your project from there."
msgstr ""

#: ../../topics/commands.rst:153
msgid "Controlling projects"
msgstr ""

#: ../../topics/commands.rst:155
msgid "You use the ``scrapy`` tool from inside your projects to control and manage them."
msgstr ""

#: ../../topics/commands.rst:158
msgid "For example, to create a new spider::"
msgstr ""

#: ../../topics/commands.rst:162
msgid "Some Scrapy commands (like :command:`crawl`) must be run from inside a Scrapy project. See the :ref:`commands reference <topics-commands-ref>` below for more information on which commands must be run from inside projects, and which not."
msgstr ""

#: ../../topics/commands.rst:166
msgid "Also keep in mind that some commands may have slightly different behaviours when running them from inside projects. For example, the fetch command will use spider-overridden behaviours (such as the ``user_agent`` attribute to override the user-agent) if the url being fetched is associated with some specific spider. This is intentional, as the ``fetch`` command is meant to be used to check how spiders are downloading pages."
msgstr ""

#: ../../topics/commands.rst:176
msgid "Available tool commands"
msgstr ""

#: ../../topics/commands.rst:178
msgid "This section contains a list of the available built-in commands with a description and some usage examples. Remember, you can always get more info about each command by running::"
msgstr ""

#: ../../topics/commands.rst:184
msgid "And you can see all available commands with::"
msgstr ""

#: ../../topics/commands.rst:188
msgid "There are two kinds of commands, those that only work from inside a Scrapy project (Project-specific commands) and those that also work without an active Scrapy project (Global commands), though they may behave slightly different when running from inside a project (as they would use the project overridden settings)."
msgstr ""

#: ../../topics/commands.rst:194
msgid "Global commands:"
msgstr ""

#: ../../topics/commands.rst:196
msgid ":command:`startproject`"
msgstr ""

#: ../../topics/commands.rst:197
msgid ":command:`genspider`"
msgstr ""

#: ../../topics/commands.rst:198
msgid ":command:`settings`"
msgstr ""

#: ../../topics/commands.rst:199
msgid ":command:`runspider`"
msgstr ""

#: ../../topics/commands.rst:200
msgid ":command:`shell`"
msgstr ""

#: ../../topics/commands.rst:201
msgid ":command:`fetch`"
msgstr ""

#: ../../topics/commands.rst:202
msgid ":command:`view`"
msgstr ""

#: ../../topics/commands.rst:203
msgid ":command:`version`"
msgstr ""

#: ../../topics/commands.rst:205
msgid "Project-only commands:"
msgstr ""

#: ../../topics/commands.rst:207
msgid ":command:`crawl`"
msgstr ""

#: ../../topics/commands.rst:208
msgid ":command:`check`"
msgstr ""

#: ../../topics/commands.rst:209
msgid ":command:`list`"
msgstr ""

#: ../../topics/commands.rst:210
msgid ":command:`edit`"
msgstr ""

#: ../../topics/commands.rst:211
msgid ":command:`parse`"
msgstr ""

#: ../../topics/commands.rst:212
msgid ":command:`bench`"
msgstr ""

#: ../../topics/commands.rst:217
msgid "startproject"
msgstr ""

#: ../../topics/commands.rst:219
msgid "Syntax: ``scrapy startproject <project_name> [project_dir]``"
msgstr ""

#: ../../topics/commands.rst:220
#: ../../topics/commands.rst:236
#: ../../topics/commands.rst:349
#: ../../topics/commands.rst:393
#: ../../topics/commands.rst:416
#: ../../topics/commands.rst:519
#: ../../topics/commands.rst:539
#: ../../topics/commands.rst:555
#: ../../topics/commands.rst:568
msgid "Requires project: *no*"
msgstr ""

#: ../../topics/commands.rst:222
msgid "Creates a new Scrapy project named ``project_name``, under the ``project_dir`` directory. If ``project_dir`` wasn't specified, ``project_dir`` will be the same as ``project_name``."
msgstr ""

#: ../../topics/commands.rst:226
#: ../../topics/commands.rst:240
#: ../../topics/commands.rst:318
#: ../../topics/commands.rst:339
#: ../../topics/commands.rst:405
#: ../../topics/commands.rst:433
#: ../../topics/commands.rst:496
msgid "Usage example::"
msgstr ""

#: ../../topics/commands.rst:233
msgid "genspider"
msgstr ""

#: ../../topics/commands.rst:235
msgid "Syntax: ``scrapy genspider [-t template] <name> <domain>``"
msgstr ""

#: ../../topics/commands.rst:238
msgid "Create a new spider in the current folder or in the current project's ``spiders`` folder, if called from inside a project. The ``<name>`` parameter is set as the spider's ``name``, while ``<domain>`` is used to generate the ``allowed_domains`` and ``start_urls`` spider's attributes."
msgstr ""

#: ../../topics/commands.rst:255
msgid "This is just a convenience shortcut command for creating spiders based on pre-defined templates, but certainly not the only way to create spiders. You can just create the spider source code files yourself, instead of using this command."
msgstr ""

#: ../../topics/commands.rst:263
msgid "crawl"
msgstr ""

#: ../../topics/commands.rst:265
msgid "Syntax: ``scrapy crawl <spider>``"
msgstr ""

#: ../../topics/commands.rst:266
#: ../../topics/commands.rst:282
#: ../../topics/commands.rst:313
#: ../../topics/commands.rst:330
#: ../../topics/commands.rst:457
msgid "Requires project: *yes*"
msgstr ""

#: ../../topics/commands.rst:268
msgid "Start crawling using a spider."
msgstr ""

#: ../../topics/commands.rst:270
#: ../../topics/commands.rst:288
#: ../../topics/commands.rst:371
msgid "Usage examples::"
msgstr ""

#: ../../topics/commands.rst:279
msgid "check"
msgstr ""

#: ../../topics/commands.rst:281
msgid "Syntax: ``scrapy check [-l] <spider>``"
msgstr ""

#: ../../topics/commands.rst:284
msgid "Run contract checks."
msgstr ""

#: ../../topics/commands.rst:310
msgid "list"
msgstr ""

#: ../../topics/commands.rst:312
msgid "Syntax: ``scrapy list``"
msgstr ""

#: ../../topics/commands.rst:315
msgid "List all available spiders in the current project. The output is one spider per line."
msgstr ""

#: ../../topics/commands.rst:327
msgid "edit"
msgstr ""

#: ../../topics/commands.rst:329
msgid "Syntax: ``scrapy edit <spider>``"
msgstr ""

#: ../../topics/commands.rst:332
msgid "Edit the given spider using the editor defined in the ``EDITOR`` environment variable or (if unset) the :setting:`EDITOR` setting."
msgstr ""

#: ../../topics/commands.rst:335
msgid "This command is provided only as a convenience shortcut for the most common case, the developer is of course free to choose any tool or IDE to write and debug spiders."
msgstr ""

#: ../../topics/commands.rst:346
msgid "fetch"
msgstr ""

#: ../../topics/commands.rst:348
msgid "Syntax: ``scrapy fetch <url>``"
msgstr ""

#: ../../topics/commands.rst:351
msgid "Downloads the given URL using the Scrapy downloader and writes the contents to standard output."
msgstr ""

#: ../../topics/commands.rst:354
msgid "The interesting thing about this command is that it fetches the page how the spider would download it. For example, if the spider has a ``USER_AGENT`` attribute which overrides the User Agent, it will use that one."
msgstr ""

#: ../../topics/commands.rst:358
msgid "So this command can be used to \"see\" how your spider would fetch a certain page."
msgstr ""

#: ../../topics/commands.rst:360
msgid "If used outside a project, no particular per-spider behaviour would be applied and it will just use the default Scrapy downloader settings."
msgstr ""

#: ../../topics/commands.rst:363
#: ../../topics/commands.rst:399
#: ../../topics/commands.rst:423
#: ../../topics/commands.rst:462
msgid "Supported options:"
msgstr ""

#: ../../topics/commands.rst:365
#: ../../topics/commands.rst:401
#: ../../topics/commands.rst:425
#: ../../topics/commands.rst:464
msgid "``--spider=SPIDER``: bypass spider autodetection and force use of specific spider"
msgstr ""

#: ../../topics/commands.rst:367
msgid "``--headers``: print the response's HTTP headers instead of the response's body"
msgstr ""

#: ../../topics/commands.rst:369
#: ../../topics/commands.rst:403
msgid "``--no-redirect``: do not follow HTTP 3xx redirects (default is to follow them)"
msgstr ""

#: ../../topics/commands.rst:390
msgid "view"
msgstr ""

#: ../../topics/commands.rst:392
msgid "Syntax: ``scrapy view <url>``"
msgstr ""

#: ../../topics/commands.rst:395
msgid "Opens the given URL in a browser, as your Scrapy spider would \"see\" it. Sometimes spiders see pages differently from regular users, so this can be used to check what the spider \"sees\" and confirm it's what you expect."
msgstr ""

#: ../../topics/commands.rst:413
msgid "shell"
msgstr ""

#: ../../topics/commands.rst:415
msgid "Syntax: ``scrapy shell [url]``"
msgstr ""

#: ../../topics/commands.rst:418
msgid "Starts the Scrapy shell for the given URL (if given) or empty if no URL is given. Also supports UNIX-style local file paths, either relative with ``./`` or ``../`` prefixes or absolute file paths. See :ref:`topics-shell` for more info."
msgstr ""

#: ../../topics/commands.rst:427
msgid "``-c code``: evaluate the code in the shell, print the result and exit"
msgstr ""

#: ../../topics/commands.rst:429
msgid "``--no-redirect``: do not follow HTTP 3xx redirects (default is to follow them); this only affects the URL you may pass as argument on the command line; once you are inside the shell, ``fetch(url)`` will still follow HTTP redirects by default."
msgstr ""

#: ../../topics/commands.rst:454
msgid "parse"
msgstr ""

#: ../../topics/commands.rst:456
msgid "Syntax: ``scrapy parse <url> [options]``"
msgstr ""

#: ../../topics/commands.rst:459
msgid "Fetches the given URL and parses it with the spider that handles it, using the method passed with the ``--callback`` option, or ``parse`` if not given."
msgstr ""

#: ../../topics/commands.rst:466
msgid "``--a NAME=VALUE``: set spider argument (may be repeated)"
msgstr ""

#: ../../topics/commands.rst:468
msgid "``--callback`` or ``-c``: spider method to use as callback for parsing the response"
msgstr ""

#: ../../topics/commands.rst:471
msgid "``--meta`` or ``-m``: additional request meta that will be passed to the callback request. This must be a valid json string. Example: --meta='{\"foo\" : \"bar\"}'"
msgstr ""

#: ../../topics/commands.rst:474
msgid "``--cbkwargs``: additional keyword arguments that will be passed to the callback. This must be a valid json string. Example: --cbkwargs='{\"foo\" : \"bar\"}'"
msgstr ""

#: ../../topics/commands.rst:477
msgid "``--pipelines``: process items through pipelines"
msgstr ""

#: ../../topics/commands.rst:479
msgid "``--rules`` or ``-r``: use :class:`~scrapy.spiders.CrawlSpider` rules to discover the callback (i.e. spider method) to use for parsing the response"
msgstr ""

#: ../../topics/commands.rst:483
msgid "``--noitems``: don't show scraped items"
msgstr ""

#: ../../topics/commands.rst:485
msgid "``--nolinks``: don't show extracted links"
msgstr ""

#: ../../topics/commands.rst:487
msgid "``--nocolour``: avoid using pygments to colorize the output"
msgstr ""

#: ../../topics/commands.rst:489
msgid "``--depth`` or ``-d``: depth level for which the requests should be followed recursively (default: 1)"
msgstr ""

#: ../../topics/commands.rst:492
msgid "``--verbose`` or ``-v``: display information for each depth level"
msgstr ""

#: ../../topics/commands.rst:516
msgid "settings"
msgstr ""

#: ../../topics/commands.rst:518
msgid "Syntax: ``scrapy settings [options]``"
msgstr ""

#: ../../topics/commands.rst:521
msgid "Get the value of a Scrapy setting."
msgstr ""

#: ../../topics/commands.rst:523
msgid "If used inside a project it'll show the project setting value, otherwise it'll show the default Scrapy value for that setting."
msgstr ""

#: ../../topics/commands.rst:526
#: ../../topics/commands.rst:544
msgid "Example usage::"
msgstr ""

#: ../../topics/commands.rst:536
msgid "runspider"
msgstr ""

#: ../../topics/commands.rst:538
msgid "Syntax: ``scrapy runspider <spider_file.py>``"
msgstr ""

#: ../../topics/commands.rst:541
msgid "Run a spider self-contained in a Python file, without having to create a project."
msgstr ""

#: ../../topics/commands.rst:552
msgid "version"
msgstr ""

#: ../../topics/commands.rst:554
msgid "Syntax: ``scrapy version [-v]``"
msgstr ""

#: ../../topics/commands.rst:557
msgid "Prints the Scrapy version. If used with ``-v`` it also prints Python, Twisted and Platform info, which is useful for bug reports."
msgstr ""

#: ../../topics/commands.rst:563
msgid "bench"
msgstr ""

#: ../../topics/commands.rst:567
msgid "Syntax: ``scrapy bench``"
msgstr ""

#: ../../topics/commands.rst:570
msgid "Run a quick benchmark test. :ref:`benchmarking`."
msgstr ""

#: ../../topics/commands.rst:573
msgid "Custom project commands"
msgstr ""

#: ../../topics/commands.rst:575
msgid "You can also add your custom project commands by using the :setting:`COMMANDS_MODULE` setting. See the Scrapy commands in `scrapy/commands`_ for examples on how to implement your commands."
msgstr ""

#: ../../topics/commands.rst:583
msgid "COMMANDS_MODULE"
msgstr ""

#: ../../topics/commands.rst:585
#: ../../topics/feed-exports.rst:362
msgid "Default: ``''`` (empty string)"
msgstr ""

#: ../../topics/commands.rst:587
msgid "A module to use for looking up custom Scrapy commands. This is used to add custom commands for your Scrapy project."
msgstr ""

#: ../../topics/commands.rst:590
#: ../../topics/downloader-middleware.rst:418
#: ../../topics/loaders.rst:643
#: ../../topics/loaders.rst:656
#: ../../topics/loaders.rst:692
#: ../../topics/loaders.rst:756
#: ../../topics/selectors.rst:538
#: ../../topics/selectors.rst:584
msgid "Example:"
msgstr ""

#: ../../topics/commands.rst:599
msgid "Register commands via setup.py entry points"
msgstr ""

#: ../../topics/commands.rst:601
msgid "This is an experimental feature, use with caution."
msgstr ""

#: ../../topics/commands.rst:603
msgid "You can also add Scrapy commands from an external library by adding a ``scrapy.commands`` section in the entry points of the library ``setup.py`` file."
msgstr ""

#: ../../topics/commands.rst:607
msgid "The following example adds ``my_command`` command:"
msgstr ""

#: ../../topics/contracts.rst:5
msgid "Spiders Contracts"
msgstr ""

#: ../../topics/contracts.rst:9
msgid "Testing spiders can get particularly annoying and while nothing prevents you from writing unit tests the task gets cumbersome quickly. Scrapy offers an integrated way of testing your spiders by the means of contracts."
msgstr ""

#: ../../topics/contracts.rst:13
msgid "This allows you to test each callback of your spider by hardcoding a sample url and check various constraints for how the callback processes the response. Each contract is prefixed with an ``@`` and included in the docstring. See the following example::"
msgstr ""

#: ../../topics/contracts.rst:28
msgid "This callback is tested using three built-in contracts:"
msgstr ""

#: ../../topics/contracts.rst:34
msgid "This contract (``@url``) sets the sample URL used when checking other contract conditions for this spider. This contract is mandatory. All callbacks lacking this contract are ignored when running the checks::"
msgstr ""

#: ../../topics/contracts.rst:42
msgid "This contract (``@cb_kwargs``) sets the :attr:`cb_kwargs <scrapy.http.Request.cb_kwargs>` attribute for the sample request. It must be a valid JSON dictionary. ::"
msgstr ""

#: ../../topics/contracts.rst:50
msgid "This contract (``@returns``) sets lower and upper bounds for the items and requests returned by the spider. The upper bound is optional::"
msgstr ""

#: ../../topics/contracts.rst:57
msgid "This contract (``@scrapes``) checks that all the items returned by the callback have the specified fields::"
msgstr ""

#: ../../topics/contracts.rst:62
msgid "Use the :command:`check` command to run the contract checks."
msgstr ""

#: ../../topics/contracts.rst:65
msgid "Custom Contracts"
msgstr ""

#: ../../topics/contracts.rst:67
msgid "If you find you need more power than the built-in Scrapy contracts you can create and load your own contracts in the project by using the :setting:`SPIDER_CONTRACTS` setting::"
msgstr ""

#: ../../topics/contracts.rst:76
msgid "Each contract must inherit from :class:`~scrapy.contracts.Contract` and can override three methods:"
msgstr ""

#: ../../topics/contracts.rst:83
msgid "callback function to which the contract is associated"
msgstr ""

#: ../../topics/contracts.rst:86
msgid "list of arguments passed into the docstring (whitespace separated)"
msgstr ""

#: ../../topics/contracts.rst:92
msgid "This receives a ``dict`` as an argument containing default arguments for request object. :class:`~scrapy.http.Request` is used by default, but this can be changed with the ``request_cls`` attribute. If multiple contracts in chain have this attribute defined, the last one is used."
msgstr ""

#: ../../topics/contracts.rst:97
msgid "Must return the same or a modified version of it."
msgstr ""

#: ../../topics/contracts.rst:101
msgid "This allows hooking in various checks on the response received from the sample request, before it's being passed to the callback."
msgstr ""

#: ../../topics/contracts.rst:106
msgid "This allows processing the output of the callback. Iterators are converted listified before being passed to this hook."
msgstr ""

#: ../../topics/contracts.rst:109
msgid "Raise :class:`~scrapy.exceptions.ContractFail` from :class:`~scrapy.contracts.Contract.pre_process` or :class:`~scrapy.contracts.Contract.post_process` if expectations are not met:"
msgstr ""

#: ../../../scrapy/exceptions.py:docstring of scrapy.exceptions.ContractFail:1
msgid "Error raised in case of a failing contract"
msgstr ""

#: ../../topics/contracts.rst:115
msgid "Here is a demo contract which checks the presence of a custom header in the response received::"
msgstr ""

#: ../../topics/contracts.rst:136
msgid "Detecting check runs"
msgstr ""

#: ../../topics/contracts.rst:138
msgid "When ``scrapy check`` is running, the ``SCRAPY_CHECK`` environment variable is set to the ``true`` string. You can use :data:`os.environ` to perform any change to your spiders or your settings when ``scrapy check`` is used::"
msgstr ""

#: ../../topics/coroutines.rst:3
msgid "Coroutines"
msgstr ""

#: ../../topics/coroutines.rst:7
msgid "Scrapy has :ref:`partial support <coroutine-support>` for the :ref:`coroutine syntax <async>`."
msgstr ""

#: ../../topics/coroutines.rst:13
msgid "Supported callables"
msgstr ""

#: ../../topics/coroutines.rst:15
msgid "The following callables may be defined as coroutines using ``async def``, and hence use coroutine syntax (e.g. ``await``, ``async for``, ``async with``):"
msgstr ""

#: ../../topics/coroutines.rst:18
msgid ":class:`~scrapy.http.Request` callbacks."
msgstr ""

#: ../../topics/coroutines.rst:20
msgid "The following are known caveats of the current implementation that we aim to address in future versions of Scrapy:"
msgstr ""

#: ../../topics/coroutines.rst:23
msgid "The callback output is not processed until the whole callback finishes."
msgstr ""

#: ../../topics/coroutines.rst:25
msgid "As a side effect, if the callback raises an exception, none of its output is processed."
msgstr ""

#: ../../topics/coroutines.rst:28
msgid "Because `asynchronous generators were introduced in Python 3.6`_, you can only use ``yield`` if you are using Python 3.6 or later."
msgstr ""

#: ../../topics/coroutines.rst:31
msgid "If you need to output multiple items or requests and you are using Python 3.5, return an iterable (e.g. a list) instead."
msgstr ""

#: ../../topics/coroutines.rst:34
msgid "The :meth:`process_item` method of :ref:`item pipelines <topics-item-pipeline>`."
msgstr ""

#: ../../topics/coroutines.rst:37
msgid "The :meth:`~scrapy.downloadermiddlewares.DownloaderMiddleware.process_request`, :meth:`~scrapy.downloadermiddlewares.DownloaderMiddleware.process_response`, and :meth:`~scrapy.downloadermiddlewares.DownloaderMiddleware.process_exception` methods of :ref:`downloader middlewares <topics-downloader-middleware-custom>`."
msgstr ""

#: ../../topics/coroutines.rst:45
msgid ":ref:`Signal handlers that support deferreds <signal-deferred>`."
msgstr ""

#: ../../topics/coroutines.rst:50
msgid "Usage"
msgstr ""

#: ../../topics/coroutines.rst:52
msgid "There are several use cases for coroutines in Scrapy. Code that would return Deferreds when written for previous Scrapy versions, such as downloader middlewares and signal handlers, can be rewritten to be shorter and cleaner::"
msgstr ""

#: ../../topics/coroutines.rst:66
msgid "becomes::"
msgstr ""

#: ../../topics/coroutines.rst:73
msgid "Coroutines may be used to call asynchronous code. This includes other coroutines, functions that return Deferreds and functions that return :term:`awaitable objects <awaitable>` such as :class:`~asyncio.Future`. This means you can use many useful Python libraries providing such code::"
msgstr ""

#: ../../topics/coroutines.rst:91
msgid "Many libraries that use coroutines, such as `aio-libs`_, require the :mod:`asyncio` loop and to use them you need to :doc:`enable asyncio support in Scrapy<asyncio>`."
msgstr ""

#: ../../topics/coroutines.rst:95
msgid "Common use cases for asynchronous code include:"
msgstr ""

#: ../../topics/coroutines.rst:97
msgid "requesting data from websites, databases and other services (in callbacks, pipelines and middlewares);"
msgstr ""

#: ../../topics/coroutines.rst:99
msgid "storing data in databases (in pipelines and middlewares);"
msgstr ""

#: ../../topics/coroutines.rst:100
msgid "delaying the spider initialization until some external event (in the :signal:`spider_opened` handler);"
msgstr ""

#: ../../topics/coroutines.rst:102
msgid "calling asynchronous Scrapy methods like ``ExecutionEngine.download`` (see :ref:`the screenshot pipeline example<ScreenshotPipeline>`)."
msgstr ""

#: ../../topics/debug.rst:5
msgid "Debugging Spiders"
msgstr ""

#: ../../topics/debug.rst:7
msgid "This document explains the most common techniques for debugging spiders. Consider the following Scrapy spider below::"
msgstr ""

#: ../../topics/debug.rst:37
msgid "Basically this is a simple spider which parses two pages of items (the start_urls). Items also have a details page with additional information, so we use the ``cb_kwargs`` functionality of :class:`~scrapy.http.Request` to pass a partially populated item."
msgstr ""

#: ../../topics/debug.rst:44
msgid "Parse Command"
msgstr ""

#: ../../topics/debug.rst:46
msgid "The most basic way of checking the output of your spider is to use the :command:`parse` command. It allows to check the behaviour of different parts of the spider at the method level. It has the advantage of being flexible and simple to use, but does not allow debugging code inside a method."
msgstr ""

#: ../../topics/debug.rst:55
msgid "In order to see the item scraped from a specific url::"
msgstr ""

#: ../../topics/debug.rst:67
msgid "Using the ``--verbose`` or ``-v`` option we can see the status at each depth level::"
msgstr ""

#: ../../topics/debug.rst:87
msgid "Checking items scraped from a single start_url, can also be easily achieved using::"
msgstr ""

#: ../../topics/debug.rst:96
msgid "Scrapy Shell"
msgstr ""

#: ../../topics/debug.rst:98
msgid "While the :command:`parse` command is very useful for checking behaviour of a spider, it is of little help to check what happens inside a callback, besides showing the response received and the output. How to debug the situation when ``parse_details`` sometimes receives no item?"
msgstr ""

#: ../../topics/debug.rst:105
msgid "Fortunately, the :command:`shell` is your bread and butter in this case (see :ref:`topics-shell-inspect-response`)::"
msgstr ""

#: ../../topics/debug.rst:117
msgid "See also: :ref:`topics-shell-inspect-response`."
msgstr ""

#: ../../topics/debug.rst:120
msgid "Open in browser"
msgstr ""

#: ../../topics/debug.rst:122
msgid "Sometimes you just want to see how a certain response looks in a browser, you can use the ``open_in_browser`` function for that. Here is an example of how you would use it::"
msgstr ""

#: ../../topics/debug.rst:132
msgid "``open_in_browser`` will open a browser with the response received by Scrapy at that point, adjusting the `base tag`_ so that images and styles are displayed properly."
msgstr ""

#: ../../topics/debug.rst:137
#: ../../topics/logging.rst:5
msgid "Logging"
msgstr ""

#: ../../topics/debug.rst:139
msgid "Logging is another useful option for getting information about your spider run. Although not as convenient, it comes with the advantage that the logs will be available in all future runs should they be necessary again::"
msgstr ""

#: ../../topics/debug.rst:150
msgid "For more information, check the :ref:`topics-logging` section."
msgstr ""

#: ../../topics/deploy.rst:5
msgid "Deploying Spiders"
msgstr ""

#: ../../topics/deploy.rst:7
msgid "This section describes the different options you have for deploying your Scrapy spiders to run them on a regular basis. Running Scrapy spiders in your local machine is very convenient for the (early) development stage, but not so much when you need to execute long-running spiders or move spiders to run in production continuously. This is where the solutions for deploying Scrapy spiders come in."
msgstr ""

#: ../../topics/deploy.rst:14
msgid "Popular choices for deploying Scrapy spiders are:"
msgstr ""

#: ../../topics/deploy.rst:16
msgid ":ref:`Scrapyd <deploy-scrapyd>` (open source)"
msgstr ""

#: ../../topics/deploy.rst:17
msgid ":ref:`Scrapy Cloud <deploy-scrapy-cloud>` (cloud-based)"
msgstr ""

#: ../../topics/deploy.rst:22
msgid "Deploying to a Scrapyd Server"
msgstr ""

#: ../../topics/deploy.rst:24
msgid "`Scrapyd`_ is an open source application to run Scrapy spiders. It provides a server with HTTP API, capable of running and monitoring Scrapy spiders."
msgstr ""

#: ../../topics/deploy.rst:27
msgid "To deploy spiders to Scrapyd, you can use the scrapyd-deploy tool provided by the `scrapyd-client`_ package. Please refer to the `scrapyd-deploy documentation`_ for more information."
msgstr ""

#: ../../topics/deploy.rst:31
msgid "Scrapyd is maintained by some of the Scrapy developers."
msgstr ""

#: ../../topics/deploy.rst:36
msgid "Deploying to Scrapy Cloud"
msgstr ""

#: ../../topics/deploy.rst:38
msgid "`Scrapy Cloud`_ is a hosted, cloud-based service by `Scrapinghub`_, the company behind Scrapy."
msgstr ""

#: ../../topics/deploy.rst:41
msgid "Scrapy Cloud removes the need to setup and monitor servers and provides a nice UI to manage spiders and review scraped items, logs and stats."
msgstr ""

#: ../../topics/deploy.rst:45
msgid "To deploy spiders to Scrapy Cloud you can use the `shub`_ command line tool. Please refer to the `Scrapy Cloud documentation`_ for more information."
msgstr ""

#: ../../topics/deploy.rst:48
msgid "Scrapy Cloud is compatible with Scrapyd and one can switch between them as needed - the configuration is read from the ``scrapy.cfg`` file just like ``scrapyd-deploy``."
msgstr ""

#: ../../topics/developer-tools.rst:5
msgid "Using your browser's Developer Tools for scraping"
msgstr ""

#: ../../topics/developer-tools.rst:7
msgid "Here is a general guide on how to use your browser's Developer Tools to ease the scraping process. Today almost all browsers come with built in `Developer Tools`_ and although we will use Firefox in this guide, the concepts are applicable to any other browser."
msgstr ""

#: ../../topics/developer-tools.rst:12
msgid "In this guide we'll introduce the basic tools to use from a browser's Developer Tools by scraping `quotes.toscrape.com`_."
msgstr ""

#: ../../topics/developer-tools.rst:18
msgid "Caveats with inspecting the live browser DOM"
msgstr ""

#: ../../topics/developer-tools.rst:20
msgid "Since Developer Tools operate on a live browser DOM, what you'll actually see when inspecting the page source is not the original HTML, but a modified one after applying some browser clean up and executing Javascript code.  Firefox, in particular, is known for adding ``<tbody>`` elements to tables.  Scrapy, on the other hand, does not modify the original page HTML, so you won't be able to extract any data if you use ``<tbody>`` in your XPath expressions."
msgstr ""

#: ../../topics/developer-tools.rst:27
msgid "Therefore, you should keep in mind the following things:"
msgstr ""

#: ../../topics/developer-tools.rst:29
msgid "Disable Javascript while inspecting the DOM looking for XPaths to be used in Scrapy (in the Developer Tools settings click `Disable JavaScript`)"
msgstr ""

#: ../../topics/developer-tools.rst:32
msgid "Never use full XPath paths, use relative and clever ones based on attributes (such as ``id``, ``class``, ``width``, etc) or any identifying features like ``contains(@href, 'image')``."
msgstr ""

#: ../../topics/developer-tools.rst:36
msgid "Never include ``<tbody>`` elements in your XPath expressions unless you really know what you're doing"
msgstr ""

#: ../../topics/developer-tools.rst:42
msgid "Inspecting a website"
msgstr ""

#: ../../topics/developer-tools.rst:44
msgid "By far the most handy feature of the Developer Tools is the `Inspector` feature, which allows you to inspect the underlying HTML code of any webpage. To demonstrate the Inspector, let's look at the `quotes.toscrape.com`_-site."
msgstr ""

#: ../../topics/developer-tools.rst:49
msgid "On the site we have a total of ten quotes from various authors with specific tags, as well as the Top Ten Tags. Let's say we want to extract all the quotes on this page, without any meta-information about authors, tags, etc."
msgstr ""

#: ../../topics/developer-tools.rst:53
msgid "Instead of viewing the whole source code for the page, we can simply right click on a quote and select ``Inspect Element (Q)``, which opens up the `Inspector`. In it you should see something like this:"
msgstr ""

#: ../../topics/developer-tools.rst:62
msgid "The interesting part for us is this:"
msgstr ""

#: ../../topics/developer-tools.rst:72
msgid "If you hover over the first ``div`` directly above the ``span`` tag highlighted in the screenshot, you'll see that the corresponding section of the webpage gets highlighted as well. So now we have a section, but we can't find our quote text anywhere."
msgstr ""

#: ../../topics/developer-tools.rst:77
msgid "The advantage of the `Inspector` is that it automatically expands and collapses sections and tags of a webpage, which greatly improves readability. You can expand and collapse a tag by clicking on the arrow in front of it or by double clicking directly on the tag. If we expand the ``span`` tag with the ``class= \"text\"`` we will see the quote-text we clicked on. The `Inspector` lets you copy XPaths to selected elements. Let's try it out."
msgstr ""

#: ../../topics/developer-tools.rst:84
msgid "First open the Scrapy shell at http://quotes.toscrape.com/ in a terminal:"
msgstr ""

#: ../../topics/developer-tools.rst:90
msgid "Then, back to your web browser, right-click on the ``span`` tag, select ``Copy > XPath`` and paste it in the Scrapy shell like so:"
msgstr ""

#: ../../topics/developer-tools.rst:100
msgid "Adding ``text()`` at the end we are able to extract the first quote with this basic selector. But this XPath is not really that clever. All it does is go down a desired path in the source code starting from ``html``. So let's see if we can refine our XPath a bit:"
msgstr ""

#: ../../topics/developer-tools.rst:105
msgid "If we check the `Inspector` again we'll see that directly beneath our expanded ``div`` tag we have nine identical ``div`` tags, each with the same attributes as our first. If we expand any of them, we'll see the same structure as with our first quote: Two ``span`` tags and one ``div`` tag. We can expand each ``span`` tag with the ``class=\"text\"`` inside our ``div`` tags and see each quote:"
msgstr ""

#: ../../topics/developer-tools.rst:123
msgid "With this knowledge we can refine our XPath: Instead of a path to follow, we'll simply select all ``span`` tags with the ``class=\"text\"`` by using the `has-class-extension`_:"
msgstr ""

#: ../../topics/developer-tools.rst:133
msgid "And with one simple, cleverer XPath we are able to extract all quotes from the page. We could have constructed a loop over our first XPath to increase the number of the last ``div``, but this would have been unnecessarily complex and by simply constructing an XPath with ``has-class(\"text\")`` we were able to extract all quotes in one line."
msgstr ""

#: ../../topics/developer-tools.rst:139
msgid "The `Inspector` has a lot of other helpful features, such as searching in the source code or directly scrolling to an element you selected. Let's demonstrate a use case:"
msgstr ""

#: ../../topics/developer-tools.rst:143
msgid "Say you want to find the ``Next`` button on the page. Type ``Next`` into the search bar on the top right of the `Inspector`. You should get two results. The first is a ``li`` tag with the ``class=\"next\"``, the second the text of an ``a`` tag. Right click on the ``a`` tag and select ``Scroll into View``. If you hover over the tag, you'll see the button highlighted. From here we could easily create a :ref:`Link Extractor <topics-link-extractors>` to follow the pagination. On a simple site such as this, there may not be the need to find an element visually but the ``Scroll into View`` function can be quite useful on complex sites."
msgstr ""

#: ../../topics/developer-tools.rst:153
msgid "Note that the search bar can also be used to search for and test CSS selectors. For example, you could search for ``span.text`` to find all quote texts. Instead of a full text search, this searches for exactly the ``span`` tag with the ``class=\"text\"`` in the page."
msgstr ""

#: ../../topics/developer-tools.rst:161
msgid "The Network-tool"
msgstr ""

#: ../../topics/developer-tools.rst:162
msgid "While scraping you may come across dynamic webpages where some parts of the page are loaded dynamically through multiple requests. While this can be quite tricky, the `Network`-tool in the Developer Tools greatly facilitates this task. To demonstrate the Network-tool, let's take a look at the page `quotes.toscrape.com/scroll`_."
msgstr ""

#: ../../topics/developer-tools.rst:168
msgid "The page is quite similar to the basic `quotes.toscrape.com`_-page, but instead of the above-mentioned ``Next`` button, the page automatically loads new quotes when you scroll to the bottom. We could go ahead and try out different XPaths directly, but instead we'll check another quite useful command from the Scrapy shell:"
msgstr ""

#: ../../topics/developer-tools.rst:182
msgid "A browser window should open with the webpage but with one crucial difference: Instead of the quotes we just see a greenish bar with the word ``Loading...``."
msgstr ""

#: ../../topics/developer-tools.rst:191
msgid "The ``view(response)`` command let's us view the response our shell or later our spider receives from the server. Here we see that some basic template is loaded which includes the title, the login-button and the footer, but the quotes are missing. This tells us that the quotes are being loaded from a different request than ``quotes.toscrape/scroll``."
msgstr ""

#: ../../topics/developer-tools.rst:198
msgid "If you click on the ``Network`` tab, you will probably only see two entries. The first thing we do is enable persistent logs by clicking on ``Persist Logs``. If this option is disabled, the log is automatically cleared each time you navigate to a different page. Enabling this option is a good default, since it gives us control on when to clear the logs."
msgstr ""

#: ../../topics/developer-tools.rst:205
msgid "If we reload the page now, you'll see the log get populated with six new requests."
msgstr ""

#: ../../topics/developer-tools.rst:213
msgid "Here we see every request that has been made when reloading the page and can inspect each request and its response. So let's find out where our quotes are coming from:"
msgstr ""

#: ../../topics/developer-tools.rst:217
msgid "First click on the request with the name ``scroll``. On the right you can now inspect the request. In ``Headers`` you'll find details about the request headers, such as the URL, the method, the IP-address, and so on. We'll ignore the other tabs and click directly on ``Response``."
msgstr ""

#: ../../topics/developer-tools.rst:222
msgid "What you should see in the ``Preview`` pane is the rendered HTML-code, that is exactly what we saw when we called ``view(response)`` in the shell. Accordingly the ``type`` of the request in the log is ``html``. The other requests have types like ``css`` or ``js``, but what interests us is the one request called ``quotes?page=1`` with the type ``json``."
msgstr ""

#: ../../topics/developer-tools.rst:229
msgid "If we click on this request, we see that the request URL is ``http://quotes.toscrape.com/api/quotes?page=1`` and the response is a JSON-object that contains our quotes. We can also right-click on the request and open ``Open in new tab`` to get a better overview."
msgstr ""

#: ../../topics/developer-tools.rst:239
msgid "With this response we can now easily parse the JSON-object and also request each page to get every quote on the site::"
msgstr ""

#: ../../topics/developer-tools.rst:261
msgid "This spider starts at the first page of the quotes-API. With each response, we parse the ``response.text`` and assign it to ``data``. This lets us operate on the JSON-object like on a Python dictionary. We iterate through the ``quotes`` and print out the ``quote[\"text\"]``. If the handy ``has_next`` element is ``true`` (try loading `quotes.toscrape.com/api/quotes?page=10`_ in your browser or a page-number greater than 10), we increment the ``page`` attribute and ``yield`` a new request, inserting the incremented page-number into our ``url``."
msgstr ""

#: ../../topics/developer-tools.rst:273
msgid "In more complex websites, it could be difficult to easily reproduce the requests, as we could need to add ``headers`` or ``cookies`` to make it work. In those cases you can export the requests in `cURL <https://curl.haxx.se/>`_ format, by right-clicking on each of them in the network tool and using the :meth:`~scrapy.http.Request.from_curl()` method to generate an equivalent request::"
msgstr ""

#: ../../topics/developer-tools.rst:291
msgid "Alternatively, if you want to know the arguments needed to recreate that request you can use the :func:`scrapy.utils.curl.curl_to_request_kwargs` function to get a dictionary with the equivalent arguments."
msgstr ""

#: ../../topics/developer-tools.rst:295
#: ../../topics/dynamic-content.rst:107
msgid "Note that to translate a cURL command into a Scrapy request, you may use `curl2scrapy <https://michael-shub.github.io/curl2scrapy/>`_."
msgstr ""

#: ../../topics/developer-tools.rst:298
msgid "As you can see, with a few inspections in the `Network`-tool we were able to easily replicate the dynamic requests of the scrolling functionality of the page. Crawling dynamic pages can be quite daunting and pages can be very complex, but it (mostly) boils down to identifying the correct request and replicating it in your spider."
msgstr ""

#: ../../topics/djangoitem.rst:7
msgid "DjangoItem"
msgstr ""

#: ../../topics/djangoitem.rst:9
msgid "DjangoItem has been moved into a separate project."
msgstr ""

#: ../../topics/djangoitem.rst:11
#: ../../topics/webservice.rst:9
msgid "It is hosted at:"
msgstr ""

#: ../../topics/djangoitem.rst:13
msgid "https://github.com/scrapy-plugins/scrapy-djangoitem"
msgstr ""

#: ../../topics/downloader-middleware.rst:5
msgid "Downloader Middleware"
msgstr ""

#: ../../topics/downloader-middleware.rst:7
msgid "The downloader middleware is a framework of hooks into Scrapy's request/response processing.  It's a light, low-level system for globally altering Scrapy's requests and responses."
msgstr ""

#: ../../topics/downloader-middleware.rst:14
msgid "Activating a downloader middleware"
msgstr ""

#: ../../topics/downloader-middleware.rst:16
msgid "To activate a downloader middleware component, add it to the :setting:`DOWNLOADER_MIDDLEWARES` setting, which is a dict whose keys are the middleware class paths and their values are the middleware orders."
msgstr ""

#: ../../topics/downloader-middleware.rst:20
#: ../../topics/spider-middleware.rst:21
msgid "Here's an example::"
msgstr ""

#: ../../topics/downloader-middleware.rst:26
msgid "The :setting:`DOWNLOADER_MIDDLEWARES` setting is merged with the :setting:`DOWNLOADER_MIDDLEWARES_BASE` setting defined in Scrapy (and not meant to be overridden) and then sorted by order to get the final sorted list of enabled middlewares: the first middleware is the one closer to the engine and the last is the one closer to the downloader. In other words, the :meth:`~scrapy.downloadermiddlewares.DownloaderMiddleware.process_request` method of each middleware will be invoked in increasing middleware order (100, 200, 300, ...) and the :meth:`~scrapy.downloadermiddlewares.DownloaderMiddleware.process_response` method of each middleware will be invoked in decreasing order."
msgstr ""

#: ../../topics/downloader-middleware.rst:36
msgid "To decide which order to assign to your middleware see the :setting:`DOWNLOADER_MIDDLEWARES_BASE` setting and pick a value according to where you want to insert the middleware. The order does matter because each middleware performs a different action and your middleware could depend on some previous (or subsequent) middleware being applied."
msgstr ""

#: ../../topics/downloader-middleware.rst:42
msgid "If you want to disable a built-in middleware (the ones defined in :setting:`DOWNLOADER_MIDDLEWARES_BASE` and enabled by default) you must define it in your project's :setting:`DOWNLOADER_MIDDLEWARES` setting and assign ``None`` as its value.  For example, if you want to disable the user-agent middleware::"
msgstr ""

#: ../../topics/downloader-middleware.rst:52
#: ../../topics/spider-middleware.rst:54
msgid "Finally, keep in mind that some middlewares may need to be enabled through a particular setting. See each middleware documentation for more info."
msgstr ""

#: ../../topics/downloader-middleware.rst:58
msgid "Writing your own downloader middleware"
msgstr ""

#: ../../topics/downloader-middleware.rst:60
msgid "Each downloader middleware is a Python class that defines one or more of the methods defined below."
msgstr ""

#: ../../topics/downloader-middleware.rst:63
#: ../../topics/spider-middleware.rst:65
msgid "The main entry point is the ``from_crawler`` class method, which receives a :class:`~scrapy.crawler.Crawler` instance. The :class:`~scrapy.crawler.Crawler` object gives you access, for example, to the :ref:`settings <topics-settings>`."
msgstr ""

#: ../../topics/downloader-middleware.rst:71
msgid "Any of the downloader middleware methods may also return a deferred."
msgstr ""

#: ../../topics/downloader-middleware.rst:75
msgid "This method is called for each request that goes through the download middleware."
msgstr ""

#: ../../topics/downloader-middleware.rst:78
msgid ":meth:`process_request` should either: return ``None``, return a :class:`~scrapy.http.Response` object, return a :class:`~scrapy.http.Request` object, or raise :exc:`~scrapy.exceptions.IgnoreRequest`."
msgstr ""

#: ../../topics/downloader-middleware.rst:82
msgid "If it returns ``None``, Scrapy will continue processing this request, executing all other middlewares until, finally, the appropriate downloader handler is called the request performed (and its response downloaded)."
msgstr ""

#: ../../topics/downloader-middleware.rst:86
msgid "If it returns a :class:`~scrapy.http.Response` object, Scrapy won't bother calling *any* other :meth:`process_request` or :meth:`process_exception` methods, or the appropriate download function; it'll return that response. The :meth:`process_response` methods of installed middleware is always called on every response."
msgstr ""

#: ../../topics/downloader-middleware.rst:91
msgid "If it returns a :class:`~scrapy.http.Request` object, Scrapy will stop calling process_request methods and reschedule the returned request. Once the newly returned request is performed, the appropriate middleware chain will be called on the downloaded response."
msgstr ""

#: ../../topics/downloader-middleware.rst:96
msgid "If it raises an :exc:`~scrapy.exceptions.IgnoreRequest` exception, the :meth:`process_exception` methods of installed downloader middleware will be called. If none of them handle the exception, the errback function of the request (``Request.errback``) is called. If no code handles the raised exception, it is ignored and not logged (unlike other exceptions)."
msgstr ""

#: ../../topics/downloader-middleware.rst:102
msgid "the request being processed"
msgstr ""

#: ../../topics/downloader-middleware.rst:105
#: ../../topics/downloader-middleware.rst:163
msgid "the spider for which this request is intended"
msgstr ""

#: ../../topics/downloader-middleware.rst:110
msgid ":meth:`process_response` should either: return a :class:`~scrapy.http.Response` object, return a :class:`~scrapy.http.Request` object or raise a :exc:`~scrapy.exceptions.IgnoreRequest` exception."
msgstr ""

#: ../../topics/downloader-middleware.rst:114
msgid "If it returns a :class:`~scrapy.http.Response` (it could be the same given response, or a brand-new one), that response will continue to be processed with the :meth:`process_response` of the next middleware in the chain."
msgstr ""

#: ../../topics/downloader-middleware.rst:118
msgid "If it returns a :class:`~scrapy.http.Request` object, the middleware chain is halted and the returned request is rescheduled to be downloaded in the future. This is the same behavior as if a request is returned from :meth:`process_request`."
msgstr ""

#: ../../topics/downloader-middleware.rst:122
msgid "If it raises an :exc:`~scrapy.exceptions.IgnoreRequest` exception, the errback function of the request (``Request.errback``) is called. If no code handles the raised exception, it is ignored and not logged (unlike other exceptions)."
msgstr ""

#: ../../topics/downloader-middleware.rst:126
msgid "the request that originated the response"
msgstr ""

#: ../../topics/downloader-middleware.rst:129
#: ../../topics/spider-middleware.rst:92
msgid "the response being processed"
msgstr ""

#: ../../topics/downloader-middleware.rst:132
#: ../../topics/spider-middleware.rst:95
msgid "the spider for which this response is intended"
msgstr ""

#: ../../topics/downloader-middleware.rst:137
msgid "Scrapy calls :meth:`process_exception` when a download handler or a :meth:`process_request` (from a downloader middleware) raises an exception (including an :exc:`~scrapy.exceptions.IgnoreRequest` exception)"
msgstr ""

#: ../../topics/downloader-middleware.rst:141
msgid ":meth:`process_exception` should return: either ``None``, a :class:`~scrapy.http.Response` object, or a :class:`~scrapy.http.Request` object."
msgstr ""

#: ../../topics/downloader-middleware.rst:144
msgid "If it returns ``None``, Scrapy will continue processing this exception, executing any other :meth:`process_exception` methods of installed middleware, until no middleware is left and the default exception handling kicks in."
msgstr ""

#: ../../topics/downloader-middleware.rst:148
msgid "If it returns a :class:`~scrapy.http.Response` object, the :meth:`process_response` method chain of installed middleware is started, and Scrapy won't bother calling any other :meth:`process_exception` methods of middleware."
msgstr ""

#: ../../topics/downloader-middleware.rst:152
msgid "If it returns a :class:`~scrapy.http.Request` object, the returned request is rescheduled to be downloaded in the future. This stops the execution of :meth:`process_exception` methods of the middleware the same as returning a response would."
msgstr ""

#: ../../topics/downloader-middleware.rst:157
msgid "the request that generated the exception"
msgstr ""

#: ../../topics/downloader-middleware.rst:160
msgid "the raised exception"
msgstr ""

#: ../../topics/downloader-middleware.rst:168
#: ../../topics/spider-middleware.rst:177
msgid "If present, this classmethod is called to create a middleware instance from a :class:`~scrapy.crawler.Crawler`. It must return a new instance of the middleware. Crawler object provides access to all Scrapy core components like settings and signals; it is a way for middleware to access them and hook its functionality into Scrapy."
msgstr ""

#: ../../topics/downloader-middleware.rst:174
#: ../../topics/spider-middleware.rst:183
msgid "crawler that uses this middleware"
msgstr ""

#: ../../topics/downloader-middleware.rst:180
msgid "Built-in downloader middleware reference"
msgstr ""

#: ../../topics/downloader-middleware.rst:182
msgid "This page describes all downloader middleware components that come with Scrapy. For information on how to use them and how to write your own downloader middleware, see the :ref:`downloader middleware usage guide <topics-downloader-middleware>`."
msgstr ""

#: ../../topics/downloader-middleware.rst:187
msgid "For a list of the components enabled by default (and their orders) see the :setting:`DOWNLOADER_MIDDLEWARES_BASE` setting."
msgstr ""

#: ../../topics/downloader-middleware.rst:193
msgid "CookiesMiddleware"
msgstr ""

#: ../../topics/downloader-middleware.rst:200
msgid "This middleware enables working with sites that require cookies, such as those that use sessions. It keeps track of cookies sent by web servers, and sends them back on subsequent requests (from that spider), just like web browsers do."
msgstr ""

#: ../../topics/downloader-middleware.rst:205
msgid "The following settings can be used to configure the cookie middleware:"
msgstr ""

#: ../../topics/downloader-middleware.rst:207
msgid ":setting:`COOKIES_ENABLED`"
msgstr ""

#: ../../topics/downloader-middleware.rst:208
msgid ":setting:`COOKIES_DEBUG`"
msgstr ""

#: ../../topics/downloader-middleware.rst:213
msgid "Multiple cookie sessions per spider"
msgstr ""

#: ../../topics/downloader-middleware.rst:217
msgid "There is support for keeping multiple cookie sessions per spider by using the :reqmeta:`cookiejar` Request meta key. By default it uses a single cookie jar (session), but you can pass an identifier to use different ones."
msgstr ""

#: ../../topics/downloader-middleware.rst:221
#: ../../topics/exceptions.rst:36
#: ../../topics/items.rst:210
#: ../../topics/media-pipeline.rst:332
#: ../../topics/media-pipeline.rst:372
#: ../../topics/spiders.rst:661
#: ../../topics/spiders.rst:685
#: ../../topics/spiders.rst:703
msgid "For example::"
msgstr ""

#: ../../topics/downloader-middleware.rst:227
msgid "Keep in mind that the :reqmeta:`cookiejar` meta key is not \"sticky\". You need to keep passing it along on subsequent requests. For example::"
msgstr ""

#: ../../topics/downloader-middleware.rst:239
msgid "COOKIES_ENABLED"
msgstr ""

#: ../../topics/downloader-middleware.rst:241
#: ../../topics/downloader-middleware.rst:722
#: ../../topics/downloader-middleware.rst:817
#: ../../topics/downloader-middleware.rst:860
#: ../../topics/downloader-middleware.rst:924
#: ../../topics/downloader-middleware.rst:1193
#: ../../topics/settings.rst:364
#: ../../topics/settings.rst:555
#: ../../topics/settings.rst:679
#: ../../topics/settings.rst:794
#: ../../topics/settings.rst:861
#: ../../topics/settings.rst:985
#: ../../topics/settings.rst:1078
#: ../../topics/settings.rst:1373
#: ../../topics/settings.rst:1395
#: ../../topics/spider-middleware.rst:347
msgid "Default: ``True``"
msgstr ""

#: ../../topics/downloader-middleware.rst:243
msgid "Whether to enable the cookies middleware. If disabled, no cookies will be sent to web servers."
msgstr ""

#: ../../topics/downloader-middleware.rst:246
msgid "Notice that despite the value of :setting:`COOKIES_ENABLED` setting if ``Request.``:reqmeta:`meta['dont_merge_cookies'] <dont_merge_cookies>` evaluates to ``True`` the request cookies will **not** be sent to the web server and received cookies in :class:`~scrapy.http.Response` will **not** be merged with the existing cookies."
msgstr ""

#: ../../topics/downloader-middleware.rst:252
msgid "For more detailed information see the ``cookies`` parameter in :class:`~scrapy.http.Request`."
msgstr ""

#: ../../topics/downloader-middleware.rst:258
msgid "COOKIES_DEBUG"
msgstr ""

#: ../../topics/downloader-middleware.rst:262
msgid "If enabled, Scrapy will log all cookies sent in requests (i.e. ``Cookie`` header) and all cookies received in responses (i.e. ``Set-Cookie`` header)."
msgstr ""

#: ../../topics/downloader-middleware.rst:265
msgid "Here's an example of a log with :setting:`COOKIES_DEBUG` enabled::"
msgstr ""

#: ../../topics/downloader-middleware.rst:279
msgid "DefaultHeadersMiddleware"
msgstr ""

#: ../../topics/downloader-middleware.rst:286
msgid "This middleware sets all default requests headers specified in the :setting:`DEFAULT_REQUEST_HEADERS` setting."
msgstr ""

#: ../../topics/downloader-middleware.rst:290
msgid "DownloadTimeoutMiddleware"
msgstr ""

#: ../../topics/downloader-middleware.rst:297
msgid "This middleware sets the download timeout for requests specified in the :setting:`DOWNLOAD_TIMEOUT` setting or :attr:`download_timeout` spider attribute."
msgstr ""

#: ../../topics/downloader-middleware.rst:303
msgid "You can also set download timeout per-request using :reqmeta:`download_timeout` Request.meta key; this is supported even when DownloadTimeoutMiddleware is disabled."
msgstr ""

#: ../../topics/downloader-middleware.rst:308
msgid "HttpAuthMiddleware"
msgstr ""

#: ../../topics/downloader-middleware.rst:315
msgid "This middleware authenticates all requests generated from certain spiders using `Basic access authentication`_ (aka. HTTP auth)."
msgstr ""

#: ../../topics/downloader-middleware.rst:318
msgid "To enable HTTP authentication from certain spiders, set the ``http_user`` and ``http_pass`` attributes of those spiders."
msgstr ""

#: ../../topics/downloader-middleware.rst:321
#: ../../topics/exporters.rst:94
#: ../../topics/exporters.rst:115
#: ../../topics/loaders.rst:528
#: ../../topics/loaders.rst:539
#: ../../topics/loaders.rst:551
#: ../../topics/request-response.rst:216
#: ../../topics/settings.rst:61
#: ../../topics/settings.rst:839
#: ../../topics/settings.rst:976
#: ../../topics/settings.rst:1042
#: ../../topics/settings.rst:1069
#: ../../topics/settings.rst:1354
#: ../../topics/spiders.rst:510
msgid "Example::"
msgstr ""

#: ../../topics/downloader-middleware.rst:337
msgid "HttpCacheMiddleware"
msgstr ""

#: ../../topics/downloader-middleware.rst:344
msgid "This middleware provides low-level cache to all HTTP requests and responses. It has to be combined with a cache storage backend as well as a cache policy."
msgstr ""

#: ../../topics/downloader-middleware.rst:347
msgid "Scrapy ships with three HTTP cache storage backends:"
msgstr ""

#: ../../topics/downloader-middleware.rst:349
msgid ":ref:`httpcache-storage-fs`"
msgstr ""

#: ../../topics/downloader-middleware.rst:350
msgid ":ref:`httpcache-storage-dbm`"
msgstr ""

#: ../../topics/downloader-middleware.rst:352
msgid "You can change the HTTP cache storage backend with the :setting:`HTTPCACHE_STORAGE` setting. Or you can also :ref:`implement your own storage backend. <httpcache-storage-custom>`"
msgstr ""

#: ../../topics/downloader-middleware.rst:355
msgid "Scrapy ships with two HTTP cache policies:"
msgstr ""

#: ../../topics/downloader-middleware.rst:357
msgid ":ref:`httpcache-policy-rfc2616`"
msgstr ""

#: ../../topics/downloader-middleware.rst:358
msgid ":ref:`httpcache-policy-dummy`"
msgstr ""

#: ../../topics/downloader-middleware.rst:360
msgid "You can change the HTTP cache policy with the :setting:`HTTPCACHE_POLICY` setting. Or you can also implement your own policy."
msgstr ""

#: ../../topics/downloader-middleware.rst:365
msgid "You can also avoid caching a response on every policy using :reqmeta:`dont_cache` meta key equals ``True``."
msgstr ""

#: ../../topics/downloader-middleware.rst:373
msgid "Dummy policy (default)"
msgstr ""

#: ../../topics/downloader-middleware.rst:377
msgid "This policy has no awareness of any HTTP Cache-Control directives. Every request and its corresponding response are cached.  When the same request is seen again, the response is returned without transferring anything from the Internet."
msgstr ""

#: ../../topics/downloader-middleware.rst:382
msgid "The Dummy policy is useful for testing spiders faster (without having to wait for downloads every time) and for trying your spider offline, when an Internet connection is not available. The goal is to be able to \"replay\" a spider run *exactly as it ran before*."
msgstr ""

#: ../../topics/downloader-middleware.rst:391
msgid "RFC2616 policy"
msgstr ""

#: ../../topics/downloader-middleware.rst:395
msgid "This policy provides a RFC2616 compliant HTTP cache, i.e. with HTTP Cache-Control awareness, aimed at production and used in continuous runs to avoid downloading unmodified data (to save bandwidth and speed up crawls)."
msgstr ""

#: ../../topics/downloader-middleware.rst:400
msgid "What is implemented:"
msgstr ""

#: ../../topics/downloader-middleware.rst:402
msgid "Do not attempt to store responses/requests with ``no-store`` cache-control directive set"
msgstr ""

#: ../../topics/downloader-middleware.rst:403
msgid "Do not serve responses from cache if ``no-cache`` cache-control directive is set even for fresh responses"
msgstr ""

#: ../../topics/downloader-middleware.rst:404
msgid "Compute freshness lifetime from ``max-age`` cache-control directive"
msgstr ""

#: ../../topics/downloader-middleware.rst:405
msgid "Compute freshness lifetime from ``Expires`` response header"
msgstr ""

#: ../../topics/downloader-middleware.rst:406
msgid "Compute freshness lifetime from ``Last-Modified`` response header (heuristic used by Firefox)"
msgstr ""

#: ../../topics/downloader-middleware.rst:407
msgid "Compute current age from ``Age`` response header"
msgstr ""

#: ../../topics/downloader-middleware.rst:408
msgid "Compute current age from ``Date`` header"
msgstr ""

#: ../../topics/downloader-middleware.rst:409
msgid "Revalidate stale responses based on ``Last-Modified`` response header"
msgstr ""

#: ../../topics/downloader-middleware.rst:410
msgid "Revalidate stale responses based on ``ETag`` response header"
msgstr ""

#: ../../topics/downloader-middleware.rst:411
msgid "Set ``Date`` header for any received response missing it"
msgstr ""

#: ../../topics/downloader-middleware.rst:412
msgid "Support ``max-stale`` cache-control directive in requests"
msgstr ""

#: ../../topics/downloader-middleware.rst:414
msgid "This allows spiders to be configured with the full RFC2616 cache policy, but avoid revalidation on a request-by-request basis, while remaining conformant with the HTTP spec."
msgstr ""

#: ../../topics/downloader-middleware.rst:420
msgid "Add ``Cache-Control: max-stale=600`` to Request headers to accept responses that have exceeded their expiration time by no more than 600 seconds."
msgstr ""

#: ../../topics/downloader-middleware.rst:423
msgid "See also: RFC2616, 14.9.3"
msgstr ""

#: ../../topics/downloader-middleware.rst:425
msgid "What is missing:"
msgstr ""

#: ../../topics/downloader-middleware.rst:427
msgid "``Pragma: no-cache`` support https://www.w3.org/Protocols/rfc2616/rfc2616-sec14.html#sec14.9.1"
msgstr ""

#: ../../topics/downloader-middleware.rst:428
msgid "``Vary`` header support https://www.w3.org/Protocols/rfc2616/rfc2616-sec13.html#sec13.6"
msgstr ""

#: ../../topics/downloader-middleware.rst:429
msgid "Invalidation after updates or deletes https://www.w3.org/Protocols/rfc2616/rfc2616-sec13.html#sec13.10"
msgstr ""

#: ../../topics/downloader-middleware.rst:430
msgid "... probably others .."
msgstr ""

#: ../../topics/downloader-middleware.rst:436
msgid "Filesystem storage backend (default)"
msgstr ""

#: ../../topics/downloader-middleware.rst:440
msgid "File system storage backend is available for the HTTP cache middleware."
msgstr ""

#: ../../topics/downloader-middleware.rst:442
msgid "Each request/response pair is stored in a different directory containing the following files:"
msgstr ""

#: ../../topics/downloader-middleware.rst:445
msgid "``request_body`` - the plain request body"
msgstr ""

#: ../../topics/downloader-middleware.rst:447
msgid "``request_headers`` - the request headers (in raw HTTP format)"
msgstr ""

#: ../../topics/downloader-middleware.rst:449
msgid "``response_body`` - the plain response body"
msgstr ""

#: ../../topics/downloader-middleware.rst:451
msgid "``response_headers`` - the request headers (in raw HTTP format)"
msgstr ""

#: ../../topics/downloader-middleware.rst:453
msgid "``meta`` - some metadata of this cache resource in Python ``repr()`` format (grep-friendly format)"
msgstr ""

#: ../../topics/downloader-middleware.rst:456
msgid "``pickled_meta`` - the same metadata in ``meta`` but pickled for more efficient deserialization"
msgstr ""

#: ../../topics/downloader-middleware.rst:459
msgid "The directory name is made from the request fingerprint (see ``scrapy.utils.request.fingerprint``), and one level of subdirectories is used to avoid creating too many files into the same directory (which is inefficient in many file systems). An example directory could be::"
msgstr ""

#: ../../topics/downloader-middleware.rst:469
msgid "DBM storage backend"
msgstr ""

#: ../../topics/downloader-middleware.rst:475
msgid "A DBM_ storage backend is also available for the HTTP cache middleware."
msgstr ""

#: ../../topics/downloader-middleware.rst:477
msgid "By default, it uses the :mod:`dbm`, but you can change it with the :setting:`HTTPCACHE_DBM_MODULE` setting."
msgstr ""

#: ../../topics/downloader-middleware.rst:483
msgid "Writing your own storage backend"
msgstr ""

#: ../../topics/downloader-middleware.rst:485
msgid "You can implement a cache storage backend by creating a Python class that defines the methods described below."
msgstr ""

#: ../../topics/downloader-middleware.rst:494
msgid "This method gets called after a spider has been opened for crawling. It handles the :signal:`open_spider <spider_opened>` signal."
msgstr ""

#: ../../topics/downloader-middleware.rst:497
#: ../../topics/signals.rst:249
msgid "the spider which has been opened"
msgstr ""

#: ../../topics/downloader-middleware.rst:502
msgid "This method gets called after a spider has been closed. It handles the :signal:`close_spider <spider_closed>` signal."
msgstr ""

#: ../../topics/downloader-middleware.rst:505
#: ../../topics/signals.rst:225
msgid "the spider which has been closed"
msgstr ""

#: ../../topics/downloader-middleware.rst:510
msgid "Return response if present in cache, or ``None`` otherwise."
msgstr ""

#: ../../topics/downloader-middleware.rst:512
msgid "the spider which generated the request"
msgstr ""

#: ../../topics/downloader-middleware.rst:515
msgid "the request to find cached response for"
msgstr ""

#: ../../topics/downloader-middleware.rst:520
msgid "Store the given response in the cache."
msgstr ""

#: ../../topics/downloader-middleware.rst:522
#: ../../topics/signals.rst:393
#: ../../topics/signals.rst:412
msgid "the spider for which the response is intended"
msgstr ""

#: ../../topics/downloader-middleware.rst:525
msgid "the corresponding request the spider generated"
msgstr ""

#: ../../topics/downloader-middleware.rst:528
msgid "the response to store in the cache"
msgstr ""

#: ../../topics/downloader-middleware.rst:531
msgid "In order to use your storage backend, set:"
msgstr ""

#: ../../topics/downloader-middleware.rst:533
msgid ":setting:`HTTPCACHE_STORAGE` to the Python import path of your custom storage class."
msgstr ""

#: ../../topics/downloader-middleware.rst:537
msgid "HTTPCache middleware settings"
msgstr ""

#: ../../topics/downloader-middleware.rst:539
msgid "The :class:`HttpCacheMiddleware` can be configured through the following settings:"
msgstr ""

#: ../../topics/downloader-middleware.rst:545
msgid "HTTPCACHE_ENABLED"
msgstr ""

#: ../../topics/downloader-middleware.rst:551
msgid "Whether the HTTP cache will be enabled."
msgstr ""

#: ../../topics/downloader-middleware.rst:553
msgid "Before 0.11, :setting:`HTTPCACHE_DIR` was used to enable cache."
msgstr ""

#: ../../topics/downloader-middleware.rst:559
msgid "HTTPCACHE_EXPIRATION_SECS"
msgstr ""

#: ../../topics/downloader-middleware.rst:561
#: ../../topics/extensions.rst:265
#: ../../topics/extensions.rst:277
#: ../../topics/extensions.rst:293
#: ../../topics/extensions.rst:307
#: ../../topics/feed-exports.rst:314
#: ../../topics/settings.rst:271
#: ../../topics/settings.rst:314
#: ../../topics/settings.rst:326
#: ../../topics/settings.rst:564
#: ../../topics/settings.rst:1002
#: ../../topics/settings.rst:1053
msgid "Default: ``0``"
msgstr ""

#: ../../topics/downloader-middleware.rst:563
msgid "Expiration time for cached requests, in seconds."
msgstr ""

#: ../../topics/downloader-middleware.rst:565
msgid "Cached requests older than this time will be re-downloaded. If zero, cached requests will never expire."
msgstr ""

#: ../../topics/downloader-middleware.rst:568
msgid "Before 0.11, zero meant cached requests always expire."
msgstr ""

#: ../../topics/downloader-middleware.rst:574
msgid "HTTPCACHE_DIR"
msgstr ""

#: ../../topics/downloader-middleware.rst:576
msgid "Default: ``'httpcache'``"
msgstr ""

#: ../../topics/downloader-middleware.rst:578
msgid "The directory to use for storing the (low-level) HTTP cache. If empty, the HTTP cache will be disabled. If a relative path is given, is taken relative to the project data dir. For more info see: :ref:`topics-project-structure`."
msgstr ""

#: ../../topics/downloader-middleware.rst:585
msgid "HTTPCACHE_IGNORE_HTTP_CODES"
msgstr ""

#: ../../topics/downloader-middleware.rst:589
#: ../../topics/downloader-middleware.rst:684
#: ../../topics/downloader-middleware.rst:869
#: ../../topics/settings.rst:970
#: ../../topics/settings.rst:1350
#: ../../topics/spider-middleware.rst:276
msgid "Default: ``[]``"
msgstr ""

#: ../../topics/downloader-middleware.rst:591
msgid "Don't cache response with these HTTP codes."
msgstr ""

#: ../../topics/downloader-middleware.rst:596
msgid "HTTPCACHE_IGNORE_MISSING"
msgstr ""

#: ../../topics/downloader-middleware.rst:600
msgid "If enabled, requests not found in the cache will be ignored instead of downloaded."
msgstr ""

#: ../../topics/downloader-middleware.rst:605
msgid "HTTPCACHE_IGNORE_SCHEMES"
msgstr ""

#: ../../topics/downloader-middleware.rst:609
msgid "Default: ``['file']``"
msgstr ""

#: ../../topics/downloader-middleware.rst:611
msgid "Don't cache responses with these URI schemes."
msgstr ""

#: ../../topics/downloader-middleware.rst:616
msgid "HTTPCACHE_STORAGE"
msgstr ""

#: ../../topics/downloader-middleware.rst:618
msgid "Default: ``'scrapy.extensions.httpcache.FilesystemCacheStorage'``"
msgstr ""

#: ../../topics/downloader-middleware.rst:620
msgid "The class which implements the cache storage backend."
msgstr ""

#: ../../topics/downloader-middleware.rst:625
msgid "HTTPCACHE_DBM_MODULE"
msgstr ""

#: ../../topics/downloader-middleware.rst:629
msgid "Default: ``'dbm'``"
msgstr ""

#: ../../topics/downloader-middleware.rst:631
msgid "The database module to use in the :ref:`DBM storage backend <httpcache-storage-dbm>`. This setting is specific to the DBM backend."
msgstr ""

#: ../../topics/downloader-middleware.rst:637
msgid "HTTPCACHE_POLICY"
msgstr ""

#: ../../topics/downloader-middleware.rst:641
msgid "Default: ``'scrapy.extensions.httpcache.DummyPolicy'``"
msgstr ""

#: ../../topics/downloader-middleware.rst:643
msgid "The class which implements the cache policy."
msgstr ""

#: ../../topics/downloader-middleware.rst:648
msgid "HTTPCACHE_GZIP"
msgstr ""

#: ../../topics/downloader-middleware.rst:654
msgid "If enabled, will compress all cached data with gzip. This setting is specific to the Filesystem backend."
msgstr ""

#: ../../topics/downloader-middleware.rst:660
msgid "HTTPCACHE_ALWAYS_STORE"
msgstr ""

#: ../../topics/downloader-middleware.rst:666
msgid "If enabled, will cache pages unconditionally."
msgstr ""

#: ../../topics/downloader-middleware.rst:668
msgid "A spider may wish to have all responses available in the cache, for future use with ``Cache-Control: max-stale``, for instance. The DummyPolicy caches all responses but never revalidates them, and sometimes a more nuanced policy is desirable."
msgstr ""

#: ../../topics/downloader-middleware.rst:673
msgid "This setting still respects ``Cache-Control: no-store`` directives in responses. If you don't want that, filter ``no-store`` out of the Cache-Control headers in responses you feed to the cache middleware."
msgstr ""

#: ../../topics/downloader-middleware.rst:680
msgid "HTTPCACHE_IGNORE_RESPONSE_CACHE_CONTROLS"
msgstr ""

#: ../../topics/downloader-middleware.rst:686
msgid "List of Cache-Control directives in responses to be ignored."
msgstr ""

#: ../../topics/downloader-middleware.rst:688
msgid "Sites often set \"no-store\", \"no-cache\", \"must-revalidate\", etc., but get upset at the traffic a spider can generate if it actually respects those directives. This allows to selectively ignore Cache-Control directives that are known to be unimportant for the sites being crawled."
msgstr ""

#: ../../topics/downloader-middleware.rst:693
msgid "We assume that the spider will not issue Cache-Control directives in requests unless it actually needs them, so directives in requests are not filtered."
msgstr ""

#: ../../topics/downloader-middleware.rst:698
msgid "HttpCompressionMiddleware"
msgstr ""

#: ../../topics/downloader-middleware.rst:705
msgid "This middleware allows compressed (gzip, deflate) traffic to be sent/received from web sites."
msgstr ""

#: ../../topics/downloader-middleware.rst:708
msgid "This middleware also supports decoding `brotli-compressed`_ responses, provided `brotlipy`_ is installed."
msgstr ""

#: ../../topics/downloader-middleware.rst:715
msgid "HttpCompressionMiddleware Settings"
msgstr ""

#: ../../topics/downloader-middleware.rst:720
msgid "COMPRESSION_ENABLED"
msgstr ""

#: ../../topics/downloader-middleware.rst:724
msgid "Whether the Compression middleware will be enabled."
msgstr ""

#: ../../topics/downloader-middleware.rst:728
msgid "HttpProxyMiddleware"
msgstr ""

#: ../../topics/downloader-middleware.rst:739
msgid "This middleware sets the HTTP proxy to use for requests, by setting the ``proxy`` meta value for :class:`~scrapy.http.Request` objects."
msgstr ""

#: ../../topics/downloader-middleware.rst:742
msgid "Like the Python standard library module :mod:`urllib.request`, it obeys the following environment variables:"
msgstr ""

#: ../../topics/downloader-middleware.rst:745
msgid "``http_proxy``"
msgstr ""

#: ../../topics/downloader-middleware.rst:746
msgid "``https_proxy``"
msgstr ""

#: ../../topics/downloader-middleware.rst:747
msgid "``no_proxy``"
msgstr ""

#: ../../topics/downloader-middleware.rst:749
msgid "You can also set the meta key ``proxy`` per-request, to a value like ``http://some_proxy_server:port`` or ``http://username:password@some_proxy_server:port``. Keep in mind this value will take precedence over ``http_proxy``/``https_proxy`` environment variables, and it will also ignore ``no_proxy`` environment variable."
msgstr ""

#: ../../topics/downloader-middleware.rst:755
msgid "RedirectMiddleware"
msgstr ""

#: ../../topics/downloader-middleware.rst:762
msgid "This middleware handles redirection of requests based on response status."
msgstr ""

#: ../../topics/downloader-middleware.rst:766
msgid "The urls which the request goes through (while being redirected) can be found in the ``redirect_urls`` :attr:`Request.meta <scrapy.http.Request.meta>` key."
msgstr ""

#: ../../topics/downloader-middleware.rst:771
msgid "The reason behind each redirect in :reqmeta:`redirect_urls` can be found in the ``redirect_reasons`` :attr:`Request.meta <scrapy.http.Request.meta>` key. For example: ``[301, 302, 307, 'meta refresh']``."
msgstr ""

#: ../../topics/downloader-middleware.rst:775
msgid "The format of a reason depends on the middleware that handled the corresponding redirect. For example, :class:`RedirectMiddleware` indicates the triggering response status code as an integer, while :class:`MetaRefreshMiddleware` always uses the ``'meta refresh'`` string as reason."
msgstr ""

#: ../../topics/downloader-middleware.rst:780
msgid "The :class:`RedirectMiddleware` can be configured through the following settings (see the settings documentation for more info):"
msgstr ""

#: ../../topics/downloader-middleware.rst:783
msgid ":setting:`REDIRECT_ENABLED`"
msgstr ""

#: ../../topics/downloader-middleware.rst:784
msgid ":setting:`REDIRECT_MAX_TIMES`"
msgstr ""

#: ../../topics/downloader-middleware.rst:788
msgid "If :attr:`Request.meta <scrapy.http.Request.meta>` has ``dont_redirect`` key set to True, the request will be ignored by this middleware."
msgstr ""

#: ../../topics/downloader-middleware.rst:791
msgid "If you want to handle some redirect status codes in your spider, you can specify these in the ``handle_httpstatus_list`` spider attribute."
msgstr ""

#: ../../topics/downloader-middleware.rst:794
msgid "For example, if you want the redirect middleware to ignore 301 and 302 responses (and pass them through to your spider) you can do this::"
msgstr ""

#: ../../topics/downloader-middleware.rst:800
#: ../../topics/spider-middleware.rst:256
msgid "The ``handle_httpstatus_list`` key of :attr:`Request.meta <scrapy.http.Request.meta>` can also be used to specify which response codes to allow on a per-request basis. You can also set the meta key ``handle_httpstatus_all`` to ``True`` if you want to allow any response code for a request."
msgstr ""

#: ../../topics/downloader-middleware.rst:808
msgid "RedirectMiddleware settings"
msgstr ""

#: ../../topics/downloader-middleware.rst:813
msgid "REDIRECT_ENABLED"
msgstr ""

#: ../../topics/downloader-middleware.rst:819
msgid "Whether the Redirect middleware will be enabled."
msgstr ""

#: ../../topics/downloader-middleware.rst:824
msgid "REDIRECT_MAX_TIMES"
msgstr ""

#: ../../topics/downloader-middleware.rst:826
msgid "Default: ``20``"
msgstr ""

#: ../../topics/downloader-middleware.rst:828
msgid "The maximum number of redirections that will be followed for a single request. After this maximum, the request's response is returned as is."
msgstr ""

#: ../../topics/downloader-middleware.rst:832
msgid "MetaRefreshMiddleware"
msgstr ""

#: ../../topics/downloader-middleware.rst:836
msgid "This middleware handles redirection of requests based on meta-refresh html tag."
msgstr ""

#: ../../topics/downloader-middleware.rst:838
msgid "The :class:`MetaRefreshMiddleware` can be configured through the following settings (see the settings documentation for more info):"
msgstr ""

#: ../../topics/downloader-middleware.rst:841
msgid ":setting:`METAREFRESH_ENABLED`"
msgstr ""

#: ../../topics/downloader-middleware.rst:842
msgid ":setting:`METAREFRESH_IGNORE_TAGS`"
msgstr ""

#: ../../topics/downloader-middleware.rst:843
msgid ":setting:`METAREFRESH_MAXDELAY`"
msgstr ""

#: ../../topics/downloader-middleware.rst:845
msgid "This middleware obey :setting:`REDIRECT_MAX_TIMES` setting, :reqmeta:`dont_redirect`, :reqmeta:`redirect_urls` and :reqmeta:`redirect_reasons` request meta keys as described for :class:`RedirectMiddleware`"
msgstr ""

#: ../../topics/downloader-middleware.rst:851
msgid "MetaRefreshMiddleware settings"
msgstr ""

#: ../../topics/downloader-middleware.rst:856
msgid "METAREFRESH_ENABLED"
msgstr ""

#: ../../topics/downloader-middleware.rst:862
msgid "Whether the Meta Refresh middleware will be enabled."
msgstr ""

#: ../../topics/downloader-middleware.rst:867
msgid "METAREFRESH_IGNORE_TAGS"
msgstr ""

#: ../../topics/downloader-middleware.rst:871
msgid "Meta tags within these tags are ignored."
msgstr ""

#: ../../topics/downloader-middleware.rst:873
msgid "The default value of :setting:`METAREFRESH_IGNORE_TAGS` changed from ``['script', 'noscript']`` to ``[]``."
msgstr ""

#: ../../topics/downloader-middleware.rst:880
msgid "METAREFRESH_MAXDELAY"
msgstr ""

#: ../../topics/downloader-middleware.rst:882
#: ../../topics/settings.rst:237
msgid "Default: ``100``"
msgstr ""

#: ../../topics/downloader-middleware.rst:884
msgid "The maximum meta-refresh delay (in seconds) to follow the redirection. Some sites use meta-refresh for redirecting to a session expired page, so we restrict automatic redirection to the maximum delay."
msgstr ""

#: ../../topics/downloader-middleware.rst:889
msgid "RetryMiddleware"
msgstr ""

#: ../../topics/downloader-middleware.rst:896
msgid "A middleware to retry failed requests that are potentially caused by temporary problems such as a connection timeout or HTTP 500 error."
msgstr ""

#: ../../topics/downloader-middleware.rst:899
msgid "Failed pages are collected on the scraping process and rescheduled at the end, once the spider has finished crawling all regular (non failed) pages."
msgstr ""

#: ../../topics/downloader-middleware.rst:902
msgid "The :class:`RetryMiddleware` can be configured through the following settings (see the settings documentation for more info):"
msgstr ""

#: ../../topics/downloader-middleware.rst:905
msgid ":setting:`RETRY_ENABLED`"
msgstr ""

#: ../../topics/downloader-middleware.rst:906
msgid ":setting:`RETRY_TIMES`"
msgstr ""

#: ../../topics/downloader-middleware.rst:907
msgid ":setting:`RETRY_HTTP_CODES`"
msgstr ""

#: ../../topics/downloader-middleware.rst:911
msgid "If :attr:`Request.meta <scrapy.http.Request.meta>` has ``dont_retry`` key set to True, the request will be ignored by this middleware."
msgstr ""

#: ../../topics/downloader-middleware.rst:915
msgid "RetryMiddleware Settings"
msgstr ""

#: ../../topics/downloader-middleware.rst:920
msgid "RETRY_ENABLED"
msgstr ""

#: ../../topics/downloader-middleware.rst:926
msgid "Whether the Retry middleware will be enabled."
msgstr ""

#: ../../topics/downloader-middleware.rst:931
msgid "RETRY_TIMES"
msgstr ""

#: ../../topics/downloader-middleware.rst:933
msgid "Default: ``2``"
msgstr ""

#: ../../topics/downloader-middleware.rst:935
msgid "Maximum number of times to retry, in addition to the first download."
msgstr ""

#: ../../topics/downloader-middleware.rst:937
msgid "Maximum number of retries can also be specified per-request using :reqmeta:`max_retry_times` attribute of :attr:`Request.meta <scrapy.http.Request.meta>`. When initialized, the :reqmeta:`max_retry_times` meta key takes higher precedence over the :setting:`RETRY_TIMES` setting."
msgstr ""

#: ../../topics/downloader-middleware.rst:945
msgid "RETRY_HTTP_CODES"
msgstr ""

#: ../../topics/downloader-middleware.rst:947
msgid "Default: ``[500, 502, 503, 504, 522, 524, 408, 429]``"
msgstr ""

#: ../../topics/downloader-middleware.rst:949
msgid "Which HTTP response codes to retry. Other errors (DNS lookup issues, connections lost, etc) are always retried."
msgstr ""

#: ../../topics/downloader-middleware.rst:952
msgid "In some cases you may want to add 400 to :setting:`RETRY_HTTP_CODES` because it is a common code used to indicate server overload. It is not included by default because HTTP specs say so."
msgstr ""

#: ../../topics/downloader-middleware.rst:960
msgid "RobotsTxtMiddleware"
msgstr ""

#: ../../topics/downloader-middleware.rst:967
msgid "This middleware filters out requests forbidden by the robots.txt exclusion standard."
msgstr ""

#: ../../topics/downloader-middleware.rst:970
msgid "To make sure Scrapy respects robots.txt make sure the middleware is enabled and the :setting:`ROBOTSTXT_OBEY` setting is enabled."
msgstr ""

#: ../../topics/downloader-middleware.rst:973
msgid "The :setting:`ROBOTSTXT_USER_AGENT` setting can be used to specify the user agent string to use for matching in the robots.txt_ file. If it is ``None``, the User-Agent header you are sending with the request or the :setting:`USER_AGENT` setting (in that order) will be used for determining the user agent to use in the robots.txt_ file."
msgstr ""

#: ../../topics/downloader-middleware.rst:979
msgid "This middleware has to be combined with a robots.txt_ parser."
msgstr ""

#: ../../topics/downloader-middleware.rst:981
msgid "Scrapy ships with support for the following robots.txt_ parsers:"
msgstr ""

#: ../../topics/downloader-middleware.rst:983
msgid ":ref:`Protego <protego-parser>` (default)"
msgstr ""

#: ../../topics/downloader-middleware.rst:984
msgid ":ref:`RobotFileParser <python-robotfileparser>`"
msgstr ""

#: ../../topics/downloader-middleware.rst:985
msgid ":ref:`Reppy <reppy-parser>`"
msgstr ""

#: ../../topics/downloader-middleware.rst:986
msgid ":ref:`Robotexclusionrulesparser <rerp-parser>`"
msgstr ""

#: ../../topics/downloader-middleware.rst:988
msgid "You can change the robots.txt_ parser with the :setting:`ROBOTSTXT_PARSER` setting. Or you can also :ref:`implement support for a new parser <support-for-new-robots-parser>`."
msgstr ""

#: ../../topics/downloader-middleware.rst:993
msgid "If :attr:`Request.meta <scrapy.http.Request.meta>` has ``dont_obey_robotstxt`` key set to True the request will be ignored by this middleware even if :setting:`ROBOTSTXT_OBEY` is enabled."
msgstr ""

#: ../../topics/downloader-middleware.rst:998
msgid "Parsers vary in several aspects:"
msgstr ""

#: ../../topics/downloader-middleware.rst:1000
msgid "Language of implementation"
msgstr ""

#: ../../topics/downloader-middleware.rst:1002
msgid "Supported specification"
msgstr ""

#: ../../topics/downloader-middleware.rst:1004
msgid "Support for wildcard matching"
msgstr ""

#: ../../topics/downloader-middleware.rst:1006
msgid "Usage of `length based rule <https://developers.google.com/search/reference/robots_txt#order-of-precedence-for-group-member-lines>`_: in particular for ``Allow`` and ``Disallow`` directives, where the most specific rule based on the length of the path trumps the less specific (shorter) rule"
msgstr ""

#: ../../topics/downloader-middleware.rst:1011
msgid "Performance comparison of different parsers is available at `the following link <https://anubhavp28.github.io/gsoc-weekly-checkin-12/>`_."
msgstr ""

#: ../../topics/downloader-middleware.rst:1017
msgid "Protego parser"
msgstr ""

#: ../../topics/downloader-middleware.rst:1019
msgid "Based on `Protego <https://github.com/scrapy/protego>`_:"
msgstr ""

#: ../../topics/downloader-middleware.rst:1021
#: ../../topics/downloader-middleware.rst:1087
msgid "implemented in Python"
msgstr ""

#: ../../topics/downloader-middleware.rst:1023
msgid "is compliant with `Google's Robots.txt Specification <https://developers.google.com/search/reference/robots_txt>`_"
msgstr ""

#: ../../topics/downloader-middleware.rst:1026
#: ../../topics/downloader-middleware.rst:1067
#: ../../topics/downloader-middleware.rst:1092
msgid "supports wildcard matching"
msgstr ""

#: ../../topics/downloader-middleware.rst:1028
#: ../../topics/downloader-middleware.rst:1069
msgid "uses the length based rule"
msgstr ""

#: ../../topics/downloader-middleware.rst:1030
msgid "Scrapy uses this parser by default."
msgstr ""

#: ../../topics/downloader-middleware.rst:1035
msgid "RobotFileParser"
msgstr ""

#: ../../topics/downloader-middleware.rst:1037
msgid "Based on :class:`~urllib.robotparser.RobotFileParser`:"
msgstr ""

#: ../../topics/downloader-middleware.rst:1039
msgid "is Python's built-in robots.txt_ parser"
msgstr ""

#: ../../topics/downloader-middleware.rst:1041
#: ../../topics/downloader-middleware.rst:1064
#: ../../topics/downloader-middleware.rst:1089
msgid "is compliant with `Martijn Koster's 1996 draft specification <https://www.robotstxt.org/norobots-rfc.txt>`_"
msgstr ""

#: ../../topics/downloader-middleware.rst:1044
msgid "lacks support for wildcard matching"
msgstr ""

#: ../../topics/downloader-middleware.rst:1046
#: ../../topics/downloader-middleware.rst:1094
msgid "doesn't use the length based rule"
msgstr ""

#: ../../topics/downloader-middleware.rst:1048
msgid "It is faster than Protego and backward-compatible with versions of Scrapy before 1.8.0."
msgstr ""

#: ../../topics/downloader-middleware.rst:1050
msgid "In order to use this parser, set:"
msgstr ""

#: ../../topics/downloader-middleware.rst:1052
msgid ":setting:`ROBOTSTXT_PARSER` to ``scrapy.robotstxt.PythonRobotParser``"
msgstr ""

#: ../../topics/downloader-middleware.rst:1057
msgid "Reppy parser"
msgstr ""

#: ../../topics/downloader-middleware.rst:1059
msgid "Based on `Reppy <https://github.com/seomoz/reppy/>`_:"
msgstr ""

#: ../../topics/downloader-middleware.rst:1061
msgid "is a Python wrapper around `Robots Exclusion Protocol Parser for C++ <https://github.com/seomoz/rep-cpp>`_"
msgstr ""

#: ../../topics/downloader-middleware.rst:1071
msgid "Native implementation, provides better speed than Protego."
msgstr ""

#: ../../topics/downloader-middleware.rst:1073
#: ../../topics/downloader-middleware.rst:1096
msgid "In order to use this parser:"
msgstr ""

#: ../../topics/downloader-middleware.rst:1075
msgid "Install `Reppy <https://github.com/seomoz/reppy/>`_ by running ``pip install reppy``"
msgstr ""

#: ../../topics/downloader-middleware.rst:1077
msgid "Set :setting:`ROBOTSTXT_PARSER` setting to ``scrapy.robotstxt.ReppyRobotParser``"
msgstr ""

#: ../../topics/downloader-middleware.rst:1083
msgid "Robotexclusionrulesparser"
msgstr ""

#: ../../topics/downloader-middleware.rst:1085
msgid "Based on `Robotexclusionrulesparser <http://nikitathespider.com/python/rerp/>`_:"
msgstr ""

#: ../../topics/downloader-middleware.rst:1098
msgid "Install `Robotexclusionrulesparser <http://nikitathespider.com/python/rerp/>`_ by running ``pip install robotexclusionrulesparser``"
msgstr ""

#: ../../topics/downloader-middleware.rst:1101
msgid "Set :setting:`ROBOTSTXT_PARSER` setting to ``scrapy.robotstxt.RerpRobotParser``"
msgstr ""

#: ../../topics/downloader-middleware.rst:1107
msgid "Implementing support for a new parser"
msgstr ""

#: ../../topics/downloader-middleware.rst:1109
msgid "You can implement support for a new robots.txt_ parser by subclassing the abstract base class :class:`~scrapy.robotstxt.RobotParser` and implementing the methods described below."
msgstr ""

#: ../../../scrapy/robotstxt.py:docstring of scrapy.robotstxt.RobotParser.allowed:1
msgid "Return ``True`` if  ``user_agent`` is allowed to crawl ``url``, otherwise return ``False``."
msgstr ""

#: ../../../scrapy/robotstxt.py:docstring of scrapy.robotstxt.RobotParser.allowed:3
msgid "Absolute URL"
msgstr ""

#: ../../../scrapy/robotstxt.py:docstring of scrapy.robotstxt.RobotParser.allowed:6
msgid "User agent"
msgstr ""

#: ../../../scrapy/robotstxt.py:docstring of scrapy.robotstxt.RobotParser.from_crawler:1
msgid "Parse the content of a robots.txt_ file as bytes. This must be a class method. It must return a new instance of the parser backend."
msgstr ""

#: ../../../scrapy/robotstxt.py:docstring of scrapy.robotstxt.RobotParser.from_crawler:4
msgid "crawler which made the request"
msgstr ""

#: ../../../scrapy/robotstxt.py:docstring of scrapy.robotstxt.RobotParser.from_crawler:7
msgid "content of a robots.txt_ file."
msgstr ""

#: ../../topics/downloader-middleware.rst:1122
msgid "DownloaderStats"
msgstr ""

#: ../../topics/downloader-middleware.rst:1129
msgid "Middleware that stores stats of all requests, responses and exceptions that pass through it."
msgstr ""

#: ../../topics/downloader-middleware.rst:1132
msgid "To use this middleware you must enable the :setting:`DOWNLOADER_STATS` setting."
msgstr ""

#: ../../topics/downloader-middleware.rst:1136
msgid "UserAgentMiddleware"
msgstr ""

#: ../../topics/downloader-middleware.rst:1143
msgid "Middleware that allows spiders to override the default user agent."
msgstr ""

#: ../../topics/downloader-middleware.rst:1145
msgid "In order for a spider to override the default user agent, its ``user_agent`` attribute must be set."
msgstr ""

#: ../../topics/downloader-middleware.rst:1151
msgid "AjaxCrawlMiddleware"
msgstr ""

#: ../../topics/downloader-middleware.rst:1157
msgid "Middleware that finds 'AJAX crawlable' page variants based on meta-fragment html tag. See https://developers.google.com/search/docs/ajax-crawling/docs/getting-started for more info."
msgstr ""

#: ../../topics/downloader-middleware.rst:1164
msgid "Scrapy finds 'AJAX crawlable' pages for URLs like ``'http://example.com/!#foo=bar'`` even without this middleware. AjaxCrawlMiddleware is necessary when URL doesn't contain ``'!#'``. This is often a case for 'index' or 'main' website pages."
msgstr ""

#: ../../topics/downloader-middleware.rst:1170
msgid "AjaxCrawlMiddleware Settings"
msgstr ""

#: ../../topics/downloader-middleware.rst:1175
msgid "AJAXCRAWL_ENABLED"
msgstr ""

#: ../../topics/downloader-middleware.rst:1181
msgid "Whether the AjaxCrawlMiddleware will be enabled. You may want to enable it for :ref:`broad crawls <topics-broad-crawls>`."
msgstr ""

#: ../../topics/downloader-middleware.rst:1185
msgid "HttpProxyMiddleware settings"
msgstr ""

#: ../../topics/downloader-middleware.rst:1191
msgid "HTTPPROXY_ENABLED"
msgstr ""

#: ../../topics/downloader-middleware.rst:1195
msgid "Whether or not to enable the :class:`HttpProxyMiddleware`."
msgstr ""

#: ../../topics/downloader-middleware.rst:1198
msgid "HTTPPROXY_AUTH_ENCODING"
msgstr ""

#: ../../topics/downloader-middleware.rst:1200
msgid "Default: ``\"latin-1\"``"
msgstr ""

#: ../../topics/downloader-middleware.rst:1202
msgid "The default encoding for proxy authentication on :class:`HttpProxyMiddleware`."
msgstr ""

#: ../../topics/dynamic-content.rst:5
msgid "Selecting dynamically-loaded content"
msgstr ""

#: ../../topics/dynamic-content.rst:7
msgid "Some webpages show the desired data when you load them in a web browser. However, when you download them using Scrapy, you cannot reach the desired data using :ref:`selectors <topics-selectors>`."
msgstr ""

#: ../../topics/dynamic-content.rst:11
msgid "When this happens, the recommended approach is to :ref:`find the data source <topics-finding-data-source>` and extract the data from it."
msgstr ""

#: ../../topics/dynamic-content.rst:15
msgid "If you fail to do that, and you can nonetheless access the desired data through the :ref:`DOM <topics-livedom>` from your web browser, see :ref:`topics-javascript-rendering`."
msgstr ""

#: ../../topics/dynamic-content.rst:22
msgid "Finding the data source"
msgstr ""

#: ../../topics/dynamic-content.rst:24
msgid "To extract the desired data, you must first find its source location."
msgstr ""

#: ../../topics/dynamic-content.rst:26
msgid "If the data is in a non-text-based format, such as an image or a PDF document, use the :ref:`network tool <topics-network-tool>` of your web browser to find the corresponding request, and :ref:`reproduce it <topics-reproducing-requests>`."
msgstr ""

#: ../../topics/dynamic-content.rst:31
msgid "If your web browser lets you select the desired data as text, the data may be defined in embedded JavaScript code, or loaded from an external resource in a text-based format."
msgstr ""

#: ../../topics/dynamic-content.rst:35
msgid "In that case, you can use a tool like wgrep_ to find the URL of that resource."
msgstr ""

#: ../../topics/dynamic-content.rst:37
msgid "If the data turns out to come from the original URL itself, you must :ref:`inspect the source code of the webpage <topics-inspecting-source>` to determine where the data is located."
msgstr ""

#: ../../topics/dynamic-content.rst:41
msgid "If the data comes from a different URL, you will need to :ref:`reproduce the corresponding request <topics-reproducing-requests>`."
msgstr ""

#: ../../topics/dynamic-content.rst:47
msgid "Inspecting the source code of a webpage"
msgstr ""

#: ../../topics/dynamic-content.rst:49
msgid "Sometimes you need to inspect the source code of a webpage (not the :ref:`DOM <topics-livedom>`) to determine where some desired data is located."
msgstr ""

#: ../../topics/dynamic-content.rst:52
msgid "Use Scrapyâ€™s :command:`fetch` command to download the webpage contents as seen by Scrapy::"
msgstr ""

#: ../../topics/dynamic-content.rst:57
msgid "If the desired data is in embedded JavaScript code within a ``<script/>`` element, see :ref:`topics-parsing-javascript`."
msgstr ""

#: ../../topics/dynamic-content.rst:60
msgid "If you cannot find the desired data, first make sure itâ€™s not just Scrapy: download the webpage with an HTTP client like curl_ or wget_ and see if the information can be found in the response they get."
msgstr ""

#: ../../topics/dynamic-content.rst:64
msgid "If they get a response with the desired data, modify your Scrapy :class:`~scrapy.http.Request` to match that of the other HTTP client. For example, try using the same user-agent string (:setting:`USER_AGENT`) or the same :attr:`~scrapy.http.Request.headers`."
msgstr ""

#: ../../topics/dynamic-content.rst:69
msgid "If they also get a response without the desired data, youâ€™ll need to take steps to make your request more similar to that of the web browser. See :ref:`topics-reproducing-requests`."
msgstr ""

#: ../../topics/dynamic-content.rst:76
msgid "Reproducing requests"
msgstr ""

#: ../../topics/dynamic-content.rst:78
msgid "Sometimes we need to reproduce a request the way our web browser performs it."
msgstr ""

#: ../../topics/dynamic-content.rst:80
msgid "Use the :ref:`network tool <topics-network-tool>` of your web browser to see how your web browser performs the desired request, and try to reproduce that request with Scrapy."
msgstr ""

#: ../../topics/dynamic-content.rst:84
msgid "It might be enough to yield a :class:`~scrapy.http.Request` with the same HTTP method and URL. However, you may also need to reproduce the body, headers and form parameters (see :class:`~scrapy.http.FormRequest`) of that request."
msgstr ""

#: ../../topics/dynamic-content.rst:88
msgid "As all major browsers allow to export the requests in `cURL <https://curl.haxx.se/>`_ format, Scrapy incorporates the method :meth:`~scrapy.http.Request.from_curl()` to generate an equivalent :class:`~scrapy.http.Request` from a cURL command. To get more information visit :ref:`request from curl <requests-from-curl>` inside the network tool section."
msgstr ""

#: ../../topics/dynamic-content.rst:95
msgid "Once you get the expected response, you can :ref:`extract the desired data from it <topics-handling-response-formats>`."
msgstr ""

#: ../../topics/dynamic-content.rst:98
msgid "You can reproduce any request with Scrapy. However, some times reproducing all necessary requests may not seem efficient in developer time. If that is your case, and crawling speed is not a major concern for you, you can alternatively consider :ref:`JavaScript pre-rendering <topics-javascript-rendering>`."
msgstr ""

#: ../../topics/dynamic-content.rst:103
msgid "If you get the expected response `sometimes`, but not always, the issue is probably not your request, but the target server. The target server might be buggy, overloaded, or :ref:`banning <bans>` some of your requests."
msgstr ""

#: ../../topics/dynamic-content.rst:113
msgid "Handling different response formats"
msgstr ""

#: ../../topics/dynamic-content.rst:115
msgid "Once you have a response with the desired data, how you extract the desired data from it depends on the type of response:"
msgstr ""

#: ../../topics/dynamic-content.rst:118
msgid "If the response is HTML or XML, use :ref:`selectors <topics-selectors>` as usual."
msgstr ""

#: ../../topics/dynamic-content.rst:121
msgid "If the response is JSON, use :func:`json.loads` to load the desired data from :attr:`response.text <scrapy.http.TextResponse.text>`::"
msgstr ""

#: ../../topics/dynamic-content.rst:126
msgid "If the desired data is inside HTML or XML code embedded within JSON data, you can load that HTML or XML code into a :class:`~scrapy.selector.Selector` and then :ref:`use it <topics-selectors>` as usual::"
msgstr ""

#: ../../topics/dynamic-content.rst:133
msgid "If the response is JavaScript, or HTML with a ``<script/>`` element containing the desired data, see :ref:`topics-parsing-javascript`."
msgstr ""

#: ../../topics/dynamic-content.rst:136
msgid "If the response is CSS, use a :doc:`regular expression <library/re>` to extract the desired data from :attr:`response.text <scrapy.http.TextResponse.text>`."
msgstr ""

#: ../../topics/dynamic-content.rst:142
msgid "If the response is an image or another format based on images (e.g. PDF), read the response as bytes from :attr:`response.body <scrapy.http.TextResponse.body>` and use an OCR solution to extract the desired data as text."
msgstr ""

#: ../../topics/dynamic-content.rst:147
msgid "For example, you can use pytesseract_. To read a table from a PDF, `tabula-py`_ may be a better choice."
msgstr ""

#: ../../topics/dynamic-content.rst:150
msgid "If the response is SVG, or HTML with embedded SVG containing the desired data, you may be able to extract the desired data using :ref:`selectors <topics-selectors>`, since SVG is based on XML."
msgstr ""

#: ../../topics/dynamic-content.rst:154
msgid "Otherwise, you might need to convert the SVG code into a raster image, and :ref:`handle that raster image <topics-parsing-images>`."
msgstr ""

#: ../../topics/dynamic-content.rst:160
msgid "Parsing JavaScript code"
msgstr ""

#: ../../topics/dynamic-content.rst:162
msgid "If the desired data is hardcoded in JavaScript, you first need to get the JavaScript code:"
msgstr ""

#: ../../topics/dynamic-content.rst:165
msgid "If the JavaScript code is in a JavaScript file, simply read :attr:`response.text <scrapy.http.TextResponse.text>`."
msgstr ""

#: ../../topics/dynamic-content.rst:168
msgid "If the JavaScript code is within a ``<script/>`` element of an HTML page, use :ref:`selectors <topics-selectors>` to extract the text within that ``<script/>`` element."
msgstr ""

#: ../../topics/dynamic-content.rst:172
msgid "Once you have a string with the JavaScript code, you can extract the desired data from it:"
msgstr ""

#: ../../topics/dynamic-content.rst:175
msgid "You might be able to use a :doc:`regular expression <library/re>` to extract the desired data in JSON format, which you can then parse with :func:`json.loads`."
msgstr ""

#: ../../topics/dynamic-content.rst:179
msgid "For example, if the JavaScript code contains a separate line like ``var data = {\"field\": \"value\"};`` you can extract that data as follows:"
msgstr ""

#: ../../topics/dynamic-content.rst:187
msgid "Otherwise, use js2xml_ to convert the JavaScript code into an XML document that you can parse using :ref:`selectors <topics-selectors>`."
msgstr ""

#: ../../topics/dynamic-content.rst:190
msgid "For example, if the JavaScript code contains ``var data = {field: \"value\"};`` you can extract that data as follows:"
msgstr ""

#: ../../topics/dynamic-content.rst:205
msgid "Pre-rendering JavaScript"
msgstr ""

#: ../../topics/dynamic-content.rst:207
msgid "On webpages that fetch data from additional requests, reproducing those requests that contain the desired data is the preferred approach. The effort is often worth the result: structured, complete data with minimum parsing time and network transfer."
msgstr ""

#: ../../topics/dynamic-content.rst:212
msgid "However, sometimes it can be really hard to reproduce certain requests. Or you may need something that no request can give you, such as a screenshot of a webpage as seen in a web browser."
msgstr ""

#: ../../topics/dynamic-content.rst:216
msgid "In these cases use the Splash_ JavaScript-rendering service, along with `scrapy-splash`_ for seamless integration."
msgstr ""

#: ../../topics/dynamic-content.rst:219
msgid "Splash returns as HTML the :ref:`DOM <topics-livedom>` of a webpage, so that you can parse it with :ref:`selectors <topics-selectors>`. It provides great flexibility through configuration_ or scripting_."
msgstr ""

#: ../../topics/dynamic-content.rst:223
msgid "If you need something beyond what Splash offers, such as interacting with the DOM on-the-fly from Python code instead of using a previously-written script, or handling multiple web browser windows, you might need to :ref:`use a headless browser <topics-headless-browsing>` instead."
msgstr ""

#: ../../topics/dynamic-content.rst:234
msgid "Using a headless browser"
msgstr ""

#: ../../topics/dynamic-content.rst:236
msgid "A `headless browser`_ is a special web browser that provides an API for automation."
msgstr ""

#: ../../topics/dynamic-content.rst:239
msgid "The easiest way to use a headless browser with Scrapy is to use Selenium_, along with `scrapy-selenium`_ for seamless integration."
msgstr ""

#: ../../topics/email.rst:5
msgid "Sending e-mail"
msgstr ""

#: ../../topics/email.rst:10
msgid "Although Python makes sending e-mails relatively easy via the :mod:`smtplib` library, Scrapy provides its own facility for sending e-mails which is very easy to use and it's implemented using :doc:`Twisted non-blocking IO <twisted:core/howto/defer-intro>`, to avoid interfering with the non-blocking IO of the crawler. It also provides a simple API for sending attachments and it's very easy to configure, with a few :ref:`settings <topics-email-settings>`."
msgstr ""

#: ../../topics/email.rst:19
msgid "Quick example"
msgstr ""

#: ../../topics/email.rst:21
msgid "There are two ways to instantiate the mail sender. You can instantiate it using the standard ``__init__`` method::"
msgstr ""

#: ../../topics/email.rst:27
msgid "Or you can instantiate it passing a Scrapy settings object, which will respect the :ref:`settings <topics-email-settings>`::"
msgstr ""

#: ../../topics/email.rst:32
msgid "And here is how to use it to send an e-mail (without attachments)::"
msgstr ""

#: ../../topics/email.rst:37
msgid "MailSender class reference"
msgstr ""

#: ../../topics/email.rst:39
msgid "MailSender is the preferred class to use for sending emails from Scrapy, as it uses :doc:`Twisted non-blocking IO <twisted:core/howto/defer-intro>`, like the rest of the framework."
msgstr ""

#: ../../topics/email.rst:45
msgid "the SMTP host to use for sending the emails. If omitted, the :setting:`MAIL_HOST` setting will be used."
msgstr ""

#: ../../topics/email.rst:49
msgid "the address used to send emails (in the ``From:`` header). If omitted, the :setting:`MAIL_FROM` setting will be used."
msgstr ""

#: ../../topics/email.rst:53
msgid "the SMTP user. If omitted, the :setting:`MAIL_USER` setting will be used. If not given, no SMTP authentication will be performed."
msgstr ""

#: ../../topics/email.rst:58
msgid "the SMTP pass for authentication."
msgstr ""

#: ../../topics/email.rst:61
msgid "the SMTP port to connect to"
msgstr ""

#: ../../topics/email.rst:64
msgid "enforce using SMTP STARTTLS"
msgstr ""

#: ../../topics/email.rst:67
msgid "enforce using a secure SSL connection"
msgstr ""

#: ../../topics/email.rst:72
msgid "Instantiate using a Scrapy settings object, which will respect :ref:`these Scrapy settings <topics-email-settings>`."
msgstr ""

#: ../../topics/email.rst:75
#: ../../topics/email.rst:82
msgid "the e-mail recipients"
msgstr ""

#: ../../topics/email.rst:80
msgid "Send email to the given recipients."
msgstr ""

#: ../../topics/email.rst:85
msgid "the subject of the e-mail"
msgstr ""

#: ../../topics/email.rst:88
msgid "the e-mails to CC"
msgstr ""

#: ../../topics/email.rst:91
msgid "the e-mail body"
msgstr ""

#: ../../topics/email.rst:94
msgid "an iterable of tuples ``(attach_name, mimetype, file_object)`` where  ``attach_name`` is a string with the name that will appear on the e-mail's attachment, ``mimetype`` is the mimetype of the attachment and ``file_object`` is a readable file object with the contents of the attachment"
msgstr ""

#: ../../topics/email.rst:101
msgid "the MIME type of the e-mail"
msgstr ""

#: ../../topics/email.rst:104
msgid "the character encoding to use for the e-mail contents"
msgstr ""

#: ../../topics/email.rst:111
msgid "Mail settings"
msgstr ""

#: ../../topics/email.rst:113
msgid "These settings define the default ``__init__`` method values of the :class:`MailSender` class, and can be used to configure e-mail notifications in your project without writing any code (for those extensions and code that uses :class:`MailSender`)."
msgstr ""

#: ../../topics/email.rst:120
msgid "MAIL_FROM"
msgstr ""

#: ../../topics/email.rst:122
msgid "Default: ``'scrapy@localhost'``"
msgstr ""

#: ../../topics/email.rst:124
msgid "Sender email to use (``From:`` header) for sending emails."
msgstr ""

#: ../../topics/email.rst:129
msgid "MAIL_HOST"
msgstr ""

#: ../../topics/email.rst:131
msgid "Default: ``'localhost'``"
msgstr ""

#: ../../topics/email.rst:133
msgid "SMTP host to use for sending emails."
msgstr ""

#: ../../topics/email.rst:138
msgid "MAIL_PORT"
msgstr ""

#: ../../topics/email.rst:140
msgid "Default: ``25``"
msgstr ""

#: ../../topics/email.rst:142
msgid "SMTP port to use for sending emails."
msgstr ""

#: ../../topics/email.rst:147
msgid "MAIL_USER"
msgstr ""

#: ../../topics/email.rst:149
#: ../../topics/email.rst:159
#: ../../topics/feed-exports.rst:280
#: ../../topics/feed-exports.rst:294
#: ../../topics/settings.rst:166
#: ../../topics/settings.rst:176
#: ../../topics/settings.rst:186
#: ../../topics/settings.rst:195
#: ../../topics/settings.rst:205
#: ../../topics/settings.rst:215
#: ../../topics/settings.rst:879
#: ../../topics/settings.rst:1166
#: ../../topics/settings.rst:1421
#: ../../topics/telnetconsole.rst:205
msgid "Default: ``None``"
msgstr ""

#: ../../topics/email.rst:151
msgid "User to use for SMTP authentication. If disabled no SMTP authentication will be performed."
msgstr ""

#: ../../topics/email.rst:157
msgid "MAIL_PASS"
msgstr ""

#: ../../topics/email.rst:161
msgid "Password to use for SMTP authentication, along with :setting:`MAIL_USER`."
msgstr ""

#: ../../topics/email.rst:166
msgid "MAIL_TLS"
msgstr ""

#: ../../topics/email.rst:170
msgid "Enforce using STARTTLS. STARTTLS is a way to take an existing insecure connection, and upgrade it to a secure connection using SSL/TLS."
msgstr ""

#: ../../topics/email.rst:175
msgid "MAIL_SSL"
msgstr ""

#: ../../topics/email.rst:179
msgid "Enforce connecting using an SSL encrypted connection"
msgstr ""

#: ../../topics/exceptions.rst:5
msgid "Exceptions"
msgstr ""

#: ../../topics/exceptions.rst:13
msgid "Built-in Exceptions reference"
msgstr ""

#: ../../topics/exceptions.rst:15
msgid "Here's a list of all exceptions included in Scrapy and their usage."
msgstr ""

#: ../../topics/exceptions.rst:18
msgid "DropItem"
msgstr ""

#: ../../topics/exceptions.rst:22
msgid "The exception that must be raised by item pipeline stages to stop processing an Item. For more information see :ref:`topics-item-pipeline`."
msgstr ""

#: ../../topics/exceptions.rst:26
msgid "CloseSpider"
msgstr ""

#: ../../topics/exceptions.rst:30
msgid "This exception can be raised from a spider callback to request the spider to be closed/stopped. Supported arguments:"
msgstr ""

#: ../../topics/exceptions.rst:33
msgid "the reason for closing"
msgstr ""

#: ../../topics/exceptions.rst:43
msgid "DontCloseSpider"
msgstr ""

#: ../../topics/exceptions.rst:47
msgid "This exception can be raised in a :signal:`spider_idle` signal handler to prevent the spider from being closed."
msgstr ""

#: ../../topics/exceptions.rst:51
msgid "IgnoreRequest"
msgstr ""

#: ../../topics/exceptions.rst:55
msgid "This exception can be raised by the Scheduler or any downloader middleware to indicate that the request should be ignored."
msgstr ""

#: ../../topics/exceptions.rst:59
msgid "NotConfigured"
msgstr ""

#: ../../topics/exceptions.rst:63
msgid "This exception can be raised by some components to indicate that they will remain disabled. Those components include:"
msgstr ""

#: ../../topics/exceptions.rst:66
#: ../../topics/extensions.rst:5
msgid "Extensions"
msgstr ""

#: ../../topics/exceptions.rst:67
msgid "Item pipelines"
msgstr ""

#: ../../topics/exceptions.rst:71
msgid "The exception must be raised in the component's ``__init__`` method."
msgstr ""

#: ../../topics/exceptions.rst:74
msgid "NotSupported"
msgstr ""

#: ../../topics/exceptions.rst:78
msgid "This exception is raised to indicate an unsupported feature."
msgstr ""

#: ../../topics/exporters.rst:5
msgid "Item Exporters"
msgstr ""

#: ../../topics/exporters.rst:10
msgid "Once you have scraped your items, you often want to persist or export those items, to use the data in some other application. That is, after all, the whole purpose of the scraping process."
msgstr ""

#: ../../topics/exporters.rst:14
msgid "For this purpose Scrapy provides a collection of Item Exporters for different output formats, such as XML, CSV or JSON."
msgstr ""

#: ../../topics/exporters.rst:18
msgid "Using Item Exporters"
msgstr ""

#: ../../topics/exporters.rst:20
msgid "If you are in a hurry, and just want to use an Item Exporter to output scraped data see the :ref:`topics-feed-exports`. Otherwise, if you want to know how Item Exporters work or need more custom functionality (not covered by the default exports), continue reading below."
msgstr ""

#: ../../topics/exporters.rst:25
msgid "In order to use an Item Exporter, you  must instantiate it with its required args. Each Item Exporter requires different arguments, so check each exporter documentation to be sure, in :ref:`topics-exporters-reference`. After you have instantiated your exporter, you have to:"
msgstr ""

#: ../../topics/exporters.rst:30
msgid "1. call the method :meth:`~BaseItemExporter.start_exporting` in order to signal the beginning of the exporting process"
msgstr ""

#: ../../topics/exporters.rst:33
msgid "2. call the :meth:`~BaseItemExporter.export_item` method for each item you want to export"
msgstr ""

#: ../../topics/exporters.rst:36
msgid "3. and finally call the :meth:`~BaseItemExporter.finish_exporting` to signal the end of the exporting process"
msgstr ""

#: ../../topics/exporters.rst:39
msgid "Here you can see an :doc:`Item Pipeline <item-pipeline>` which uses multiple Item Exporters to group scraped items to different files according to the value of one of their fields::"
msgstr ""

#: ../../topics/exporters.rst:73
msgid "Serialization of item fields"
msgstr ""

#: ../../topics/exporters.rst:75
msgid "By default, the field values are passed unmodified to the underlying serialization library, and the decision of how to serialize them is delegated to each particular serialization library."
msgstr ""

#: ../../topics/exporters.rst:79
msgid "However, you can customize how each field value is serialized *before it is passed to the serialization library*."
msgstr ""

#: ../../topics/exporters.rst:82
msgid "There are two ways to customize how a field will be serialized, which are described next."
msgstr ""

#: ../../topics/exporters.rst:88
msgid "1. Declaring a serializer in the field"
msgstr ""

#: ../../topics/exporters.rst:90
msgid "If you use :class:`~.Item` you can declare a serializer in the :ref:`field metadata <topics-items-fields>`. The serializer must be a callable which receives a value and returns its serialized form."
msgstr ""

#: ../../topics/exporters.rst:107
msgid "2. Overriding the serialize_field() method"
msgstr ""

#: ../../topics/exporters.rst:109
msgid "You can also override the :meth:`~BaseItemExporter.serialize_field()` method to customize how your field value will be exported."
msgstr ""

#: ../../topics/exporters.rst:112
msgid "Make sure you call the base class :meth:`~BaseItemExporter.serialize_field()` method after your custom code."
msgstr ""

#: ../../topics/exporters.rst:129
msgid "Built-in Item Exporters reference"
msgstr ""

#: ../../topics/exporters.rst:131
msgid "Here is a list of the Item Exporters bundled with Scrapy. Some of them contain output examples, which assume you're exporting these two items::"
msgstr ""

#: ../../topics/exporters.rst:138
msgid "BaseItemExporter"
msgstr ""

#: ../../topics/exporters.rst:142
msgid "This is the (abstract) base class for all Item Exporters. It provides support for common features used by all (concrete) Item Exporters, such as defining what fields to export, whether to export empty fields, or which encoding to use."
msgstr ""

#: ../../topics/exporters.rst:147
msgid "These features can be configured through the ``__init__`` method arguments which populate their respective instance attributes: :attr:`fields_to_export`, :attr:`export_empty_fields`, :attr:`encoding`, :attr:`indent`."
msgstr ""

#: ../../topics/exporters.rst:151
msgid "The *dont_fail* parameter."
msgstr ""

#: ../../topics/exporters.rst:156
msgid "Exports the given item. This method must be implemented in subclasses."
msgstr ""

#: ../../topics/exporters.rst:160
msgid "Return the serialized value for the given field. You can override this method (in your custom Item Exporters) if you want to control how a particular field or value will be serialized/exported."
msgstr ""

#: ../../topics/exporters.rst:164
msgid "By default, this method looks for a serializer :ref:`declared in the item field <topics-exporters-serializers>` and returns the result of applying that serializer to the value. If no serializer is found, it returns the value unchanged except for ``unicode`` values which are encoded to ``str`` using the encoding declared in the :attr:`encoding` attribute."
msgstr ""

#: ../../topics/exporters.rst:170
msgid "the field being serialized. If a raw dict is being exported (not :class:`~.Item`) *field* value is an empty dict."
msgstr ""

#: ../../topics/exporters.rst:174
msgid "the name of the field being serialized"
msgstr ""

#: ../../topics/exporters.rst:177
msgid "the value being serialized"
msgstr ""

#: ../../topics/exporters.rst:181
msgid "Signal the beginning of the exporting process. Some exporters may use this to generate some required header (for example, the :class:`XmlItemExporter`). You must call this method before exporting any items."
msgstr ""

#: ../../topics/exporters.rst:188
msgid "Signal the end of the exporting process. Some exporters may use this to generate some required footer (for example, the :class:`XmlItemExporter`). You must always call this method after you have no more items to export."
msgstr ""

#: ../../topics/exporters.rst:195
msgid "A list with the name of the fields that will be exported, or None if you want to export all fields. Defaults to None."
msgstr ""

#: ../../topics/exporters.rst:198
msgid "Some exporters (like :class:`CsvItemExporter`) respect the order of the fields defined in this attribute."
msgstr ""

#: ../../topics/exporters.rst:201
msgid "Some exporters may require fields_to_export list in order to export the data properly when spiders return dicts (not :class:`~Item` instances)."
msgstr ""

#: ../../topics/exporters.rst:206
msgid "Whether to include empty/unpopulated item fields in the exported data. Defaults to ``False``. Some exporters (like :class:`CsvItemExporter`) ignore this attribute and always export all empty fields."
msgstr ""

#: ../../topics/exporters.rst:210
msgid "This option is ignored for dict items."
msgstr ""

#: ../../topics/exporters.rst:214
msgid "The encoding that will be used to encode unicode values. This only affects unicode values (which are always serialized to str using this encoding). Other value types are passed unchanged to the specific serialization library."
msgstr ""

#: ../../topics/exporters.rst:221
msgid "Amount of spaces used to indent the output on each level. Defaults to ``0``."
msgstr ""

#: ../../topics/exporters.rst:223
msgid "``indent=None`` selects the most compact representation, all items in the same line with no indentation"
msgstr ""

#: ../../topics/exporters.rst:225
msgid "``indent<=0`` each item on its own line, no indentation"
msgstr ""

#: ../../topics/exporters.rst:226
msgid "``indent>0`` each item on its own line, indented with the provided numeric value"
msgstr ""

#: ../../topics/exporters.rst:229
msgid "PythonItemExporter"
msgstr ""

#: ../../../scrapy/exporters.py:docstring of scrapy.exporters.PythonItemExporter:1
msgid "This is a base class for item exporters that extends :class:`BaseItemExporter` with support for nested items."
msgstr ""

#: ../../../scrapy/exporters.py:docstring of scrapy.exporters.PythonItemExporter:4
msgid "It serializes items to built-in Python types, so that any serialization library (e.g. :mod:`json` or msgpack_) can be used on top of it."
msgstr ""

#: ../../topics/exporters.rst:237
msgid "XmlItemExporter"
msgstr ""

#: ../../topics/exporters.rst:241
msgid "Exports Items in XML format to the specified file object."
msgstr ""

#: ../../topics/exporters.rst:243
#: ../../topics/exporters.rst:300
#: ../../topics/exporters.rst:330
#: ../../topics/exporters.rst:350
#: ../../topics/exporters.rst:374
#: ../../topics/exporters.rst:403
msgid "the file-like object to use for exporting the data. Its ``write`` method should accept ``bytes`` (a disk file opened in binary mode, a ``io.BytesIO`` object, etc)"
msgstr ""

#: ../../topics/exporters.rst:246
msgid "The name of root element in the exported XML."
msgstr ""

#: ../../topics/exporters.rst:249
msgid "The name of each item element in the exported XML."
msgstr ""

#: ../../topics/exporters.rst:252
#: ../../topics/exporters.rst:338
#: ../../topics/exporters.rst:353
msgid "The additional keyword arguments of this ``__init__`` method are passed to the :class:`BaseItemExporter` ``__init__`` method."
msgstr ""

#: ../../topics/exporters.rst:255
#: ../../topics/exporters.rst:317
#: ../../topics/exporters.rst:356
#: ../../topics/exporters.rst:377
#: ../../topics/exporters.rst:406
msgid "A typical output of this exporter would be::"
msgstr ""

#: ../../topics/exporters.rst:269
msgid "Unless overridden in the :meth:`serialize_field` method, multi-valued fields are exported by serializing each value inside a ``<value>`` element. This is for convenience, as multi-valued fields are very common."
msgstr ""

#: ../../topics/exporters.rst:273
msgid "For example, the item::"
msgstr ""

#: ../../topics/exporters.rst:277
msgid "Would be serialized as::"
msgstr ""

#: ../../topics/exporters.rst:291
msgid "CsvItemExporter"
msgstr ""

#: ../../topics/exporters.rst:295
msgid "Exports Items in CSV format to the given file-like object. If the :attr:`fields_to_export` attribute is set, it will be used to define the CSV columns and their order. The :attr:`export_empty_fields` attribute has no effect on this exporter."
msgstr ""

#: ../../topics/exporters.rst:303
msgid "If enabled, makes the exporter output a header line with the field names taken from :attr:`BaseItemExporter.fields_to_export` or the first exported item fields."
msgstr ""

#: ../../topics/exporters.rst:308
msgid "The char (or chars) that will be used for joining multi-valued fields, if found."
msgstr ""

#: ../../topics/exporters.rst:312
msgid "The additional keyword arguments of this ``__init__`` method are passed to the :class:`BaseItemExporter` ``__init__`` method, and the leftover arguments to the :func:`csv.writer` function, so you can use any :func:`csv.writer` function argument to customize this exporter."
msgstr ""

#: ../../topics/exporters.rst:324
msgid "PickleItemExporter"
msgstr ""

#: ../../topics/exporters.rst:328
msgid "Exports Items in pickle format to the given file-like object."
msgstr ""

#: ../../topics/exporters.rst:333
msgid "The pickle protocol to use."
msgstr ""

#: ../../topics/exporters.rst:336
msgid "For more information, see :mod:`pickle`."
msgstr ""

#: ../../topics/exporters.rst:341
msgid "Pickle isn't a human readable format, so no output examples are provided."
msgstr ""

#: ../../topics/exporters.rst:344
msgid "PprintItemExporter"
msgstr ""

#: ../../topics/exporters.rst:348
msgid "Exports Items in pretty print format to the specified file object."
msgstr ""

#: ../../topics/exporters.rst:361
msgid "Longer lines (when present) are pretty-formatted."
msgstr ""

#: ../../topics/exporters.rst:364
msgid "JsonItemExporter"
msgstr ""

#: ../../topics/exporters.rst:368
msgid "Exports Items in JSON format to the specified file-like object, writing all objects as a list of objects. The additional ``__init__`` method arguments are passed to the :class:`BaseItemExporter` ``__init__`` method, and the leftover arguments to the :class:`~json.JSONEncoder` ``__init__`` method, so you can use any :class:`~json.JSONEncoder` ``__init__`` method argument to customize this exporter."
msgstr ""

#: ../../topics/exporters.rst:384
msgid "JSON is very simple and flexible serialization format, but it doesn't scale well for large amounts of data since incremental (aka. stream-mode) parsing is not well supported (if at all) among JSON parsers (on any language), and most of them just parse the entire object in memory. If you want the power and simplicity of JSON with a more stream-friendly format, consider using :class:`JsonLinesItemExporter` instead, or splitting the output in multiple chunks."
msgstr ""

#: ../../topics/exporters.rst:393
msgid "JsonLinesItemExporter"
msgstr ""

#: ../../topics/exporters.rst:397
msgid "Exports Items in JSON format to the specified file-like object, writing one JSON-encoded item per line. The additional ``__init__`` method arguments are passed to the :class:`BaseItemExporter` ``__init__`` method, and the leftover arguments to the :class:`~json.JSONEncoder` ``__init__`` method, so you can use any :class:`~json.JSONEncoder` ``__init__`` method argument to customize this exporter."
msgstr ""

#: ../../topics/exporters.rst:411
msgid "Unlike the one produced by :class:`JsonItemExporter`, the format produced by this exporter is well suited for serializing large amounts of data."
msgstr ""

#: ../../topics/exporters.rst:415
msgid "MarshalItemExporter"
msgstr ""

#: ../../../scrapy/exporters.py:docstring of scrapy.exporters.MarshalItemExporter:1
msgid "Exports items in a Python-specific binary format (see :mod:`marshal`)."
msgstr ""

#: ../../../scrapy/exporters.py:docstring of scrapy.exporters.MarshalItemExporter:4
msgid "The file-like object to use for exporting the data. Its ``write`` method should accept :class:`bytes` (a disk file opened in binary mode, a :class:`~io.BytesIO` object, etc)"
msgstr ""

#: ../../topics/extensions.rst:7
msgid "The extensions framework provides a mechanism for inserting your own custom functionality into Scrapy."
msgstr ""

#: ../../topics/extensions.rst:10
msgid "Extensions are just regular classes that are instantiated at Scrapy startup, when extensions are initialized."
msgstr ""

#: ../../topics/extensions.rst:14
msgid "Extension settings"
msgstr ""

#: ../../topics/extensions.rst:16
msgid "Extensions use the :ref:`Scrapy settings <topics-settings>` to manage their settings, just like any other Scrapy code."
msgstr ""

#: ../../topics/extensions.rst:19
msgid "It is customary for extensions to prefix their settings with their own name, to avoid collision with existing (and future) extensions. For example, a hypothetic extension to handle `Google Sitemaps`_ would use settings like ``GOOGLESITEMAP_ENABLED``, ``GOOGLESITEMAP_DEPTH``, and so on."
msgstr ""

#: ../../topics/extensions.rst:27
msgid "Loading & activating extensions"
msgstr ""

#: ../../topics/extensions.rst:29
msgid "Extensions are loaded and activated at startup by instantiating a single instance of the extension class. Therefore, all the extension initialization code must be performed in the class ``__init__`` method."
msgstr ""

#: ../../topics/extensions.rst:33
msgid "To make an extension available, add it to the :setting:`EXTENSIONS` setting in your Scrapy settings. In :setting:`EXTENSIONS`, each extension is represented by a string: the full Python path to the extension's class name. For example::"
msgstr ""

#: ../../topics/extensions.rst:43
msgid "As you can see, the :setting:`EXTENSIONS` setting is a dict where the keys are the extension paths, and their values are the orders, which define the extension *loading* order. The :setting:`EXTENSIONS` setting is merged with the :setting:`EXTENSIONS_BASE` setting defined in Scrapy (and not meant to be overridden) and then sorted by order to get the final sorted list of enabled extensions."
msgstr ""

#: ../../topics/extensions.rst:50
msgid "As extensions typically do not depend on each other, their loading order is irrelevant in most cases. This is why the :setting:`EXTENSIONS_BASE` setting defines all extensions with the same order (``0``). However, this feature can be exploited if you need to add an extension which depends on other extensions already loaded."
msgstr ""

#: ../../topics/extensions.rst:57
msgid "Available, enabled and disabled extensions"
msgstr ""

#: ../../topics/extensions.rst:59
msgid "Not all available extensions will be enabled. Some of them usually depend on a particular setting. For example, the HTTP Cache extension is available by default but disabled unless the :setting:`HTTPCACHE_ENABLED` setting is set."
msgstr ""

#: ../../topics/extensions.rst:64
msgid "Disabling an extension"
msgstr ""

#: ../../topics/extensions.rst:66
msgid "In order to disable an extension that comes enabled by default (i.e. those included in the :setting:`EXTENSIONS_BASE` setting) you must set its order to ``None``. For example::"
msgstr ""

#: ../../topics/extensions.rst:75
msgid "Writing your own extension"
msgstr ""

#: ../../topics/extensions.rst:77
msgid "Each extension is a Python class. The main entry point for a Scrapy extension (this also includes middlewares and pipelines) is the ``from_crawler`` class method which receives a ``Crawler`` instance. Through the Crawler object you can access settings, signals, stats, and also control the crawling behaviour."
msgstr ""

#: ../../topics/extensions.rst:82
msgid "Typically, extensions connect to :ref:`signals <topics-signals>` and perform tasks triggered by them."
msgstr ""

#: ../../topics/extensions.rst:85
msgid "Finally, if the ``from_crawler`` method raises the :exc:`~scrapy.exceptions.NotConfigured` exception, the extension will be disabled. Otherwise, the extension will be enabled."
msgstr ""

#: ../../topics/extensions.rst:90
msgid "Sample extension"
msgstr ""

#: ../../topics/extensions.rst:92
msgid "Here we will implement a simple extension to illustrate the concepts described in the previous section. This extension will log a message every time:"
msgstr ""

#: ../../topics/extensions.rst:95
msgid "a spider is opened"
msgstr ""

#: ../../topics/extensions.rst:96
msgid "a spider is closed"
msgstr ""

#: ../../topics/extensions.rst:97
msgid "a specific number of items are scraped"
msgstr ""

#: ../../topics/extensions.rst:99
msgid "The extension will be enabled through the ``MYEXT_ENABLED`` setting and the number of items will be specified through the ``MYEXT_ITEMCOUNT`` setting."
msgstr ""

#: ../../topics/extensions.rst:102
msgid "Here is the code of such extension::"
msgstr ""

#: ../../topics/extensions.rst:152
msgid "Built-in extensions reference"
msgstr ""

#: ../../topics/extensions.rst:155
msgid "General purpose extensions"
msgstr ""

#: ../../topics/extensions.rst:158
msgid "Log Stats extension"
msgstr ""

#: ../../topics/extensions.rst:165
msgid "Log basic stats like crawled pages and scraped items."
msgstr ""

#: ../../topics/extensions.rst:168
msgid "Core Stats extension"
msgstr ""

#: ../../topics/extensions.rst:175
msgid "Enable the collection of core statistics, provided the stats collection is enabled (see :ref:`topics-stats`)."
msgstr ""

#: ../../topics/extensions.rst:181
msgid "Telnet console extension"
msgstr ""

#: ../../topics/extensions.rst:188
msgid "Provides a telnet console for getting into a Python interpreter inside the currently running Scrapy process, which can be very useful for debugging."
msgstr ""

#: ../../topics/extensions.rst:191
msgid "The telnet console must be enabled by the :setting:`TELNETCONSOLE_ENABLED` setting, and the server will listen in the port specified in :setting:`TELNETCONSOLE_PORT`."
msgstr ""

#: ../../topics/extensions.rst:198
msgid "Memory usage extension"
msgstr ""

#: ../../topics/extensions.rst:205
msgid "This extension does not work in Windows."
msgstr ""

#: ../../topics/extensions.rst:207
msgid "Monitors the memory used by the Scrapy process that runs the spider and:"
msgstr ""

#: ../../topics/extensions.rst:209
msgid "sends a notification e-mail when it exceeds a certain value"
msgstr ""

#: ../../topics/extensions.rst:210
msgid "closes the spider when it exceeds a certain value"
msgstr ""

#: ../../topics/extensions.rst:212
msgid "The notification e-mails can be triggered when a certain warning value is reached (:setting:`MEMUSAGE_WARNING_MB`) and when the maximum value is reached (:setting:`MEMUSAGE_LIMIT_MB`) which will also cause the spider to be closed and the Scrapy process to be terminated."
msgstr ""

#: ../../topics/extensions.rst:217
msgid "This extension is enabled by the :setting:`MEMUSAGE_ENABLED` setting and can be configured with the following settings:"
msgstr ""

#: ../../topics/extensions.rst:220
msgid ":setting:`MEMUSAGE_LIMIT_MB`"
msgstr ""

#: ../../topics/extensions.rst:221
msgid ":setting:`MEMUSAGE_WARNING_MB`"
msgstr ""

#: ../../topics/extensions.rst:222
msgid ":setting:`MEMUSAGE_NOTIFY_MAIL`"
msgstr ""

#: ../../topics/extensions.rst:223
msgid ":setting:`MEMUSAGE_CHECK_INTERVAL_SECONDS`"
msgstr ""

#: ../../topics/extensions.rst:226
msgid "Memory debugger extension"
msgstr ""

#: ../../topics/extensions.rst:233
msgid "An extension for debugging memory usage. It collects information about:"
msgstr ""

#: ../../topics/extensions.rst:235
msgid "objects uncollected by the Python garbage collector"
msgstr ""

#: ../../topics/extensions.rst:236
msgid "objects left alive that shouldn't. For more info, see :ref:`topics-leaks-trackrefs`"
msgstr ""

#: ../../topics/extensions.rst:238
msgid "To enable this extension, turn on the :setting:`MEMDEBUG_ENABLED` setting. The info will be stored in the stats."
msgstr ""

#: ../../topics/extensions.rst:242
msgid "Close spider extension"
msgstr ""

#: ../../topics/extensions.rst:249
msgid "Closes a spider automatically when some conditions are met, using a specific closing reason for each condition."
msgstr ""

#: ../../topics/extensions.rst:252
msgid "The conditions for closing a spider can be configured through the following settings:"
msgstr ""

#: ../../topics/extensions.rst:255
msgid ":setting:`CLOSESPIDER_TIMEOUT`"
msgstr ""

#: ../../topics/extensions.rst:256
msgid ":setting:`CLOSESPIDER_ITEMCOUNT`"
msgstr ""

#: ../../topics/extensions.rst:257
msgid ":setting:`CLOSESPIDER_PAGECOUNT`"
msgstr ""

#: ../../topics/extensions.rst:258
msgid ":setting:`CLOSESPIDER_ERRORCOUNT`"
msgstr ""

#: ../../topics/extensions.rst:263
msgid "CLOSESPIDER_TIMEOUT"
msgstr ""

#: ../../topics/extensions.rst:267
msgid "An integer which specifies a number of seconds. If the spider remains open for more than that number of second, it will be automatically closed with the reason ``closespider_timeout``. If zero (or non set), spiders won't be closed by timeout."
msgstr ""

#: ../../topics/extensions.rst:275
msgid "CLOSESPIDER_ITEMCOUNT"
msgstr ""

#: ../../topics/extensions.rst:279
msgid "An integer which specifies a number of items. If the spider scrapes more than that amount and those items are passed by the item pipeline, the spider will be closed with the reason ``closespider_itemcount``. Requests which  are currently in the downloader queue (up to :setting:`CONCURRENT_REQUESTS` requests) are still processed. If zero (or non set), spiders won't be closed by number of passed items."
msgstr ""

#: ../../topics/extensions.rst:289
msgid "CLOSESPIDER_PAGECOUNT"
msgstr ""

#: ../../topics/extensions.rst:295
msgid "An integer which specifies the maximum number of responses to crawl. If the spider crawls more than that, the spider will be closed with the reason ``closespider_pagecount``. If zero (or non set), spiders won't be closed by number of crawled responses."
msgstr ""

#: ../../topics/extensions.rst:303
msgid "CLOSESPIDER_ERRORCOUNT"
msgstr ""

#: ../../topics/extensions.rst:309
msgid "An integer which specifies the maximum number of errors to receive before closing the spider. If the spider generates more than that number of errors, it will be closed with the reason ``closespider_errorcount``. If zero (or non set), spiders won't be closed by number of errors."
msgstr ""

#: ../../topics/extensions.rst:315
msgid "StatsMailer extension"
msgstr ""

#: ../../topics/extensions.rst:322
msgid "This simple extension can be used to send a notification e-mail every time a domain has finished scraping, including the Scrapy stats collected. The email will be sent to all recipients specified in the :setting:`STATSMAILER_RCPTS` setting."
msgstr ""

#: ../../topics/extensions.rst:331
msgid "Debugging extensions"
msgstr ""

#: ../../topics/extensions.rst:334
msgid "Stack trace dump extension"
msgstr ""

#: ../../topics/extensions.rst:338
msgid "Dumps information about the running process when a `SIGQUIT`_ or `SIGUSR2`_ signal is received. The information dumped is the following:"
msgstr ""

#: ../../topics/extensions.rst:341
msgid "engine status (using ``scrapy.utils.engine.get_engine_status()``)"
msgstr ""

#: ../../topics/extensions.rst:342
msgid "live references (see :ref:`topics-leaks-trackrefs`)"
msgstr ""

#: ../../topics/extensions.rst:343
msgid "stack trace of all threads"
msgstr ""

#: ../../topics/extensions.rst:345
msgid "After the stack trace and engine status is dumped, the Scrapy process continues running normally."
msgstr ""

#: ../../topics/extensions.rst:348
msgid "This extension only works on POSIX-compliant platforms (i.e. not Windows), because the `SIGQUIT`_ and `SIGUSR2`_ signals are not available on Windows."
msgstr ""

#: ../../topics/extensions.rst:351
msgid "There are at least two ways to send Scrapy the `SIGQUIT`_ signal:"
msgstr ""

#: ../../topics/extensions.rst:353
msgid "By pressing Ctrl-\\ while a Scrapy process is running (Linux only?)"
msgstr ""

#: ../../topics/extensions.rst:354
msgid "By running this command (assuming ``<pid>`` is the process id of the Scrapy process)::"
msgstr ""

#: ../../topics/extensions.rst:363
msgid "Debugger extension"
msgstr ""

#: ../../topics/extensions.rst:367
msgid "Invokes a :doc:`Python debugger <library/pdb>` inside a running Scrapy process when a `SIGUSR2`_ signal is received. After the debugger is exited, the Scrapy process continues running normally."
msgstr ""

#: ../../topics/extensions.rst:371
msgid "For more info see `Debugging in Python`_."
msgstr ""

#: ../../topics/extensions.rst:373
msgid "This extension only works on POSIX-compliant platforms (i.e. not Windows)."
msgstr ""

#: ../../topics/feed-exports.rst:5
msgid "Feed exports"
msgstr ""

#: ../../topics/feed-exports.rst:9
msgid "One of the most frequently required features when implementing scrapers is being able to store the scraped data properly and, quite often, that means generating an \"export file\" with the scraped data (commonly called \"export feed\") to be consumed by other systems."
msgstr ""

#: ../../topics/feed-exports.rst:14
msgid "Scrapy provides this functionality out of the box with the Feed Exports, which allows you to generate feeds with the scraped items, using multiple serialization formats and storage backends."
msgstr ""

#: ../../topics/feed-exports.rst:21
msgid "Serialization formats"
msgstr ""

#: ../../topics/feed-exports.rst:23
msgid "For serializing the scraped data, the feed exports use the :ref:`Item exporters <topics-exporters>`. These formats are supported out of the box:"
msgstr ""

#: ../../topics/feed-exports.rst:26
msgid ":ref:`topics-feed-format-json`"
msgstr ""

#: ../../topics/feed-exports.rst:27
msgid ":ref:`topics-feed-format-jsonlines`"
msgstr ""

#: ../../topics/feed-exports.rst:28
msgid ":ref:`topics-feed-format-csv`"
msgstr ""

#: ../../topics/feed-exports.rst:29
msgid ":ref:`topics-feed-format-xml`"
msgstr ""

#: ../../topics/feed-exports.rst:31
msgid "But you can also extend the supported format through the :setting:`FEED_EXPORTERS` setting."
msgstr ""

#: ../../topics/feed-exports.rst:37
msgid "JSON"
msgstr ""

#: ../../topics/feed-exports.rst:39
msgid "Value for the ``format`` key in the :setting:`FEEDS` setting: ``json``"
msgstr ""

#: ../../topics/feed-exports.rst:40
msgid "Exporter used: :class:`~scrapy.exporters.JsonItemExporter`"
msgstr ""

#: ../../topics/feed-exports.rst:41
msgid "See :ref:`this warning <json-with-large-data>` if you're using JSON with large feeds."
msgstr ""

#: ../../topics/feed-exports.rst:47
msgid "JSON lines"
msgstr ""

#: ../../topics/feed-exports.rst:49
msgid "Value for the ``format`` key in the :setting:`FEEDS` setting: ``jsonlines``"
msgstr ""

#: ../../topics/feed-exports.rst:50
msgid "Exporter used: :class:`~scrapy.exporters.JsonLinesItemExporter`"
msgstr ""

#: ../../topics/feed-exports.rst:55
msgid "CSV"
msgstr ""

#: ../../topics/feed-exports.rst:57
msgid "Value for the ``format`` key in the :setting:`FEEDS` setting: ``csv``"
msgstr ""

#: ../../topics/feed-exports.rst:58
msgid "Exporter used: :class:`~scrapy.exporters.CsvItemExporter`"
msgstr ""

#: ../../topics/feed-exports.rst:59
msgid "To specify columns to export and their order use :setting:`FEED_EXPORT_FIELDS`. Other feed exporters can also use this option, but it is important for CSV because unlike many other export formats CSV uses a fixed header."
msgstr ""

#: ../../topics/feed-exports.rst:67
msgid "XML"
msgstr ""

#: ../../topics/feed-exports.rst:69
msgid "Value for the ``format`` key in the :setting:`FEEDS` setting: ``xml``"
msgstr ""

#: ../../topics/feed-exports.rst:70
msgid "Exporter used: :class:`~scrapy.exporters.XmlItemExporter`"
msgstr ""

#: ../../topics/feed-exports.rst:75
msgid "Pickle"
msgstr ""

#: ../../topics/feed-exports.rst:77
msgid "Value for the ``format`` key in the :setting:`FEEDS` setting: ``pickle``"
msgstr ""

#: ../../topics/feed-exports.rst:78
msgid "Exporter used: :class:`~scrapy.exporters.PickleItemExporter`"
msgstr ""

#: ../../topics/feed-exports.rst:83
msgid "Marshal"
msgstr ""

#: ../../topics/feed-exports.rst:85
msgid "Value for the ``format`` key in the :setting:`FEEDS` setting: ``marshal``"
msgstr ""

#: ../../topics/feed-exports.rst:86
msgid "Exporter used: :class:`~scrapy.exporters.MarshalItemExporter`"
msgstr ""

#: ../../topics/feed-exports.rst:92
msgid "Storages"
msgstr ""

#: ../../topics/feed-exports.rst:94
msgid "When using the feed exports you define where to store the feed using one or multiple URIs_ (through the :setting:`FEEDS` setting). The feed exports supports multiple storage backend types which are defined by the URI scheme."
msgstr ""

#: ../../topics/feed-exports.rst:98
msgid "The storages backends supported out of the box are:"
msgstr ""

#: ../../topics/feed-exports.rst:100
msgid ":ref:`topics-feed-storage-fs`"
msgstr ""

#: ../../topics/feed-exports.rst:101
msgid ":ref:`topics-feed-storage-ftp`"
msgstr ""

#: ../../topics/feed-exports.rst:102
msgid ":ref:`topics-feed-storage-s3` (requires botocore_)"
msgstr ""

#: ../../topics/feed-exports.rst:103
msgid ":ref:`topics-feed-storage-stdout`"
msgstr ""

#: ../../topics/feed-exports.rst:105
msgid "Some storage backends may be unavailable if the required external libraries are not available. For example, the S3 backend is only available if the botocore_ library is installed."
msgstr ""

#: ../../topics/feed-exports.rst:113
msgid "Storage URI parameters"
msgstr ""

#: ../../topics/feed-exports.rst:115
msgid "The storage URI can also contain parameters that get replaced when the feed is being created. These parameters are:"
msgstr ""

#: ../../topics/feed-exports.rst:118
msgid "``%(time)s`` - gets replaced by a timestamp when the feed is being created"
msgstr ""

#: ../../topics/feed-exports.rst:119
msgid "``%(name)s`` - gets replaced by the spider name"
msgstr ""

#: ../../topics/feed-exports.rst:121
msgid "Any other named parameter gets replaced by the spider attribute of the same name. For example, ``%(site_id)s`` would get replaced by the ``spider.site_id`` attribute the moment the feed is being created."
msgstr ""

#: ../../topics/feed-exports.rst:125
msgid "Here are some examples to illustrate:"
msgstr ""

#: ../../topics/feed-exports.rst:127
msgid "Store in FTP using one directory per spider:"
msgstr ""

#: ../../topics/feed-exports.rst:129
msgid "``ftp://user:password@ftp.example.com/scraping/feeds/%(name)s/%(time)s.json``"
msgstr ""

#: ../../topics/feed-exports.rst:131
msgid "Store in S3 using one directory per spider:"
msgstr ""

#: ../../topics/feed-exports.rst:133
msgid "``s3://mybucket/scraping/feeds/%(name)s/%(time)s.json``"
msgstr ""

#: ../../topics/feed-exports.rst:139
msgid "Storage backends"
msgstr ""

#: ../../topics/feed-exports.rst:144
msgid "Local filesystem"
msgstr ""

#: ../../topics/feed-exports.rst:146
msgid "The feeds are stored in the local filesystem."
msgstr ""

#: ../../topics/feed-exports.rst:148
msgid "URI scheme: ``file``"
msgstr ""

#: ../../topics/feed-exports.rst:149
msgid "Example URI: ``file:///tmp/export.csv``"
msgstr ""

#: ../../topics/feed-exports.rst:150
#: ../../topics/feed-exports.rst:165
#: ../../topics/feed-exports.rst:206
msgid "Required external libraries: none"
msgstr ""

#: ../../topics/feed-exports.rst:152
msgid "Note that for the local filesystem storage (only) you can omit the scheme if you specify an absolute path like ``/tmp/export.csv``. This only works on Unix systems though."
msgstr ""

#: ../../topics/feed-exports.rst:159
msgid "FTP"
msgstr ""

#: ../../topics/feed-exports.rst:161
msgid "The feeds are stored in a FTP server."
msgstr ""

#: ../../topics/feed-exports.rst:163
msgid "URI scheme: ``ftp``"
msgstr ""

#: ../../topics/feed-exports.rst:164
msgid "Example URI: ``ftp://user:pass@ftp.example.com/path/to/export.csv``"
msgstr ""

#: ../../topics/feed-exports.rst:167
msgid "FTP supports two different connection modes: `active or passive <https://stackoverflow.com/a/1699163>`_. Scrapy uses the passive connection mode by default. To use the active connection mode instead, set the :setting:`FEED_STORAGE_FTP_ACTIVE` setting to ``True``."
msgstr ""

#: ../../topics/feed-exports.rst:175
msgid "S3"
msgstr ""

#: ../../topics/feed-exports.rst:177
msgid "The feeds are stored on `Amazon S3`_."
msgstr ""

#: ../../topics/feed-exports.rst:179
msgid "URI scheme: ``s3``"
msgstr ""

#: ../../topics/feed-exports.rst:180
msgid "Example URIs:"
msgstr ""

#: ../../topics/feed-exports.rst:182
msgid "``s3://mybucket/path/to/export.csv``"
msgstr ""

#: ../../topics/feed-exports.rst:183
msgid "``s3://aws_key:aws_secret@mybucket/path/to/export.csv``"
msgstr ""

#: ../../topics/feed-exports.rst:185
msgid "Required external libraries: `botocore`_"
msgstr ""

#: ../../topics/feed-exports.rst:187
msgid "The AWS credentials can be passed as user/password in the URI, or they can be passed through the following settings:"
msgstr ""

#: ../../topics/feed-exports.rst:190
msgid ":setting:`AWS_ACCESS_KEY_ID`"
msgstr ""

#: ../../topics/feed-exports.rst:191
msgid ":setting:`AWS_SECRET_ACCESS_KEY`"
msgstr ""

#: ../../topics/feed-exports.rst:193
msgid "You can also define a custom ACL for exported feeds using this setting:"
msgstr ""

#: ../../topics/feed-exports.rst:195
#: ../../topics/feed-exports.rst:221
msgid ":setting:`FEED_STORAGE_S3_ACL`"
msgstr ""

#: ../../topics/feed-exports.rst:200
msgid "Standard output"
msgstr ""

#: ../../topics/feed-exports.rst:202
msgid "The feeds are written to the standard output of the Scrapy process."
msgstr ""

#: ../../topics/feed-exports.rst:204
msgid "URI scheme: ``stdout``"
msgstr ""

#: ../../topics/feed-exports.rst:205
msgid "Example URI: ``stdout:``"
msgstr ""

#: ../../topics/feed-exports.rst:212
msgid "These are the settings used for configuring the feed exports:"
msgstr ""

#: ../../topics/feed-exports.rst:214
msgid ":setting:`FEEDS` (mandatory)"
msgstr ""

#: ../../topics/feed-exports.rst:215
msgid ":setting:`FEED_EXPORT_ENCODING`"
msgstr ""

#: ../../topics/feed-exports.rst:216
msgid ":setting:`FEED_STORE_EMPTY`"
msgstr ""

#: ../../topics/feed-exports.rst:217
msgid ":setting:`FEED_EXPORT_FIELDS`"
msgstr ""

#: ../../topics/feed-exports.rst:218
msgid ":setting:`FEED_EXPORT_INDENT`"
msgstr ""

#: ../../topics/feed-exports.rst:219
msgid ":setting:`FEED_STORAGES`"
msgstr ""

#: ../../topics/feed-exports.rst:220
msgid ":setting:`FEED_STORAGE_FTP_ACTIVE`"
msgstr ""

#: ../../topics/feed-exports.rst:222
msgid ":setting:`FEED_EXPORTERS`"
msgstr ""

#: ../../topics/feed-exports.rst:229
msgid "FEEDS"
msgstr ""

#: ../../topics/feed-exports.rst:233
#: ../../topics/feed-exports.rst:339
#: ../../topics/feed-exports.rst:397
#: ../../topics/settings.rst:590
#: ../../topics/settings.rst:833
#: ../../topics/settings.rst:851
msgid "Default: ``{}``"
msgstr ""

#: ../../topics/feed-exports.rst:235
msgid "A dictionary in which every key is a feed URI (or a :class:`pathlib.Path` object) and each value is a nested dictionary containing configuration parameters for the specific feed. This setting is required for enabling the feed export feature."
msgstr ""

#: ../../topics/feed-exports.rst:240
msgid "See :ref:`topics-feed-storage-backends` for supported URI schemes."
msgstr ""

#: ../../topics/feed-exports.rst:242
msgid "For instance::"
msgstr ""

#: ../../topics/feed-exports.rst:264
msgid "The following is a list of the accepted keys and the setting that is used as a fallback value if that key is not provided for a specific feed definition."
msgstr ""

#: ../../topics/feed-exports.rst:267
msgid "``format``: the serialization format to be used for the feed. See :ref:`topics-feed-format` for possible values. Mandatory, no fallback setting"
msgstr ""

#: ../../topics/feed-exports.rst:270
msgid "``encoding``: falls back to :setting:`FEED_EXPORT_ENCODING`"
msgstr ""

#: ../../topics/feed-exports.rst:271
msgid "``fields``: falls back to :setting:`FEED_EXPORT_FIELDS`"
msgstr ""

#: ../../topics/feed-exports.rst:272
msgid "``indent``: falls back to :setting:`FEED_EXPORT_INDENT`"
msgstr ""

#: ../../topics/feed-exports.rst:273
msgid "``store_empty``: falls back to :setting:`FEED_STORE_EMPTY`"
msgstr ""

#: ../../topics/feed-exports.rst:278
msgid "FEED_EXPORT_ENCODING"
msgstr ""

#: ../../topics/feed-exports.rst:282
msgid "The encoding to be used for the feed."
msgstr ""

#: ../../topics/feed-exports.rst:284
msgid "If unset or set to ``None`` (default) it uses UTF-8 for everything except JSON output, which uses safe numeric encoding (``\\uXXXX`` sequences) for historic reasons."
msgstr ""

#: ../../topics/feed-exports.rst:287
msgid "Use ``utf-8`` if you want UTF-8 for JSON too."
msgstr ""

#: ../../topics/feed-exports.rst:292
msgid "FEED_EXPORT_FIELDS"
msgstr ""

#: ../../topics/feed-exports.rst:296
msgid "A list of fields to export, optional. Example: ``FEED_EXPORT_FIELDS = [\"foo\", \"bar\", \"baz\"]``."
msgstr ""

#: ../../topics/feed-exports.rst:299
msgid "Use FEED_EXPORT_FIELDS option to define fields to export and their order."
msgstr ""

#: ../../topics/feed-exports.rst:301
msgid "When FEED_EXPORT_FIELDS is empty or None (default), Scrapy uses fields defined in dicts or :class:`~.Item` subclasses a spider is yielding."
msgstr ""

#: ../../topics/feed-exports.rst:304
msgid "If an exporter requires a fixed set of fields (this is the case for :ref:`CSV <topics-feed-format-csv>` export format) and FEED_EXPORT_FIELDS is empty or None, then Scrapy tries to infer field names from the exported data - currently it uses field names from the first item."
msgstr ""

#: ../../topics/feed-exports.rst:312
msgid "FEED_EXPORT_INDENT"
msgstr ""

#: ../../topics/feed-exports.rst:316
msgid "Amount of spaces used to indent the output on each level. If ``FEED_EXPORT_INDENT`` is a non-negative integer, then array elements and object members will be pretty-printed with that indent level. An indent level of ``0`` (the default), or negative, will put each item on a new line. ``None`` selects the most compact representation."
msgstr ""

#: ../../topics/feed-exports.rst:321
msgid "Currently implemented only by :class:`~scrapy.exporters.JsonItemExporter` and :class:`~scrapy.exporters.XmlItemExporter`, i.e. when you are exporting to ``.json`` or ``.xml``."
msgstr ""

#: ../../topics/feed-exports.rst:328
msgid "FEED_STORE_EMPTY"
msgstr ""

#: ../../topics/feed-exports.rst:332
msgid "Whether to export empty feeds (i.e. feeds with no items)."
msgstr ""

#: ../../topics/feed-exports.rst:337
msgid "FEED_STORAGES"
msgstr ""

#: ../../topics/feed-exports.rst:341
msgid "A dict containing additional feed storage backends supported by your project. The keys are URI schemes and the values are paths to storage classes."
msgstr ""

#: ../../topics/feed-exports.rst:347
msgid "FEED_STORAGE_FTP_ACTIVE"
msgstr ""

#: ../../topics/feed-exports.rst:351
msgid "Whether to use the active connection mode when exporting feeds to an FTP server (``True``) or use the passive connection mode instead (``False``, default)."
msgstr ""

#: ../../topics/feed-exports.rst:354
msgid "For information about FTP connection modes, see `What is the difference between active and passive FTP? <https://stackoverflow.com/a/1699163>`_."
msgstr ""

#: ../../topics/feed-exports.rst:360
msgid "FEED_STORAGE_S3_ACL"
msgstr ""

#: ../../topics/feed-exports.rst:364
msgid "A string containing a custom ACL for feeds exported to Amazon S3 by your project."
msgstr ""

#: ../../topics/feed-exports.rst:366
msgid "For a complete list of available values, access the `Canned ACL`_ section on Amazon S3 docs."
msgstr ""

#: ../../topics/feed-exports.rst:371
msgid "FEED_STORAGES_BASE"
msgstr ""

#: ../../topics/feed-exports.rst:373
#: ../../topics/feed-exports.rst:407
#: ../../topics/settings.rst:299
#: ../../topics/settings.rst:525
#: ../../topics/settings.rst:600
#: ../../topics/settings.rst:758
#: ../../topics/settings.rst:1263
#: ../../topics/settings.rst:1331
msgid "Default::"
msgstr ""

#: ../../topics/feed-exports.rst:383
msgid "A dict containing the built-in feed storage backends supported by Scrapy. You can disable any of these backends by assigning ``None`` to their URI scheme in :setting:`FEED_STORAGES`. E.g., to disable the built-in FTP storage backend (without replacement), place this in your ``settings.py``::"
msgstr ""

#: ../../topics/feed-exports.rst:395
msgid "FEED_EXPORTERS"
msgstr ""

#: ../../topics/feed-exports.rst:399
msgid "A dict containing additional exporters supported by your project. The keys are serialization formats and the values are paths to :ref:`Item exporter <topics-exporters>` classes."
msgstr ""

#: ../../topics/feed-exports.rst:406
msgid "FEED_EXPORTERS_BASE"
msgstr ""

#: ../../topics/feed-exports.rst:419
msgid "A dict containing the built-in feed exporters supported by Scrapy. You can disable any of these exporters by assigning ``None`` to their serialization format in :setting:`FEED_EXPORTERS`. E.g., to disable the built-in CSV exporter (without replacement), place this in your ``settings.py``::"
msgstr ""

#: ../../topics/item-pipeline.rst:7
msgid "After an item has been scraped by a spider, it is sent to the Item Pipeline which processes it through several components that are executed sequentially."
msgstr ""

#: ../../topics/item-pipeline.rst:10
msgid "Each item pipeline component (sometimes referred as just \"Item Pipeline\") is a Python class that implements a simple method. They receive an item and perform an action over it, also deciding if the item should continue through the pipeline or be dropped and no longer processed."
msgstr ""

#: ../../topics/item-pipeline.rst:15
msgid "Typical uses of item pipelines are:"
msgstr ""

#: ../../topics/item-pipeline.rst:17
msgid "cleansing HTML data"
msgstr ""

#: ../../topics/item-pipeline.rst:18
msgid "validating scraped data (checking that the items contain certain fields)"
msgstr ""

#: ../../topics/item-pipeline.rst:19
msgid "checking for duplicates (and dropping them)"
msgstr ""

#: ../../topics/item-pipeline.rst:20
msgid "storing the scraped item in a database"
msgstr ""

#: ../../topics/item-pipeline.rst:24
msgid "Writing your own item pipeline"
msgstr ""

#: ../../topics/item-pipeline.rst:26
msgid "Each item pipeline component is a Python class that must implement the following method:"
msgstr ""

#: ../../topics/item-pipeline.rst:30
msgid "This method is called for every item pipeline component. :meth:`process_item` must either: return a dict with data, return an :class:`~scrapy.item.Item` (or any descendant class) object, return a :class:`~twisted.internet.defer.Deferred` or raise :exc:`~scrapy.exceptions.DropItem` exception. Dropped items are no longer processed by further pipeline components."
msgstr ""

#: ../../topics/item-pipeline.rst:37
#: ../../topics/signals.rst:154
msgid "the item scraped"
msgstr ""

#: ../../topics/item-pipeline.rst:40
#: ../../topics/signals.rst:157
#: ../../topics/signals.rst:177
msgid "the spider which scraped the item"
msgstr ""

#: ../../topics/item-pipeline.rst:43
msgid "Additionally, they may also implement the following methods:"
msgstr ""

#: ../../topics/item-pipeline.rst:47
msgid "This method is called when the spider is opened."
msgstr ""

#: ../../topics/item-pipeline.rst:49
msgid "the spider which was opened"
msgstr ""

#: ../../topics/item-pipeline.rst:54
msgid "This method is called when the spider is closed."
msgstr ""

#: ../../topics/item-pipeline.rst:56
msgid "the spider which was closed"
msgstr ""

#: ../../topics/item-pipeline.rst:61
msgid "If present, this classmethod is called to create a pipeline instance from a :class:`~scrapy.crawler.Crawler`. It must return a new instance of the pipeline. Crawler object provides access to all Scrapy core components like settings and signals; it is a way for pipeline to access them and hook its functionality into Scrapy."
msgstr ""

#: ../../topics/item-pipeline.rst:67
msgid "crawler that uses this pipeline"
msgstr ""

#: ../../topics/item-pipeline.rst:72
msgid "Item pipeline example"
msgstr ""

#: ../../topics/item-pipeline.rst:75
msgid "Price validation and dropping items with no prices"
msgstr ""

#: ../../topics/item-pipeline.rst:77
msgid "Let's take a look at the following hypothetical pipeline that adjusts the ``price`` attribute for those items that do not include VAT (``price_excludes_vat`` attribute), and drops those items which don't contain a price::"
msgstr ""

#: ../../topics/item-pipeline.rst:98
msgid "Write items to a JSON file"
msgstr ""

#: ../../topics/item-pipeline.rst:100
msgid "The following pipeline stores all scraped items (from all spiders) into a single ``items.jl`` file, containing one item per line serialized in JSON format::"
msgstr ""

#: ../../topics/item-pipeline.rst:119
msgid "The purpose of JsonWriterPipeline is just to introduce how to write item pipelines. If you really want to store all scraped items into a JSON file you should use the :ref:`Feed exports <topics-feed-exports>`."
msgstr ""

#: ../../topics/item-pipeline.rst:124
msgid "Write items to MongoDB"
msgstr ""

#: ../../topics/item-pipeline.rst:126
msgid "In this example we'll write items to MongoDB_ using pymongo_. MongoDB address and database name are specified in Scrapy settings; MongoDB collection is named after item class."
msgstr ""

#: ../../topics/item-pipeline.rst:130
msgid "The main point of this example is to show how to use :meth:`from_crawler` method and how to clean up the resources properly.::"
msgstr ""

#: ../../topics/item-pipeline.rst:168
msgid "Take screenshot of item"
msgstr ""

#: ../../topics/item-pipeline.rst:170
msgid "This example demonstrates how to return a :class:`~twisted.internet.defer.Deferred` from the :meth:`process_item` method. It uses Splash_ to render screenshot of item url. Pipeline makes request to locally running instance of Splash_. After request is downloaded, it saves the screenshot to a file and adds filename to the item."
msgstr ""

#: ../../topics/item-pipeline.rst:213
msgid "Duplicates filter"
msgstr ""

#: ../../topics/item-pipeline.rst:215
msgid "A filter that looks for duplicate items, and drops those items that were already processed. Let's say that our items have a unique id, but our spider returns multiples items with the same id::"
msgstr ""

#: ../../topics/item-pipeline.rst:236
msgid "Activating an Item Pipeline component"
msgstr ""

#: ../../topics/item-pipeline.rst:238
msgid "To activate an Item Pipeline component you must add its class to the :setting:`ITEM_PIPELINES` setting, like in the following example::"
msgstr ""

#: ../../topics/item-pipeline.rst:246
msgid "The integer values you assign to classes in this setting determine the order in which they run: items go through from lower valued to higher valued classes. It's customary to define these numbers in the 0-1000 range."
msgstr ""

#: ../../topics/items.rst:5
msgid "Items"
msgstr ""

#: ../../topics/items.rst:10
msgid "The main goal in scraping is to extract structured data from unstructured sources, typically, web pages. Scrapy spiders can return the extracted data as Python dicts. While convenient and familiar, Python dicts lack structure: it is easy to make a typo in a field name or return inconsistent data, especially in a larger project with many spiders."
msgstr ""

#: ../../topics/items.rst:16
msgid "To define common output data format Scrapy provides the :class:`Item` class. :class:`Item` objects are simple containers used to collect the scraped data. They provide an API similar to :class:`dict` API with a convenient syntax for declaring their available fields."
msgstr ""

#: ../../topics/items.rst:21
msgid "Various Scrapy components use extra information provided by Items: exporters look at declared fields to figure out columns to export, serialization can be customized using Item fields metadata, :mod:`trackref` tracks Item instances to help find memory leaks (see :ref:`topics-leaks-trackrefs`), etc."
msgstr ""

#: ../../topics/items.rst:30
msgid "Declaring Items"
msgstr ""

#: ../../topics/items.rst:32
msgid "Items are declared using a simple class definition syntax and :class:`Field` objects. Here is an example::"
msgstr ""

#: ../../topics/items.rst:44
msgid "Those familiar with `Django`_ will notice that Scrapy Items are declared similar to `Django Models`_, except that Scrapy Items are much simpler as there is no concept of different field types."
msgstr ""

#: ../../topics/items.rst:54
msgid "Item Fields"
msgstr ""

#: ../../topics/items.rst:56
msgid ":class:`Field` objects are used to specify metadata for each field. For example, the serializer function for the ``last_updated`` field illustrated in the example above."
msgstr ""

#: ../../topics/items.rst:60
msgid "You can specify any kind of metadata for each field. There is no restriction on the values accepted by :class:`Field` objects. For this same reason, there is no reference list of all available metadata keys. Each key defined in :class:`Field` objects could be used by a different component, and only those components know about it. You can also define and use any other :class:`Field` key in your project too, for your own needs. The main goal of :class:`Field` objects is to provide a way to define all field metadata in one place. Typically, those components whose behaviour depends on each field use certain field keys to configure that behaviour. You must refer to their documentation to see which metadata keys are used by each component."
msgstr ""

#: ../../topics/items.rst:71
msgid "It's important to note that the :class:`Field` objects used to declare the item do not stay assigned as class attributes. Instead, they can be accessed through the :attr:`Item.fields` attribute."
msgstr ""

#: ../../topics/items.rst:76
msgid "Working with Items"
msgstr ""

#: ../../topics/items.rst:78
msgid "Here are some examples of common tasks performed with items, using the ``Product`` item :ref:`declared above  <topics-items-declaring>`. You will notice the API is very similar to the :class:`dict` API."
msgstr ""

#: ../../topics/items.rst:83
msgid "Creating items"
msgstr ""

#: ../../topics/items.rst:91
msgid "Getting field values"
msgstr ""

#: ../../topics/items.rst:131
msgid "Setting field values"
msgstr ""

#: ../../topics/items.rst:144
msgid "Accessing all populated values"
msgstr ""

#: ../../topics/items.rst:146
msgid "To access all populated values, just use the typical :class:`dict` API:"
msgstr ""

#: ../../topics/items.rst:158
msgid "Copying items"
msgstr ""

#: ../../topics/items.rst:160
msgid "To copy an item, you must first decide whether you want a shallow copy or a deep copy."
msgstr ""

#: ../../topics/items.rst:163
msgid "If your item contains :term:`mutable` values like lists or dictionaries, a shallow copy will keep references to the same mutable values across all different copies."
msgstr ""

#: ../../topics/items.rst:167
msgid "For example, if you have an item with a list of tags, and you create a shallow copy of that item, both the original item and the copy have the same list of tags. Adding a tag to the list of one of the items will add the tag to the other item as well."
msgstr ""

#: ../../topics/items.rst:172
msgid "If that is not the desired behavior, use a deep copy instead."
msgstr ""

#: ../../topics/items.rst:174
msgid "See :mod:`copy` for more information."
msgstr ""

#: ../../topics/items.rst:176
msgid "To create a shallow copy of an item, you can either call :meth:`~scrapy.item.Item.copy` on an existing item (``product2 = product.copy()``) or instantiate your item class from an existing item (``product2 = Product(product)``)."
msgstr ""

#: ../../topics/items.rst:181
msgid "To create a deep copy, call :meth:`~scrapy.item.Item.deepcopy` instead (``product2 = product.deepcopy()``)."
msgstr ""

#: ../../topics/items.rst:186
msgid "Other common tasks"
msgstr ""

#: ../../topics/items.rst:188
msgid "Creating dicts from items:"
msgstr ""

#: ../../topics/items.rst:193
msgid "Creating items from dicts:"
msgstr ""

#: ../../topics/items.rst:205
msgid "Extending Items"
msgstr ""

#: ../../topics/items.rst:207
msgid "You can extend Items (to add more fields or to change some metadata for some fields) by declaring a subclass of your original Item."
msgstr ""

#: ../../topics/items.rst:216
msgid "You can also extend field metadata by using the previous field metadata and appending more values, or changing existing values, like this::"
msgstr ""

#: ../../topics/items.rst:222
msgid "That adds (or replaces) the ``serializer`` metadata key for the ``name`` field, keeping all the previously existing metadata values."
msgstr ""

#: ../../topics/items.rst:226
msgid "Item objects"
msgstr ""

#: ../../topics/items.rst:230
msgid "Return a new Item optionally initialized from the given argument."
msgstr ""

#: ../../topics/items.rst:232
msgid "Items replicate the standard :class:`dict` API, including its ``__init__`` method, and also provide the following additional API members:"
msgstr ""

#: ../../../scrapy/item.py:docstring of scrapy.item.Item.deepcopy:1
msgid "Return a :func:`~copy.deepcopy` of this item."
msgstr ""

#: ../../topics/items.rst:241
msgid "A dictionary containing *all declared fields* for this Item, not only those populated. The keys are the field names and the values are the :class:`Field` objects used in the :ref:`Item declaration <topics-items-declaring>`."
msgstr ""

#: ../../topics/items.rst:247
msgid "Field objects"
msgstr ""

#: ../../topics/items.rst:251
msgid "The :class:`Field` class is just an alias to the built-in :class:`dict` class and doesn't provide any extra functionality or attributes. In other words, :class:`Field` objects are plain-old Python dicts. A separate class is used to support the :ref:`item declaration syntax <topics-items-declaring>` based on class attributes."
msgstr ""

#: ../../topics/items.rst:258
msgid "Other classes related to Item"
msgstr ""

#: ../../../scrapy/item.py:docstring of scrapy.item.BaseItem:1
msgid "Base class for all scraped items."
msgstr ""

#: ../../../scrapy/item.py:docstring of scrapy.item.BaseItem:3
msgid "In Scrapy, an object is considered an *item* if it is an instance of either :class:`BaseItem` or :class:`dict`. For example, when the output of a spider callback is evaluated, only instances of :class:`BaseItem` or :class:`dict` are passed to :ref:`item pipelines <topics-item-pipeline>`."
msgstr ""

#: ../../../scrapy/item.py:docstring of scrapy.item.BaseItem:8
msgid "If you need instances of a custom class to be considered items by Scrapy, you must inherit from either :class:`BaseItem` or :class:`dict`."
msgstr ""

#: ../../../scrapy/item.py:docstring of scrapy.item.BaseItem:11
msgid "Unlike instances of :class:`dict`, instances of :class:`BaseItem` may be :ref:`tracked <topics-leaks-trackrefs>` to debug memory leaks."
msgstr ""

#: ../../../scrapy/item.py:docstring of scrapy.item.ItemMeta:1
msgid "Metaclass_ of :class:`Item` that handles field definitions."
msgstr ""

#: ../../topics/jobs.rst:5
msgid "Jobs: pausing and resuming crawls"
msgstr ""

#: ../../topics/jobs.rst:7
msgid "Sometimes, for big sites, it's desirable to pause crawls and be able to resume them later."
msgstr ""

#: ../../topics/jobs.rst:10
msgid "Scrapy supports this functionality out of the box by providing the following facilities:"
msgstr ""

#: ../../topics/jobs.rst:13
msgid "a scheduler that persists scheduled requests on disk"
msgstr ""

#: ../../topics/jobs.rst:15
msgid "a duplicates filter that persists visited requests on disk"
msgstr ""

#: ../../topics/jobs.rst:17
msgid "an extension that keeps some spider state (key/value pairs) persistent between batches"
msgstr ""

#: ../../topics/jobs.rst:21
msgid "Job directory"
msgstr ""

#: ../../topics/jobs.rst:23
msgid "To enable persistence support you just need to define a *job directory* through the ``JOBDIR`` setting. This directory will be for storing all required data to keep the state of a single job (i.e. a spider run).  It's important to note that this directory must not be shared by different spiders, or even different jobs/runs of the same spider, as it's meant to be used for storing the state of a *single* job."
msgstr ""

#: ../../topics/jobs.rst:31
msgid "How to use it"
msgstr ""

#: ../../topics/jobs.rst:33
msgid "To start a spider with persistence support enabled, run it like this::"
msgstr ""

#: ../../topics/jobs.rst:37
msgid "Then, you can stop the spider safely at any time (by pressing Ctrl-C or sending a signal), and resume it later by issuing the same command::"
msgstr ""

#: ../../topics/jobs.rst:43
msgid "Keeping persistent state between batches"
msgstr ""

#: ../../topics/jobs.rst:45
msgid "Sometimes you'll want to keep some persistent spider state between pause/resume batches. You can use the ``spider.state`` attribute for that, which should be a dict. There's a built-in extension that takes care of serializing, storing and loading that attribute from the job directory, when the spider starts and stops."
msgstr ""

#: ../../topics/jobs.rst:51
msgid "Here's an example of a callback that uses the spider state (other spider code is omitted for brevity)::"
msgstr ""

#: ../../topics/jobs.rst:59
msgid "Persistence gotchas"
msgstr ""

#: ../../topics/jobs.rst:61
msgid "There are a few things to keep in mind if you want to be able to use the Scrapy persistence support:"
msgstr ""

#: ../../topics/jobs.rst:65
msgid "Cookies expiration"
msgstr ""

#: ../../topics/jobs.rst:67
msgid "Cookies may expire. So, if you don't resume your spider quickly the requests scheduled may no longer work. This won't be an issue if you spider doesn't rely on cookies."
msgstr ""

#: ../../topics/jobs.rst:75
msgid "Request serialization"
msgstr ""

#: ../../topics/jobs.rst:77
msgid "For persistence to work, :class:`~scrapy.http.Request` objects must be serializable with :mod:`pickle`, except for the ``callback`` and ``errback`` values passed to their ``__init__`` method, which must be methods of the running :class:`~scrapy.spiders.Spider` class."
msgstr ""

#: ../../topics/jobs.rst:82
msgid "If you wish to log the requests that couldn't be serialized, you can set the :setting:`SCHEDULER_DEBUG` setting to ``True`` in the project's settings page. It is ``False`` by default."
msgstr ""

#: ../../topics/leaks.rst:5
msgid "Debugging memory leaks"
msgstr ""

#: ../../topics/leaks.rst:7
msgid "In Scrapy, objects such as Requests, Responses and Items have a finite lifetime: they are created, used for a while, and finally destroyed."
msgstr ""

#: ../../topics/leaks.rst:10
msgid "From all those objects, the Request is probably the one with the longest lifetime, as it stays waiting in the Scheduler queue until it's time to process it. For more info see :ref:`topics-architecture`."
msgstr ""

#: ../../topics/leaks.rst:14
msgid "As these Scrapy objects have a (rather long) lifetime, there is always the risk of accumulating them in memory without releasing them properly and thus causing what is known as a \"memory leak\"."
msgstr ""

#: ../../topics/leaks.rst:18
msgid "To help debugging memory leaks, Scrapy provides a built-in mechanism for tracking objects references called :ref:`trackref <topics-leaks-trackrefs>`, and you can also use a third-party library called :ref:`muppy <topics-leaks-muppy>` for more advanced memory debugging (see below for more info). Both mechanisms must be used from the :ref:`Telnet Console <topics-telnetconsole>`."
msgstr ""

#: ../../topics/leaks.rst:26
msgid "Common causes of memory leaks"
msgstr ""

#: ../../topics/leaks.rst:28
msgid "It happens quite often (sometimes by accident, sometimes on purpose) that the Scrapy developer passes objects referenced in Requests (for example, using the :attr:`~scrapy.http.Request.cb_kwargs` or :attr:`~scrapy.http.Request.meta` attributes or the request callback function) and that effectively bounds the lifetime of those referenced objects to the lifetime of the Request. This is, by far, the most common cause of memory leaks in Scrapy projects, and a quite difficult one to debug for newcomers."
msgstr ""

#: ../../topics/leaks.rst:36
msgid "In big projects, the spiders are typically written by different people and some of those spiders could be \"leaking\" and thus affecting the rest of the other (well-written) spiders when they get to run concurrently, which, in turn, affects the whole crawling process."
msgstr ""

#: ../../topics/leaks.rst:41
msgid "The leak could also come from a custom middleware, pipeline or extension that you have written, if you are not releasing the (previously allocated) resources properly. For example, allocating resources on :signal:`spider_opened` but not releasing them on :signal:`spider_closed` may cause problems if you're running :ref:`multiple spiders per process <run-multiple-spiders>`."
msgstr ""

#: ../../topics/leaks.rst:48
msgid "Too Many Requests?"
msgstr ""

#: ../../topics/leaks.rst:50
msgid "By default Scrapy keeps the request queue in memory; it includes :class:`~scrapy.http.Request` objects and all objects referenced in Request attributes (e.g. in :attr:`~scrapy.http.Request.cb_kwargs` and :attr:`~scrapy.http.Request.meta`). While not necessarily a leak, this can take a lot of memory. Enabling :ref:`persistent job queue <topics-jobs>` could help keeping memory usage in control."
msgstr ""

#: ../../topics/leaks.rst:61
msgid "Debugging memory leaks with ``trackref``"
msgstr ""

#: ../../topics/leaks.rst:63
msgid ":mod:`trackref` is a module provided by Scrapy to debug the most common cases of memory leaks. It basically tracks the references to all live Requests, Responses, Item and Selector objects."
msgstr ""

#: ../../topics/leaks.rst:67
msgid "You can enter the telnet console and inspect how many objects (of the classes mentioned above) are currently alive using the ``prefs()`` function which is an alias to the :func:`~scrapy.utils.trackref.print_live_refs` function::"
msgstr ""

#: ../../topics/leaks.rst:81
msgid "As you can see, that report also shows the \"age\" of the oldest object in each class. If you're running multiple spiders per process chances are you can figure out which spider is leaking by looking at the oldest request or response. You can get the oldest object of each class using the :func:`~scrapy.utils.trackref.get_oldest` function (from the telnet console)."
msgstr ""

#: ../../topics/leaks.rst:88
msgid "Which objects are tracked?"
msgstr ""

#: ../../topics/leaks.rst:90
msgid "The objects tracked by ``trackrefs`` are all from these classes (and all its subclasses):"
msgstr ""

#: ../../topics/leaks.rst:93
msgid ":class:`scrapy.http.Request`"
msgstr ""

#: ../../topics/leaks.rst:94
msgid ":class:`scrapy.http.Response`"
msgstr ""

#: ../../topics/leaks.rst:95
msgid ":class:`scrapy.item.Item`"
msgstr ""

#: ../../topics/leaks.rst:96
msgid ":class:`scrapy.selector.Selector`"
msgstr ""

#: ../../topics/leaks.rst:97
msgid ":class:`scrapy.spiders.Spider`"
msgstr ""

#: ../../topics/leaks.rst:100
msgid "A real example"
msgstr ""

#: ../../topics/leaks.rst:102
msgid "Let's see a concrete example of a hypothetical case of memory leaks. Suppose we have some spider with a line similar to this one::"
msgstr ""

#: ../../topics/leaks.rst:108
msgid "That line is passing a response reference inside a request which effectively ties the response lifetime to the requests' one, and that would definitely cause memory leaks."
msgstr ""

#: ../../topics/leaks.rst:112
msgid "Let's see how we can discover the cause (without knowing it a priori, of course) by using the ``trackref`` tool."
msgstr ""

#: ../../topics/leaks.rst:115
msgid "After the crawler is running for a few minutes and we notice its memory usage has grown a lot, we can enter its telnet console and check the live references::"
msgstr ""

#: ../../topics/leaks.rst:127
msgid "The fact that there are so many live responses (and that they're so old) is definitely suspicious, as responses should have a relatively short lifetime compared to Requests. The number of responses is similar to the number of requests, so it looks like they are tied in a some way. We can now go and check the code of the spider to discover the nasty line that is generating the leaks (passing response references inside requests)."
msgstr ""

#: ../../topics/leaks.rst:134
msgid "Sometimes extra information about live objects can be helpful. Let's check the oldest response:"
msgstr ""

#: ../../topics/leaks.rst:142
msgid "If you want to iterate over all objects, instead of getting the oldest one, you can use the :func:`scrapy.utils.trackref.iter_all` function:"
msgstr ""

#: ../../topics/leaks.rst:152
msgid "Too many spiders?"
msgstr ""

#: ../../topics/leaks.rst:154
msgid "If your project has too many spiders executed in parallel, the output of :func:`prefs()` can be difficult to read. For this reason, that function has a ``ignore`` argument which can be used to ignore a particular class (and all its subclases). For example, this won't show any live references to spiders:"
msgstr ""

#: ../../topics/leaks.rst:167
msgid "scrapy.utils.trackref module"
msgstr ""

#: ../../topics/leaks.rst:169
msgid "Here are the functions available in the :mod:`~scrapy.utils.trackref` module."
msgstr ""

#: ../../topics/leaks.rst:173
msgid "Inherit from this class if you want to track live instances with the ``trackref`` module."
msgstr ""

#: ../../topics/leaks.rst:178
msgid "Print a report of live references, grouped by class name."
msgstr ""

#: ../../topics/leaks.rst:180
msgid "if given, all objects from the specified class (or tuple of classes) will be ignored."
msgstr ""

#: ../../topics/leaks.rst:186
msgid "Return the oldest object alive with the given class name, or ``None`` if none is found. Use :func:`print_live_refs` first to get a list of all tracked live objects per class name."
msgstr ""

#: ../../topics/leaks.rst:192
msgid "Return an iterator over all objects alive with the given class name, or ``None`` if none is found. Use :func:`print_live_refs` first to get a list of all tracked live objects per class name."
msgstr ""

#: ../../topics/leaks.rst:199
msgid "Debugging memory leaks with muppy"
msgstr ""

#: ../../topics/leaks.rst:201
msgid "``trackref`` provides a very convenient mechanism for tracking down memory leaks, but it only keeps track of the objects that are more likely to cause memory leaks (Requests, Responses, Items, and Selectors). However, there are other cases where the memory leaks could come from other (more or less obscure) objects. If this is your case, and you can't find your leaks using ``trackref``, you still have another resource: the muppy library."
msgstr ""

#: ../../topics/leaks.rst:209
msgid "You can use muppy from `Pympler`_."
msgstr ""

#: ../../topics/leaks.rst:213
msgid "If you use ``pip``, you can install muppy with the following command::"
msgstr ""

#: ../../topics/leaks.rst:217
msgid "Here's an example to view all Python objects available in the heap using muppy:"
msgstr ""

#: ../../topics/leaks.rst:245
msgid "For more info about muppy, refer to the `muppy documentation`_."
msgstr ""

#: ../../topics/leaks.rst:252
msgid "Leaks without leaks"
msgstr ""

#: ../../topics/leaks.rst:254
msgid "Sometimes, you may notice that the memory usage of your Scrapy process will only increase, but never decrease. Unfortunately, this could happen even though neither Scrapy nor your project are leaking memory. This is due to a (not so well) known problem of Python, which may not return released memory to the operating system in some cases. For more information on this issue see:"
msgstr ""

#: ../../topics/leaks.rst:260
msgid "`Python Memory Management <https://www.evanjones.ca/python-memory.html>`_"
msgstr ""

#: ../../topics/leaks.rst:261
msgid "`Python Memory Management Part 2 <https://www.evanjones.ca/python-memory-part2.html>`_"
msgstr ""

#: ../../topics/leaks.rst:262
msgid "`Python Memory Management Part 3 <https://www.evanjones.ca/python-memory-part3.html>`_"
msgstr ""

#: ../../topics/leaks.rst:264
msgid "The improvements proposed by Evan Jones, which are detailed in `this paper`_, got merged in Python 2.5, but this only reduces the problem, it doesn't fix it completely. To quote the paper:"
msgstr ""

#: ../../topics/leaks.rst:268
msgid "*Unfortunately, this patch can only free an arena if there are no more objects allocated in it anymore. This means that fragmentation is a large issue. An application could have many megabytes of free memory, scattered throughout all the arenas, but it will be unable to free any of it. This is a problem experienced by all memory allocators. The only way to solve it is to move to a compacting garbage collector, which is able to move objects in memory. This would require significant changes to the Python interpreter.*"
msgstr ""

#: ../../topics/leaks.rst:278
msgid "To keep memory consumption reasonable you can split the job into several smaller jobs or enable :ref:`persistent job queue <topics-jobs>` and stop/start spider from time to time."
msgstr ""

#: ../../topics/link-extractors.rst:5
msgid "Link Extractors"
msgstr ""

#: ../../topics/link-extractors.rst:7
msgid "A link extractor is an object that extracts links from responses."
msgstr ""

#: ../../topics/link-extractors.rst:9
msgid "The ``__init__`` method of :class:`~scrapy.linkextractors.lxmlhtml.LxmlLinkExtractor` takes settings that determine which links may be extracted. :class:`LxmlLinkExtractor.extract_links <scrapy.linkextractors.lxmlhtml.LxmlLinkExtractor.extract_links>` returns a list of matching :class:`scrapy.link.Link` objects from a :class:`~scrapy.http.Response` object."
msgstr ""

#: ../../topics/link-extractors.rst:16
msgid "Link extractors are used in :class:`~scrapy.spiders.CrawlSpider` spiders through a set of :class:`~scrapy.spiders.Rule` objects. You can also use link extractors in regular spiders."
msgstr ""

#: ../../topics/link-extractors.rst:23
msgid "Link extractor reference"
msgstr ""

#: ../../topics/link-extractors.rst:28
msgid "The link extractor class is :class:`scrapy.linkextractors.lxmlhtml.LxmlLinkExtractor`. For convenience it can also be imported as ``scrapy.linkextractors.LinkExtractor``::"
msgstr ""

#: ../../topics/link-extractors.rst:35
msgid "LxmlLinkExtractor"
msgstr ""

#: ../../topics/link-extractors.rst:43
msgid "LxmlLinkExtractor is the recommended link extractor with handy filtering options. It is implemented using lxml's robust HTMLParser."
msgstr ""

#: ../../topics/link-extractors.rst:46
msgid "a single regular expression (or list of regular expressions) that the (absolute) urls must match in order to be extracted. If not given (or empty), it will match all links."
msgstr ""

#: ../../topics/link-extractors.rst:51
msgid "a single regular expression (or list of regular expressions) that the (absolute) urls must match in order to be excluded (i.e. not extracted). It has precedence over the ``allow`` parameter. If not given (or empty) it won't exclude any links."
msgstr ""

#: ../../topics/link-extractors.rst:57
msgid "a single value or a list of string containing domains which will be considered for extracting the links"
msgstr ""

#: ../../topics/link-extractors.rst:61
msgid "a single value or a list of strings containing domains which won't be considered for extracting the links"
msgstr ""

#: ../../topics/link-extractors.rst:65
msgid "a single value or list of strings containing extensions that should be ignored when extracting links. If not given, it will default to :data:`scrapy.linkextractors.IGNORED_EXTENSIONS`.  .. versionchanged:: 2.0    :data:`~scrapy.linkextractors.IGNORED_EXTENSIONS` now includes    ``7z``, ``7zip``, ``apk``, ``bz2``, ``cdr``, ``dmg``, ``ico``,    ``iso``, ``tar``, ``tar.gz``, ``webm``, and ``xz``."
msgstr ""

#: ../../topics/link-extractors.rst:65
msgid "a single value or list of strings containing extensions that should be ignored when extracting links. If not given, it will default to :data:`scrapy.linkextractors.IGNORED_EXTENSIONS`."
msgstr ""

#: ../../topics/link-extractors.rst:70
msgid ":data:`~scrapy.linkextractors.IGNORED_EXTENSIONS` now includes ``7z``, ``7zip``, ``apk``, ``bz2``, ``cdr``, ``dmg``, ``ico``, ``iso``, ``tar``, ``tar.gz``, ``webm``, and ``xz``."
msgstr ""

#: ../../topics/link-extractors.rst:76
msgid "is an XPath (or list of XPath's) which defines regions inside the response where links should be extracted from. If given, only the text selected by those XPath will be scanned for links. See examples below."
msgstr ""

#: ../../topics/link-extractors.rst:82
msgid "a CSS selector (or list of selectors) which defines regions inside the response where links should be extracted from. Has the same behaviour as ``restrict_xpaths``."
msgstr ""

#: ../../topics/link-extractors.rst:87
msgid "a single regular expression (or list of regular expressions) that the link's text must match in order to be extracted. If not given (or empty), it will match all links. If a list of regular expressions is given, the link will be extracted if it matches at least one."
msgstr ""

#: ../../topics/link-extractors.rst:93
msgid "a tag or a list of tags to consider when extracting links. Defaults to ``('a', 'area')``."
msgstr ""

#: ../../topics/link-extractors.rst:97
msgid "an attribute or list of attributes which should be considered when looking for links to extract (only for those tags specified in the ``tags`` parameter). Defaults to ``('href',)``"
msgstr ""

#: ../../topics/link-extractors.rst:102
msgid "canonicalize each extracted url (using w3lib.url.canonicalize_url). Defaults to ``False``. Note that canonicalize_url is meant for duplicate checking; it can change the URL visible at server side, so the response can be different for requests with canonicalized and raw URLs. If you're using LinkExtractor to follow links it is more robust to keep the default ``canonicalize=False``."
msgstr ""

#: ../../topics/link-extractors.rst:111
msgid "whether duplicate filtering should be applied to extracted links."
msgstr ""

#: ../../topics/link-extractors.rst:115
msgid "a function which receives each value extracted from the tag and attributes scanned and can modify the value and return a new one, or return ``None`` to ignore the link altogether. If not given, ``process_value`` defaults to ``lambda x: x``.  .. highlight:: html  For example, to extract links from this code::      <a href=\"javascript:goToPage('../other/page.html'); return false\">Link text</a>  .. highlight:: python  You can use the following function in ``process_value``::      def process_value(value):         m = re.search(\"javascript:goToPage\\('(.*?)'\", value)         if m:             return m.group(1)"
msgstr ""

#: ../../topics/link-extractors.rst:115
msgid "a function which receives each value extracted from the tag and attributes scanned and can modify the value and return a new one, or return ``None`` to ignore the link altogether. If not given, ``process_value`` defaults to ``lambda x: x``."
msgstr ""

#: ../../topics/link-extractors.rst:122
msgid "For example, to extract links from this code::"
msgstr ""

#: ../../topics/link-extractors.rst:128
msgid "You can use the following function in ``process_value``::"
msgstr ""

#: ../../topics/link-extractors.rst:137
msgid "whether to strip whitespaces from extracted attributes. According to HTML5 standard, leading and trailing whitespaces must be stripped from ``href`` attributes of ``<a>``, ``<area>`` and many other elements, ``src`` attribute of ``<img>``, ``<iframe>`` elements, etc., so LinkExtractor strips space chars by default. Set ``strip=False`` to turn it off (e.g. if you're extracting urls from elements or attributes which allow leading/trailing whitespaces)."
msgstr ""

#: ../../../scrapy/linkextractors/lxmlhtml.py:docstring of scrapy.linkextractors.lxmlhtml.LxmlLinkExtractor.extract_links:1
msgid "Returns a list of :class:`~scrapy.link.Link` objects from the specified :class:`response <scrapy.http.Response>`."
msgstr ""

#: ../../../scrapy/linkextractors/lxmlhtml.py:docstring of scrapy.linkextractors.lxmlhtml.LxmlLinkExtractor.extract_links:4
msgid "Only links that match the settings passed to the ``__init__`` method of the link extractor are returned."
msgstr ""

#: ../../../scrapy/linkextractors/lxmlhtml.py:docstring of scrapy.linkextractors.lxmlhtml.LxmlLinkExtractor.extract_links:7
msgid "Duplicate links are omitted."
msgstr ""

#: ../../topics/loaders.rst:5
msgid "Item Loaders"
msgstr ""

#: ../../topics/loaders.rst:10
msgid "Item Loaders provide a convenient mechanism for populating scraped :ref:`Items <topics-items>`. Even though Items can be populated using their own dictionary-like API, Item Loaders provide a much more convenient API for populating them from a scraping process, by automating some common tasks like parsing the raw extracted data before assigning it."
msgstr ""

#: ../../topics/loaders.rst:16
msgid "In other words, :ref:`Items <topics-items>` provide the *container* of scraped data, while Item Loaders provide the mechanism for *populating* that container."
msgstr ""

#: ../../topics/loaders.rst:20
msgid "Item Loaders are designed to provide a flexible, efficient and easy mechanism for extending and overriding different field parsing rules, either by spider, or by source format (HTML, XML, etc) without becoming a nightmare to maintain."
msgstr ""

#: ../../topics/loaders.rst:25
msgid "Using Item Loaders to populate items"
msgstr ""

#: ../../topics/loaders.rst:27
msgid "To use an Item Loader, you must first instantiate it. You can either instantiate it with a dict-like object (e.g. Item or dict) or without one, in which case an Item is automatically instantiated in the Item Loader ``__init__`` method using the Item class specified in the :attr:`ItemLoader.default_item_class` attribute."
msgstr ""

#: ../../topics/loaders.rst:33
msgid "Then, you start collecting values into the Item Loader, typically using :ref:`Selectors <topics-selectors>`. You can add more than one value to the same item field; the Item Loader will know how to \"join\" those values later using a proper processing function."
msgstr ""

#: ../../topics/loaders.rst:38
msgid "Collected data is internally stored as lists, allowing to add several values to the same field. If an ``item`` argument is passed when creating a loader, each of the item's values will be stored as-is if it's already an iterable, or wrapped with a list if it's a single value."
msgstr ""

#: ../../topics/loaders.rst:44
msgid "Here is a typical Item Loader usage in a :ref:`Spider <topics-spiders>`, using the :ref:`Product item <topics-items-declaring>` declared in the :ref:`Items chapter <topics-items>`::"
msgstr ""

#: ../../topics/loaders.rst:60
msgid "By quickly looking at that code, we can see the ``name`` field is being extracted from two different XPath locations in the page:"
msgstr ""

#: ../../topics/loaders.rst:63
msgid "``//div[@class=\"product_name\"]``"
msgstr ""

#: ../../topics/loaders.rst:64
msgid "``//div[@class=\"product_title\"]``"
msgstr ""

#: ../../topics/loaders.rst:66
msgid "In other words, data is being collected by extracting it from two XPath locations, using the :meth:`~ItemLoader.add_xpath` method. This is the data that will be assigned to the ``name`` field later."
msgstr ""

#: ../../topics/loaders.rst:70
msgid "Afterwards, similar calls are used for ``price`` and ``stock`` fields (the latter using a CSS selector with the :meth:`~ItemLoader.add_css` method), and finally the ``last_update`` field is populated directly with a literal value (``today``) using a different method: :meth:`~ItemLoader.add_value`."
msgstr ""

#: ../../topics/loaders.rst:75
msgid "Finally, when all data is collected, the :meth:`ItemLoader.load_item` method is called which actually returns the item populated with the data previously extracted and collected with the :meth:`~ItemLoader.add_xpath`, :meth:`~ItemLoader.add_css`, and :meth:`~ItemLoader.add_value` calls."
msgstr ""

#: ../../topics/loaders.rst:83
msgid "Input and Output processors"
msgstr ""

#: ../../topics/loaders.rst:85
msgid "An Item Loader contains one input processor and one output processor for each (item) field. The input processor processes the extracted data as soon as it's received (through the :meth:`~ItemLoader.add_xpath`, :meth:`~ItemLoader.add_css` or :meth:`~ItemLoader.add_value` methods) and the result of the input processor is collected and kept inside the ItemLoader. After collecting all data, the :meth:`ItemLoader.load_item` method is called to populate and get the populated :class:`~scrapy.item.Item` object.  That's when the output processor is called with the data previously collected (and processed using the input processor). The result of the output processor is the final value that gets assigned to the item."
msgstr ""

#: ../../topics/loaders.rst:96
msgid "Let's see an example to illustrate how the input and output processors are called for a particular field (the same applies for any other field)::"
msgstr ""

#: ../../topics/loaders.rst:106
msgid "So what happens is:"
msgstr ""

#: ../../topics/loaders.rst:108
msgid "Data from ``xpath1`` is extracted, and passed through the *input processor* of the ``name`` field. The result of the input processor is collected and kept in the Item Loader (but not yet assigned to the item)."
msgstr ""

#: ../../topics/loaders.rst:112
msgid "Data from ``xpath2`` is extracted, and passed through the same *input processor* used in (1). The result of the input processor is appended to the data collected in (1) (if any)."
msgstr ""

#: ../../topics/loaders.rst:116
msgid "This case is similar to the previous ones, except that the data is extracted from the ``css`` CSS selector, and passed through the same *input processor* used in (1) and (2). The result of the input processor is appended to the data collected in (1) and (2) (if any)."
msgstr ""

#: ../../topics/loaders.rst:121
msgid "This case is also similar to the previous ones, except that the value to be collected is assigned directly, instead of being extracted from a XPath expression or a CSS selector. However, the value is still passed through the input processors. In this case, since the value is not iterable it is converted to an iterable of a single element before passing it to the input processor, because input processor always receive iterables."
msgstr ""

#: ../../topics/loaders.rst:129
msgid "The data collected in steps (1), (2), (3) and (4) is passed through the *output processor* of the ``name`` field. The result of the output processor is the value assigned to the ``name`` field in the item."
msgstr ""

#: ../../topics/loaders.rst:134
msgid "It's worth noticing that processors are just callable objects, which are called with the data to be parsed, and return a parsed value. So you can use any function as input or output processor. The only requirement is that they must accept one (and only one) positional argument, which will be an iterable."
msgstr ""

#: ../../topics/loaders.rst:139
msgid "Processors no longer need to be methods."
msgstr ""

#: ../../topics/loaders.rst:142
msgid "Both input and output processors must receive an iterable as their first argument. The output of those functions can be anything. The result of input processors will be appended to an internal list (in the Loader) containing the collected values (for that field). The result of the output processors is the value that will be finally assigned to the item."
msgstr ""

#: ../../topics/loaders.rst:148
msgid "The other thing you need to keep in mind is that the values returned by input processors are collected internally (in lists) and then passed to output processors to populate the fields."
msgstr ""

#: ../../topics/loaders.rst:152
msgid "Last, but not least, Scrapy comes with some :ref:`commonly used processors <topics-loaders-available-processors>` built-in for convenience."
msgstr ""

#: ../../topics/loaders.rst:158
msgid "Declaring Item Loaders"
msgstr ""

#: ../../topics/loaders.rst:160
msgid "Item Loaders are declared like Items, by using a class definition syntax. Here is an example::"
msgstr ""

#: ../../topics/loaders.rst:177
msgid "As you can see, input processors are declared using the ``_in`` suffix while output processors are declared using the ``_out`` suffix. And you can also declare a default input/output processors using the :attr:`ItemLoader.default_input_processor` and :attr:`ItemLoader.default_output_processor` attributes."
msgstr ""

#: ../../topics/loaders.rst:186
msgid "Declaring Input and Output Processors"
msgstr ""

#: ../../topics/loaders.rst:188
msgid "As seen in the previous section, input and output processors can be declared in the Item Loader definition, and it's very common to declare input processors this way. However, there is one more place where you can specify the input and output processors to use: in the :ref:`Item Field <topics-items-fields>` metadata. Here is an example::"
msgstr ""

#: ../../topics/loaders.rst:219
msgid "The precedence order, for both input and output processors, is as follows:"
msgstr ""

#: ../../topics/loaders.rst:221
msgid "Item Loader field-specific attributes: ``field_in`` and ``field_out`` (most precedence)"
msgstr ""

#: ../../topics/loaders.rst:223
msgid "Field metadata (``input_processor`` and ``output_processor`` key)"
msgstr ""

#: ../../topics/loaders.rst:224
msgid "Item Loader defaults: :meth:`ItemLoader.default_input_processor` and :meth:`ItemLoader.default_output_processor` (least precedence)"
msgstr ""

#: ../../topics/loaders.rst:227
msgid "See also: :ref:`topics-loaders-extending`."
msgstr ""

#: ../../topics/loaders.rst:232
msgid "Item Loader Context"
msgstr ""

#: ../../topics/loaders.rst:234
msgid "The Item Loader Context is a dict of arbitrary key/values which is shared among all input and output processors in the Item Loader. It can be passed when declaring, instantiating or using Item Loader. They are used to modify the behaviour of the input/output processors."
msgstr ""

#: ../../topics/loaders.rst:239
msgid "For example, suppose you have a function ``parse_length`` which receives a text value and extracts a length from it::"
msgstr ""

#: ../../topics/loaders.rst:247
msgid "By accepting a ``loader_context`` argument the function is explicitly telling the Item Loader that it's able to receive an Item Loader context, so the Item Loader passes the currently active context when calling it, and the processor function (``parse_length`` in this case) can thus use them."
msgstr ""

#: ../../topics/loaders.rst:252
msgid "There are several ways to modify Item Loader context values:"
msgstr ""

#: ../../topics/loaders.rst:254
msgid "By modifying the currently active Item Loader context (:attr:`~ItemLoader.context` attribute)::"
msgstr ""

#: ../../topics/loaders.rst:260
msgid "On Item Loader instantiation (the keyword arguments of Item Loader ``__init__`` method are stored in the Item Loader context)::"
msgstr ""

#: ../../topics/loaders.rst:265
msgid "On Item Loader declaration, for those input/output processors that support instantiating them with an Item Loader context. :class:`~processor.MapCompose` is one of them::"
msgstr ""

#: ../../topics/loaders.rst:274
msgid "ItemLoader objects"
msgstr ""

#: ../../topics/loaders.rst:278
msgid "Return a new Item Loader for populating the given Item. If no item is given, one is instantiated automatically using the class in :attr:`default_item_class`."
msgstr ""

#: ../../topics/loaders.rst:282
msgid "When instantiated with a ``selector`` or a ``response`` parameters the :class:`ItemLoader` class provides convenient mechanisms for extracting data from web pages using :ref:`selectors <topics-selectors>`."
msgstr ""

#: ../../topics/loaders.rst:286
msgid "The item instance to populate using subsequent calls to :meth:`~ItemLoader.add_xpath`, :meth:`~ItemLoader.add_css`, or :meth:`~ItemLoader.add_value`."
msgstr ""

#: ../../topics/loaders.rst:291
msgid "The selector to extract data from, when using the :meth:`add_xpath` (resp. :meth:`add_css`) or :meth:`replace_xpath` (resp. :meth:`replace_css`) method."
msgstr ""

#: ../../topics/loaders.rst:296
msgid "The response used to construct the selector using the :attr:`default_selector_class`, unless the selector argument is given, in which case this argument is ignored."
msgstr ""

#: ../../topics/loaders.rst:301
msgid "The item, selector, response and the remaining keyword arguments are assigned to the Loader context (accessible through the :attr:`context` attribute)."
msgstr ""

#: ../../topics/loaders.rst:304
msgid ":class:`ItemLoader` instances have the following methods:"
msgstr ""

#: ../../topics/loaders.rst:308
msgid "Process the given ``value`` by the given ``processors`` and keyword arguments."
msgstr ""

#: ../../topics/loaders.rst:311
msgid "Available keyword arguments:"
msgstr ""

#: ../../topics/loaders.rst:313
msgid "a regular expression to use for extracting data from the given value using :meth:`~scrapy.utils.misc.extract_regex` method, applied before processors"
msgstr ""

#: ../../topics/loaders.rst:318
#: ../../topics/loaders.rst:671
#: ../../topics/selectors.rst:258
msgid "Examples:"
msgstr ""

#: ../../topics/loaders.rst:326
msgid "Process and then add the given ``value`` for the given field."
msgstr ""

#: ../../topics/loaders.rst:328
msgid "The value is first passed through :meth:`get_value` by giving the ``processors`` and ``kwargs``, and then passed through the :ref:`field input processor <topics-loaders-processors>` and its result appended to the data collected for that field. If the field already contains collected data, the new data is added."
msgstr ""

#: ../../topics/loaders.rst:334
msgid "The given ``field_name`` can be ``None``, in which case values for multiple fields may be added. And the processed value should be a dict with field_name mapped to values."
msgstr ""

#: ../../topics/loaders.rst:338
#: ../../topics/loaders.rst:363
#: ../../topics/loaders.rst:381
#: ../../topics/loaders.rst:406
#: ../../topics/loaders.rst:424
msgid "Examples::"
msgstr ""

#: ../../topics/loaders.rst:348
msgid "Similar to :meth:`add_value` but replaces the collected data with the new value instead of adding it."
msgstr ""

#: ../../topics/loaders.rst:352
msgid "Similar to :meth:`ItemLoader.get_value` but receives an XPath instead of a value, which is used to extract a list of unicode strings from the selector associated with this :class:`ItemLoader`."
msgstr ""

#: ../../topics/loaders.rst:356
#: ../../topics/loaders.rst:378
msgid "the XPath to extract data from"
msgstr ""

#: ../../topics/loaders.rst:359
msgid "a regular expression to use for extracting data from the selected XPath region"
msgstr ""

#: ../../topics/loaders.rst:372
msgid "Similar to :meth:`ItemLoader.add_value` but receives an XPath instead of a value, which is used to extract a list of unicode strings from the selector associated with this :class:`ItemLoader`."
msgstr ""

#: ../../topics/loaders.rst:376
msgid "See :meth:`get_xpath` for ``kwargs``."
msgstr ""

#: ../../topics/loaders.rst:390
msgid "Similar to :meth:`add_xpath` but replaces collected data instead of adding it."
msgstr ""

#: ../../topics/loaders.rst:395
msgid "Similar to :meth:`ItemLoader.get_value` but receives a CSS selector instead of a value, which is used to extract a list of unicode strings from the selector associated with this :class:`ItemLoader`."
msgstr ""

#: ../../topics/loaders.rst:399
#: ../../topics/loaders.rst:421
msgid "the CSS selector to extract data from"
msgstr ""

#: ../../topics/loaders.rst:402
msgid "a regular expression to use for extracting data from the selected CSS region"
msgstr ""

#: ../../topics/loaders.rst:415
msgid "Similar to :meth:`ItemLoader.add_value` but receives a CSS selector instead of a value, which is used to extract a list of unicode strings from the selector associated with this :class:`ItemLoader`."
msgstr ""

#: ../../topics/loaders.rst:419
msgid "See :meth:`get_css` for ``kwargs``."
msgstr ""

#: ../../topics/loaders.rst:433
msgid "Similar to :meth:`add_css` but replaces collected data instead of adding it."
msgstr ""

#: ../../topics/loaders.rst:438
msgid "Populate the item with the data collected so far, and return it. The data collected is first passed through the :ref:`output processors <topics-loaders-processors>` to get the final value to assign to each item field."
msgstr ""

#: ../../topics/loaders.rst:445
msgid "Create a nested loader with an xpath selector. The supplied selector is applied relative to selector associated with this :class:`ItemLoader`. The nested loader shares the :class:`Item` with the parent :class:`ItemLoader` so calls to :meth:`add_xpath`, :meth:`add_value`, :meth:`replace_value`, etc. will behave as expected."
msgstr ""

#: ../../topics/loaders.rst:453
msgid "Create a nested loader with a css selector. The supplied selector is applied relative to selector associated with this :class:`ItemLoader`. The nested loader shares the :class:`Item` with the parent :class:`ItemLoader` so calls to :meth:`add_xpath`, :meth:`add_value`, :meth:`replace_value`, etc. will behave as expected."
msgstr ""

#: ../../topics/loaders.rst:461
msgid "Return the collected values for the given field."
msgstr ""

#: ../../topics/loaders.rst:465
msgid "Return the collected values parsed using the output processor, for the given field. This method doesn't populate or modify the item at all."
msgstr ""

#: ../../topics/loaders.rst:470
msgid "Return the input processor for the given field."
msgstr ""

#: ../../topics/loaders.rst:474
msgid "Return the output processor for the given field."
msgstr ""

#: ../../topics/loaders.rst:476
msgid ":class:`ItemLoader` instances have the following attributes:"
msgstr ""

#: ../../topics/loaders.rst:480
msgid "The :class:`~scrapy.item.Item` object being parsed by this Item Loader. This is mostly used as a property so when attempting to override this value, you may want to check out :attr:`default_item_class` first."
msgstr ""

#: ../../topics/loaders.rst:486
msgid "The currently active :ref:`Context <topics-loaders-context>` of this Item Loader."
msgstr ""

#: ../../topics/loaders.rst:491
msgid "An Item class (or factory), used to instantiate items when not given in the ``__init__`` method."
msgstr ""

#: ../../topics/loaders.rst:496
msgid "The default input processor to use for those fields which don't specify one."
msgstr ""

#: ../../topics/loaders.rst:501
msgid "The default output processor to use for those fields which don't specify one."
msgstr ""

#: ../../topics/loaders.rst:506
msgid "The class used to construct the :attr:`selector` of this :class:`ItemLoader`, if only a response is given in the ``__init__`` method. If a selector is given in the ``__init__`` method this attribute is ignored. This attribute is sometimes overridden in subclasses."
msgstr ""

#: ../../topics/loaders.rst:513
msgid "The :class:`~scrapy.selector.Selector` object to extract data from. It's either the selector given in the ``__init__`` method or one created from the response given in the ``__init__`` method using the :attr:`default_selector_class`. This attribute is meant to be read-only."
msgstr ""

#: ../../topics/loaders.rst:522
msgid "Nested Loaders"
msgstr ""

#: ../../topics/loaders.rst:524
msgid "When parsing related values from a subsection of a document, it can be useful to create nested loaders.  Imagine you're extracting details from a footer of a page that looks something like:"
msgstr ""

#: ../../topics/loaders.rst:536
msgid "Without nested loaders, you need to specify the full xpath (or css) for each value that you wish to extract."
msgstr ""

#: ../../topics/loaders.rst:547
msgid "Instead, you can create a nested loader with the footer selector and add values relative to the footer.  The functionality is the same but you avoid repeating the footer selector."
msgstr ""

#: ../../topics/loaders.rst:561
msgid "You can nest loaders arbitrarily and they work with either xpath or css selectors. As a general guideline, use nested loaders when they make your code simpler but do not go overboard with nesting or your parser can become difficult to read."
msgstr ""

#: ../../topics/loaders.rst:568
msgid "Reusing and extending Item Loaders"
msgstr ""

#: ../../topics/loaders.rst:570
msgid "As your project grows bigger and acquires more and more spiders, maintenance becomes a fundamental problem, especially when you have to deal with many different parsing rules for each spider, having a lot of exceptions, but also wanting to reuse the common processors."
msgstr ""

#: ../../topics/loaders.rst:575
msgid "Item Loaders are designed to ease the maintenance burden of parsing rules, without losing flexibility and, at the same time, providing a convenient mechanism for extending and overriding them. For this reason Item Loaders support traditional Python class inheritance for dealing with differences of specific spiders (or groups of spiders)."
msgstr ""

#: ../../topics/loaders.rst:581
msgid "Suppose, for example, that some particular site encloses their product names in three dashes (e.g. ``---Plasma TV---``) and you don't want to end up scraping those dashes in the final product names."
msgstr ""

#: ../../topics/loaders.rst:585
msgid "Here's how you can remove those dashes by reusing and extending the default Product Item Loader (``ProductLoader``)::"
msgstr ""

#: ../../topics/loaders.rst:597
msgid "Another case where extending Item Loaders can be very helpful is when you have multiple source formats, for example XML and HTML. In the XML version you may want to remove ``CDATA`` occurrences. Here's an example of how to do it::"
msgstr ""

#: ../../topics/loaders.rst:608
msgid "And that's how you typically extend input processors."
msgstr ""

#: ../../topics/loaders.rst:610
msgid "As for output processors, it is more common to declare them in the field metadata, as they usually depend only on the field and not on each specific site parsing rule (as input processors do). See also: :ref:`topics-loaders-processors-declaring`."
msgstr ""

#: ../../topics/loaders.rst:615
msgid "There are many other possible ways to extend, inherit and override your Item Loaders, and different Item Loaders hierarchies may fit better for different projects. Scrapy only provides the mechanism; it doesn't impose any specific organization of your Loaders collection - that's up to you and your project's needs."
msgstr ""

#: ../../topics/loaders.rst:624
msgid "Available built-in processors"
msgstr ""

#: ../../topics/loaders.rst:629
msgid "Even though you can use any callable function as input and output processors, Scrapy provides some commonly used processors, which are described below. Some of them, like the :class:`MapCompose` (which is typically used as input processor) compose the output of several functions executed in order, to produce the final parsed value."
msgstr ""

#: ../../topics/loaders.rst:635
msgid "Here is a list of all built-in processors:"
msgstr ""

#: ../../topics/loaders.rst:639
msgid "The simplest processor, which doesn't do anything. It returns the original values unchanged. It doesn't receive any ``__init__`` method arguments, nor does it accept Loader contexts."
msgstr ""

#: ../../topics/loaders.rst:652
msgid "Returns the first non-null/non-empty value from the values received, so it's typically used as an output processor to single-valued fields. It doesn't receive any ``__init__`` method arguments, nor does it accept Loader contexts."
msgstr ""

#: ../../topics/loaders.rst:665
msgid "Returns the values joined with the separator given in the ``__init__`` method, which defaults to ``u' '``. It doesn't accept Loader contexts."
msgstr ""

#: ../../topics/loaders.rst:668
msgid "When using the default separator, this processor is equivalent to the function: ``u' '.join``"
msgstr ""

#: ../../topics/loaders.rst:683
msgid "A processor which is constructed from the composition of the given functions. This means that each input value of this processor is passed to the first function, and the result of that function is passed to the second function, and so on, until the last function returns the output value of this processor."
msgstr ""

#: ../../topics/loaders.rst:689
msgid "By default, stop process on ``None`` value. This behaviour can be changed by passing keyword argument ``stop_on_none=False``."
msgstr ""

#: ../../topics/loaders.rst:699
msgid "Each function can optionally receive a ``loader_context`` parameter. For those which do, this processor will pass the currently active :ref:`Loader context <topics-loaders-context>` through that parameter."
msgstr ""

#: ../../topics/loaders.rst:703
msgid "The keyword arguments passed in the ``__init__`` method are used as the default Loader context values passed to each function call. However, the final Loader context values passed to functions are overridden with the currently active Loader context accessible through the :meth:`ItemLoader.context` attribute."
msgstr ""

#: ../../topics/loaders.rst:711
msgid "A processor which is constructed from the composition of the given functions, similar to the :class:`Compose` processor. The difference with this processor is the way internal results are passed among functions, which is as follows:"
msgstr ""

#: ../../topics/loaders.rst:716
msgid "The input value of this processor is *iterated* and the first function is applied to each element. The results of these function calls (one for each element) are concatenated to construct a new iterable, which is then used to apply the second function, and so on, until the last function is applied to each value of the list of values collected so far. The output values of the last function are concatenated together to produce the output of this processor."
msgstr ""

#: ../../topics/loaders.rst:723
msgid "Each particular function can return a value or a list of values, which is flattened with the list of values returned by the same function applied to the other input values. The functions can also return ``None`` in which case the output of that function is ignored for further processing over the chain."
msgstr ""

#: ../../topics/loaders.rst:729
msgid "This processor provides a convenient way to compose functions that only work with single values (instead of iterables). For this reason the :class:`MapCompose` processor is typically used as input processor, since data is often extracted using the :meth:`~scrapy.selector.Selector.extract` method of :ref:`selectors <topics-selectors>`, which returns a list of unicode strings."
msgstr ""

#: ../../topics/loaders.rst:736
msgid "The example below should clarify how it works:"
msgstr ""

#: ../../topics/loaders.rst:746
msgid "As with the Compose processor, functions can receive Loader contexts, and ``__init__`` method keyword arguments are used as default context values. See :class:`Compose` processor for more info."
msgstr ""

#: ../../topics/loaders.rst:752
msgid "Queries the value using the json path provided to the ``__init__`` method and returns the output. Requires jmespath (https://github.com/jmespath/jmespath.py) to run. This processor takes only one input at a time."
msgstr ""

#: ../../topics/loaders.rst:765
msgid "Working with Json:"
msgstr ""

#: ../../topics/logging.rst:8
msgid ":mod:`scrapy.log` has been deprecated alongside its functions in favor of explicit calls to the Python standard logging. Keep reading to learn more about the new logging system."
msgstr ""

#: ../../topics/logging.rst:12
msgid "Scrapy uses :mod:`logging` for event logging. We'll provide some simple examples to get you started, but for more advanced use-cases it's strongly suggested to read thoroughly its documentation."
msgstr ""

#: ../../topics/logging.rst:16
msgid "Logging works out of the box, and can be configured to some extent with the Scrapy settings listed in :ref:`topics-logging-settings`."
msgstr ""

#: ../../topics/logging.rst:19
msgid "Scrapy calls :func:`scrapy.utils.log.configure_logging` to set some reasonable defaults and handle those settings in :ref:`topics-logging-settings` when running commands, so it's recommended to manually call it if you're running Scrapy from scripts as described in :ref:`run-from-script`."
msgstr ""

#: ../../topics/logging.rst:27
msgid "Log levels"
msgstr ""

#: ../../topics/logging.rst:29
msgid "Python's builtin logging defines 5 different levels to indicate the severity of a given log message. Here are the standard ones, listed in decreasing order:"
msgstr ""

#: ../../topics/logging.rst:32
msgid "``logging.CRITICAL`` - for critical errors (highest severity)"
msgstr ""

#: ../../topics/logging.rst:33
msgid "``logging.ERROR`` - for regular errors"
msgstr ""

#: ../../topics/logging.rst:34
msgid "``logging.WARNING`` - for warning messages"
msgstr ""

#: ../../topics/logging.rst:35
msgid "``logging.INFO`` - for informational messages"
msgstr ""

#: ../../topics/logging.rst:36
msgid "``logging.DEBUG`` - for debugging messages (lowest severity)"
msgstr ""

#: ../../topics/logging.rst:39
msgid "How to log messages"
msgstr ""

#: ../../topics/logging.rst:41
msgid "Here's a quick example of how to log a message using the ``logging.WARNING`` level::"
msgstr ""

#: ../../topics/logging.rst:47
msgid "There are shortcuts for issuing log messages on any of the standard 5 levels, and there's also a general ``logging.log`` method which takes a given level as argument.  If needed, the last example could be rewritten as::"
msgstr ""

#: ../../topics/logging.rst:54
msgid "On top of that, you can create different \"loggers\" to encapsulate messages. (For example, a common practice is to create different loggers for every module). These loggers can be configured independently, and they allow hierarchical constructions."
msgstr ""

#: ../../topics/logging.rst:59
msgid "The previous examples use the root logger behind the scenes, which is a top level logger where all messages are propagated to (unless otherwise specified). Using ``logging`` helpers is merely a shortcut for getting the root logger explicitly, so this is also an equivalent of the last snippets::"
msgstr ""

#: ../../topics/logging.rst:68
msgid "You can use a different logger just by getting its name with the ``logging.getLogger`` function::"
msgstr ""

#: ../../topics/logging.rst:75
msgid "Finally, you can ensure having a custom logger for any module you're working on by using the ``__name__`` variable, which is populated with current module's path::"
msgstr ""

#: ../../topics/logging.rst:86
msgid "Module logging, :doc:`HowTo <howto/logging>`"
msgstr ""

#: ../../topics/logging.rst:86
msgid "Basic Logging Tutorial"
msgstr ""

#: ../../topics/logging.rst:88
msgid "Module logging, :ref:`Loggers <logger>`"
msgstr ""

#: ../../topics/logging.rst:89
msgid "Further documentation on loggers"
msgstr ""

#: ../../topics/logging.rst:94
msgid "Logging from Spiders"
msgstr ""

#: ../../topics/logging.rst:96
msgid "Scrapy provides a :data:`~scrapy.spiders.Spider.logger` within each Spider instance, which can be accessed and used like this::"
msgstr ""

#: ../../topics/logging.rst:109
msgid "That logger is created using the Spider's name, but you can use any custom Python logger you want. For example::"
msgstr ""

#: ../../topics/logging.rst:128
msgid "Logging configuration"
msgstr ""

#: ../../topics/logging.rst:130
msgid "Loggers on their own don't manage how messages sent through them are displayed. For this task, different \"handlers\" can be attached to any logger instance and they will redirect those messages to appropriate destinations, such as the standard output, files, emails, etc."
msgstr ""

#: ../../topics/logging.rst:135
msgid "By default, Scrapy sets and configures a handler for the root logger, based on the settings below."
msgstr ""

#: ../../topics/logging.rst:141
msgid "Logging settings"
msgstr ""

#: ../../topics/logging.rst:143
msgid "These settings can be used to configure the logging:"
msgstr ""

#: ../../topics/logging.rst:145
msgid ":setting:`LOG_FILE`"
msgstr ""

#: ../../topics/logging.rst:146
msgid ":setting:`LOG_ENABLED`"
msgstr ""

#: ../../topics/logging.rst:147
msgid ":setting:`LOG_ENCODING`"
msgstr ""

#: ../../topics/logging.rst:148
msgid ":setting:`LOG_LEVEL`"
msgstr ""

#: ../../topics/logging.rst:149
msgid ":setting:`LOG_FORMAT`"
msgstr ""

#: ../../topics/logging.rst:150
msgid ":setting:`LOG_DATEFORMAT`"
msgstr ""

#: ../../topics/logging.rst:151
msgid ":setting:`LOG_STDOUT`"
msgstr ""

#: ../../topics/logging.rst:152
msgid ":setting:`LOG_SHORT_NAMES`"
msgstr ""

#: ../../topics/logging.rst:154
msgid "The first couple of settings define a destination for log messages. If :setting:`LOG_FILE` is set, messages sent through the root logger will be redirected to a file named :setting:`LOG_FILE` with encoding :setting:`LOG_ENCODING`. If unset and :setting:`LOG_ENABLED` is ``True``, log messages will be displayed on the standard error. Lastly, if :setting:`LOG_ENABLED` is ``False``, there won't be any visible log output."
msgstr ""

#: ../../topics/logging.rst:161
msgid ":setting:`LOG_LEVEL` determines the minimum level of severity to display, those messages with lower severity will be filtered out. It ranges through the possible levels listed in :ref:`topics-logging-levels`."
msgstr ""

#: ../../topics/logging.rst:165
msgid ":setting:`LOG_FORMAT` and :setting:`LOG_DATEFORMAT` specify formatting strings used as layouts for all messages. Those strings can contain any placeholders listed in :ref:`logging's logrecord attributes docs <logrecord-attributes>` and :ref:`datetime's strftime and strptime directives <strftime-strptime-behavior>` respectively."
msgstr ""

#: ../../topics/logging.rst:171
msgid "If :setting:`LOG_SHORT_NAMES` is set, then the logs will not display the Scrapy component that prints the log. It is unset by default, hence logs contain the Scrapy component responsible for that log output."
msgstr ""

#: ../../topics/logging.rst:176
msgid "Command-line options"
msgstr ""

#: ../../topics/logging.rst:178
msgid "There are command-line arguments, available for all commands, that you can use to override some of the Scrapy settings regarding logging."
msgstr ""

#: ../../topics/logging.rst:181
msgid "``--logfile FILE``"
msgstr ""

#: ../../topics/logging.rst:182
msgid "Overrides :setting:`LOG_FILE`"
msgstr ""

#: ../../topics/logging.rst:183
msgid "``--loglevel/-L LEVEL``"
msgstr ""

#: ../../topics/logging.rst:184
msgid "Overrides :setting:`LOG_LEVEL`"
msgstr ""

#: ../../topics/logging.rst:186
msgid "``--nolog``"
msgstr ""

#: ../../topics/logging.rst:186
msgid "Sets :setting:`LOG_ENABLED` to ``False``"
msgstr ""

#: ../../topics/logging.rst:190
msgid "Module :mod:`logging.handlers`"
msgstr ""

#: ../../topics/logging.rst:191
msgid "Further documentation on available handlers"
msgstr ""

#: ../../topics/logging.rst:196
msgid "Custom Log Formats"
msgstr ""

#: ../../topics/logging.rst:198
msgid "A custom log format can be set for different actions by extending :class:`~scrapy.logformatter.LogFormatter` class and making :setting:`LOG_FORMATTER` point to your new class."
msgstr ""

#: ../../../scrapy/logformatter.py:docstring of scrapy.logformatter.LogFormatter:1
msgid "Class for generating log messages for different actions."
msgstr ""

#: ../../../scrapy/logformatter.py:docstring of scrapy.logformatter.LogFormatter:3
msgid "All methods must return a dictionary listing the parameters ``level``, ``msg`` and ``args`` which are going to be used for constructing the log message when calling ``logging.log``."
msgstr ""

#: ../../../scrapy/logformatter.py:docstring of scrapy.logformatter.LogFormatter:7
msgid "Dictionary keys for the method outputs:"
msgstr ""

#: ../../../scrapy/logformatter.py:docstring of scrapy.logformatter.LogFormatter:9
msgid "``level`` is the log level for that action, you can use those from the `python logging library <https://docs.python.org/3/library/logging.html>`_ : ``logging.DEBUG``, ``logging.INFO``, ``logging.WARNING``, ``logging.ERROR`` and ``logging.CRITICAL``."
msgstr ""

#: ../../../scrapy/logformatter.py:docstring of scrapy.logformatter.LogFormatter:13
msgid "``msg`` should be a string that can contain different formatting placeholders. This string, formatted with the provided ``args``, is going to be the long message for that action."
msgstr ""

#: ../../../scrapy/logformatter.py:docstring of scrapy.logformatter.LogFormatter:16
msgid "``args`` should be a tuple or dict with the formatting placeholders for ``msg``. The final log message is computed as ``msg % args``."
msgstr ""

#: ../../../scrapy/logformatter.py:docstring of scrapy.logformatter.LogFormatter:19
msgid "Users can define their own ``LogFormatter`` class if they want to customize how each action is logged or if they want to omit it entirely. In order to omit logging an action the method must return ``None``."
msgstr ""

#: ../../../scrapy/logformatter.py:docstring of scrapy.logformatter.LogFormatter:23
msgid "Here is an example on how to create a custom log formatter to lower the severity level of the log message when an item is dropped from the pipeline::"
msgstr ""

#: ../../../scrapy/logformatter.py:docstring of scrapy.logformatter.LogFormatter.crawled:1
msgid "Logs a message when the crawler finds a webpage."
msgstr ""

#: ../../../scrapy/logformatter.py:docstring of scrapy.logformatter.LogFormatter.download_error:1
msgid "Logs a download error message from a spider (typically coming from the engine)."
msgstr ""

#: ../../../scrapy/logformatter.py:docstring of scrapy.logformatter.LogFormatter.dropped:1
msgid "Logs a message when an item is dropped while it is passing through the item pipeline."
msgstr ""

#: ../../../scrapy/logformatter.py:docstring of scrapy.logformatter.LogFormatter.item_error:1
msgid "Logs a message when an item causes an error while it is passing through the item pipeline."
msgstr ""

#: ../../../scrapy/logformatter.py:docstring of scrapy.logformatter.LogFormatter.scraped:1
msgid "Logs a message when an item is scraped by a spider."
msgstr ""

#: ../../../scrapy/logformatter.py:docstring of scrapy.logformatter.LogFormatter.spider_error:1
msgid "Logs an error message from a spider."
msgstr ""

#: ../../topics/logging.rst:206
msgid "Advanced customization"
msgstr ""

#: ../../topics/logging.rst:208
msgid "Because Scrapy uses stdlib logging module, you can customize logging using all features of stdlib logging."
msgstr ""

#: ../../topics/logging.rst:211
msgid "For example, let's say you're scraping a website which returns many HTTP 404 and 500 responses, and you want to hide all messages like this::"
msgstr ""

#: ../../topics/logging.rst:218
msgid "The first thing to note is a logger name - it is in brackets: ``[scrapy.spidermiddlewares.httperror]``. If you get just ``[scrapy]`` then :setting:`LOG_SHORT_NAMES` is likely set to True; set it to False and re-run the crawl."
msgstr ""

#: ../../topics/logging.rst:223
msgid "Next, we can see that the message has INFO level. To hide it we should set logging level for ``scrapy.spidermiddlewares.httperror`` higher than INFO; next level after INFO is WARNING. It could be done e.g. in the spider's ``__init__`` method::"
msgstr ""

#: ../../topics/logging.rst:239
msgid "If you run this spider again then INFO messages from ``scrapy.spidermiddlewares.httperror`` logger will be gone."
msgstr ""

#: ../../topics/logging.rst:243
msgid "scrapy.utils.log module"
msgstr ""

#: ../../../scrapy/utils/log.py:docstring of scrapy.utils.log.configure_logging:1
msgid "Initialize logging defaults for Scrapy."
msgstr ""

#: ../../../scrapy/utils/log.py:docstring of scrapy.utils.log.configure_logging:3
msgid "settings used to create and configure a handler for the root logger (default: None)."
msgstr ""

#: ../../../scrapy/utils/log.py:docstring of scrapy.utils.log.configure_logging:11
msgid "This function does:"
msgstr ""

#: ../../../scrapy/utils/log.py:docstring of scrapy.utils.log.configure_logging:13
msgid "Route warnings and twisted logging through Python standard logging"
msgstr ""

#: ../../../scrapy/utils/log.py:docstring of scrapy.utils.log.configure_logging:14
msgid "Assign DEBUG and ERROR level to Scrapy and Twisted loggers respectively"
msgstr ""

#: ../../../scrapy/utils/log.py:docstring of scrapy.utils.log.configure_logging:15
msgid "Route stdout to log if LOG_STDOUT setting is True"
msgstr ""

#: ../../../scrapy/utils/log.py:docstring of scrapy.utils.log.configure_logging:17
msgid "When ``install_root_handler`` is True (default), this function also creates a handler for the root logger according to given settings (see :ref:`topics-logging-settings`). You can override default options using ``settings`` argument. When ``settings`` is empty or None, defaults are used."
msgstr ""

#: ../../topics/logging.rst:250
msgid "``configure_logging`` is automatically called when using Scrapy commands or :class:`~scrapy.crawler.CrawlerProcess`, but needs to be called explicitly when running custom scripts using :class:`~scrapy.crawler.CrawlerRunner`. In that case, its usage is not required but it's recommended."
msgstr ""

#: ../../topics/logging.rst:255
msgid "Another option when running custom scripts is to manually configure the logging. To do this you can use :func:`logging.basicConfig` to set a basic root handler."
msgstr ""

#: ../../topics/logging.rst:258
msgid "Note that :class:`~scrapy.crawler.CrawlerProcess` automatically calls ``configure_logging``, so it is recommended to only use :func:`logging.basicConfig` together with :class:`~scrapy.crawler.CrawlerRunner`."
msgstr ""

#: ../../topics/logging.rst:262
msgid "This is an example on how to redirect ``INFO`` or higher messages to a file::"
msgstr ""

#: ../../topics/logging.rst:273
msgid "Refer to :ref:`run-from-script` for more details about using Scrapy this way."
msgstr ""

#: ../../topics/media-pipeline.rst:5
msgid "Downloading and processing files and images"
msgstr ""

#: ../../topics/media-pipeline.rst:9
msgid "Scrapy provides reusable :doc:`item pipelines </topics/item-pipeline>` for downloading files attached to a particular item (for example, when you scrape products and also want to download their images locally). These pipelines share a bit of functionality and structure (we refer to them as media pipelines), but typically you'll either use the Files Pipeline or the Images Pipeline."
msgstr ""

#: ../../topics/media-pipeline.rst:15
msgid "Both pipelines implement these features:"
msgstr ""

#: ../../topics/media-pipeline.rst:17
msgid "Avoid re-downloading media that was downloaded recently"
msgstr ""

#: ../../topics/media-pipeline.rst:18
msgid "Specifying where to store the media (filesystem directory, Amazon S3 bucket, Google Cloud Storage bucket)"
msgstr ""

#: ../../topics/media-pipeline.rst:21
msgid "The Images Pipeline has a few extra functions for processing images:"
msgstr ""

#: ../../topics/media-pipeline.rst:23
msgid "Convert all downloaded images to a common format (JPG) and mode (RGB)"
msgstr ""

#: ../../topics/media-pipeline.rst:24
msgid "Thumbnail generation"
msgstr ""

#: ../../topics/media-pipeline.rst:25
msgid "Check images width/height to make sure they meet a minimum constraint"
msgstr ""

#: ../../topics/media-pipeline.rst:27
msgid "The pipelines also keep an internal queue of those media URLs which are currently being scheduled for download, and connect those responses that arrive containing the same media to that queue. This avoids downloading the same media more than once when it's shared by several items."
msgstr ""

#: ../../topics/media-pipeline.rst:33
msgid "Using the Files Pipeline"
msgstr ""

#: ../../topics/media-pipeline.rst:35
msgid "The typical workflow, when using the :class:`FilesPipeline` goes like this:"
msgstr ""

#: ../../topics/media-pipeline.rst:38
msgid "In a Spider, you scrape an item and put the URLs of the desired into a ``file_urls`` field."
msgstr ""

#: ../../topics/media-pipeline.rst:41
msgid "The item is returned from the spider and goes to the item pipeline."
msgstr ""

#: ../../topics/media-pipeline.rst:43
msgid "When the item reaches the :class:`FilesPipeline`, the URLs in the ``file_urls`` field are scheduled for download using the standard Scrapy scheduler and downloader (which means the scheduler and downloader middlewares are reused), but with a higher priority, processing them before other pages are scraped. The item remains \"locked\" at that particular pipeline stage until the files have finish downloading (or fail for some reason)."
msgstr ""

#: ../../topics/media-pipeline.rst:50
msgid "When the files are downloaded, another field (``files``) will be populated with the results. This field will contain a list of dicts with information about the downloaded files, such as the downloaded path, the original scraped url (taken from the ``file_urls`` field) , and the file checksum. The files in the list of the ``files`` field will retain the same order of the original ``file_urls`` field. If some file failed downloading, an error will be logged and the file won't be present in the ``files`` field."
msgstr ""

#: ../../topics/media-pipeline.rst:60
msgid "Using the Images Pipeline"
msgstr ""

#: ../../topics/media-pipeline.rst:62
msgid "Using the :class:`ImagesPipeline` is a lot like using the :class:`FilesPipeline`, except the default field names used are different: you use ``image_urls`` for the image URLs of an item and it will populate an ``images`` field for the information about the downloaded images."
msgstr ""

#: ../../topics/media-pipeline.rst:67
msgid "The advantage of using the :class:`ImagesPipeline` for image files is that you can configure some extra functions like generating thumbnails and filtering the images based on their size."
msgstr ""

#: ../../topics/media-pipeline.rst:71
msgid "The Images Pipeline uses `Pillow`_ for thumbnailing and normalizing images to JPEG/RGB format, so you need to install this library in order to use it. `Python Imaging Library`_ (PIL) should also work in most cases, but it is known to cause troubles in some setups, so we recommend to use `Pillow`_ instead of PIL."
msgstr ""

#: ../../topics/media-pipeline.rst:84
msgid "Enabling your Media Pipeline"
msgstr ""

#: ../../topics/media-pipeline.rst:89
msgid "To enable your media pipeline you must first add it to your project :setting:`ITEM_PIPELINES` setting."
msgstr ""

#: ../../topics/media-pipeline.rst:92
msgid "For Images Pipeline, use::"
msgstr ""

#: ../../topics/media-pipeline.rst:96
msgid "For Files Pipeline, use::"
msgstr ""

#: ../../topics/media-pipeline.rst:101
msgid "You can also use both the Files and Images Pipeline at the same time."
msgstr ""

#: ../../topics/media-pipeline.rst:104
msgid "Then, configure the target storage setting to a valid value that will be used for storing the downloaded images. Otherwise the pipeline will remain disabled, even if you include it in the :setting:`ITEM_PIPELINES` setting."
msgstr ""

#: ../../topics/media-pipeline.rst:108
msgid "For the Files Pipeline, set the :setting:`FILES_STORE` setting::"
msgstr ""

#: ../../topics/media-pipeline.rst:112
msgid "For the Images Pipeline, set the :setting:`IMAGES_STORE` setting::"
msgstr ""

#: ../../topics/media-pipeline.rst:117
msgid "Supported Storage"
msgstr ""

#: ../../topics/media-pipeline.rst:120
msgid "File system storage"
msgstr ""

#: ../../topics/media-pipeline.rst:122
msgid "The files are stored using a `SHA1 hash`_ of their URLs for the file names."
msgstr ""

#: ../../topics/media-pipeline.rst:124
msgid "For example, the following image URL::"
msgstr ""

#: ../../topics/media-pipeline.rst:128
msgid "Whose ``SHA1 hash`` is::"
msgstr ""

#: ../../topics/media-pipeline.rst:132
msgid "Will be downloaded and stored in the following file::"
msgstr ""

#: ../../topics/media-pipeline.rst:136
#: ../../topics/media-pipeline.rst:344
msgid "Where:"
msgstr ""

#: ../../topics/media-pipeline.rst:138
msgid "``<IMAGES_STORE>`` is the directory defined in :setting:`IMAGES_STORE` setting for the Images Pipeline."
msgstr ""

#: ../../topics/media-pipeline.rst:141
msgid "``full`` is a sub-directory to separate full images from thumbnails (if used). For more info see :ref:`topics-images-thumbnails`."
msgstr ""

#: ../../topics/media-pipeline.rst:147
msgid "FTP server storage"
msgstr ""

#: ../../topics/media-pipeline.rst:151
msgid ":setting:`FILES_STORE` and :setting:`IMAGES_STORE` can point to an FTP server. Scrapy will automatically upload the files to the server."
msgstr ""

#: ../../topics/media-pipeline.rst:154
msgid ":setting:`FILES_STORE` and :setting:`IMAGES_STORE` should be written in one of the following forms::"
msgstr ""

#: ../../topics/media-pipeline.rst:160
msgid "If ``username`` and ``password`` are not provided, they are taken from the :setting:`FTP_USER` and :setting:`FTP_PASSWORD` settings respectively."
msgstr ""

#: ../../topics/media-pipeline.rst:163
msgid "FTP supports two different connection modes: active or passive. Scrapy uses the passive connection mode by default. To use the active connection mode instead, set the :setting:`FEED_STORAGE_FTP_ACTIVE` setting to ``True``."
msgstr ""

#: ../../topics/media-pipeline.rst:168
msgid "Amazon S3 storage"
msgstr ""

#: ../../topics/media-pipeline.rst:173
msgid ":setting:`FILES_STORE` and :setting:`IMAGES_STORE` can represent an Amazon S3 bucket. Scrapy will automatically upload the files to the bucket."
msgstr ""

#: ../../topics/media-pipeline.rst:176
msgid "For example, this is a valid :setting:`IMAGES_STORE` value::"
msgstr ""

#: ../../topics/media-pipeline.rst:180
msgid "You can modify the Access Control List (ACL) policy used for the stored files, which is defined by the :setting:`FILES_STORE_S3_ACL` and :setting:`IMAGES_STORE_S3_ACL` settings. By default, the ACL is set to ``private``. To make the files publicly available use the ``public-read`` policy::"
msgstr ""

#: ../../topics/media-pipeline.rst:188
msgid "For more information, see `canned ACLs`_ in the Amazon S3 Developer Guide."
msgstr ""

#: ../../topics/media-pipeline.rst:190
msgid "Because Scrapy uses ``botocore`` internally you can also use other S3-like storages. Storages like self-hosted `Minio`_ or `s3.scality`_. All you need to do is set endpoint option in you Scrapy settings::"
msgstr ""

#: ../../topics/media-pipeline.rst:195
msgid "For self-hosting you also might feel the need not to use SSL and not to verify SSL connection::"
msgstr ""

#: ../../topics/media-pipeline.rst:205
msgid "Google Cloud Storage"
msgstr ""

#: ../../topics/media-pipeline.rst:211
msgid ":setting:`FILES_STORE` and :setting:`IMAGES_STORE` can represent a Google Cloud Storage bucket. Scrapy will automatically upload the files to the bucket. (requires `google-cloud-storage`_ )"
msgstr ""

#: ../../topics/media-pipeline.rst:216
msgid "For example, these are valid :setting:`IMAGES_STORE` and :setting:`GCS_PROJECT_ID` settings::"
msgstr ""

#: ../../topics/media-pipeline.rst:221
msgid "For information about authentication, see this `documentation`_."
msgstr ""

#: ../../topics/media-pipeline.rst:225
msgid "You can modify the Access Control List (ACL) policy used for the stored files, which is defined by the :setting:`FILES_STORE_GCS_ACL` and :setting:`IMAGES_STORE_GCS_ACL` settings. By default, the ACL is set to ``''`` (empty string) which means that Cloud Storage applies the bucket's default object ACL to the object. To make the files publicly available use the ``publicRead`` policy::"
msgstr ""

#: ../../topics/media-pipeline.rst:234
msgid "For more information, see `Predefined ACLs`_ in the Google Cloud Platform Developer Guide."
msgstr ""

#: ../../topics/media-pipeline.rst:239
msgid "Usage example"
msgstr ""

#: ../../topics/media-pipeline.rst:246
msgid "In order to use a media pipeline first, :ref:`enable it <topics-media-pipeline-enabling>`."
msgstr ""

#: ../../topics/media-pipeline.rst:249
msgid "Then, if a spider returns a dict with the URLs key (``file_urls`` or ``image_urls``, for the Files or Images Pipeline respectively), the pipeline will put the results under respective key (``files`` or ``images``)."
msgstr ""

#: ../../topics/media-pipeline.rst:253
msgid "If you prefer to use :class:`~.Item`, then define a custom item with the necessary fields, like in this example for Images Pipeline::"
msgstr ""

#: ../../topics/media-pipeline.rst:264
msgid "If you want to use another field name for the URLs key or for the results key, it is also possible to override it."
msgstr ""

#: ../../topics/media-pipeline.rst:267
msgid "For the Files Pipeline, set :setting:`FILES_URLS_FIELD` and/or :setting:`FILES_RESULT_FIELD` settings::"
msgstr ""

#: ../../topics/media-pipeline.rst:273
msgid "For the Images Pipeline, set :setting:`IMAGES_URLS_FIELD` and/or :setting:`IMAGES_RESULT_FIELD` settings::"
msgstr ""

#: ../../topics/media-pipeline.rst:279
msgid "If you need something more complex and want to override the custom pipeline behaviour, see :ref:`topics-media-pipeline-override`."
msgstr ""

#: ../../topics/media-pipeline.rst:282
msgid "If you have multiple image pipelines inheriting from ImagePipeline and you want to have different settings in different pipelines you can set setting keys preceded with uppercase name of your pipeline class. E.g. if your pipeline is called MyPipeline and you want to have custom IMAGES_URLS_FIELD you define setting MYPIPELINE_IMAGES_URLS_FIELD and your custom settings will be used."
msgstr ""

#: ../../topics/media-pipeline.rst:290
msgid "Additional features"
msgstr ""

#: ../../topics/media-pipeline.rst:293
msgid "File expiration"
msgstr ""

#: ../../topics/media-pipeline.rst:298
msgid "The Image Pipeline avoids downloading files that were downloaded recently. To adjust this retention delay use the :setting:`FILES_EXPIRES` setting (or :setting:`IMAGES_EXPIRES`, in case of Images Pipeline), which specifies the delay in number of days::"
msgstr ""

#: ../../topics/media-pipeline.rst:309
msgid "The default value for both settings is 90 days."
msgstr ""

#: ../../topics/media-pipeline.rst:311
msgid "If you have pipeline that subclasses FilesPipeline and you'd like to have different setting for it you can set setting keys preceded by uppercase class name. E.g. given pipeline class called MyPipeline you can set setting key:"
msgstr ""

#: ../../topics/media-pipeline.rst:315
msgid "MYPIPELINE_FILES_EXPIRES = 180"
msgstr ""

#: ../../topics/media-pipeline.rst:317
msgid "and pipeline class MyPipeline will have expiration time set to 180."
msgstr ""

#: ../../topics/media-pipeline.rst:322
msgid "Thumbnail generation for images"
msgstr ""

#: ../../topics/media-pipeline.rst:324
msgid "The Images Pipeline can automatically create thumbnails of the downloaded images."
msgstr ""

#: ../../topics/media-pipeline.rst:329
msgid "In order to use this feature, you must set :setting:`IMAGES_THUMBS` to a dictionary where the keys are the thumbnail names and the values are their dimensions."
msgstr ""

#: ../../topics/media-pipeline.rst:339
msgid "When you use this feature, the Images Pipeline will create thumbnails of the each specified size with this format::"
msgstr ""

#: ../../topics/media-pipeline.rst:346
msgid "``<size_name>`` is the one specified in the :setting:`IMAGES_THUMBS` dictionary keys (``small``, ``big``, etc)"
msgstr ""

#: ../../topics/media-pipeline.rst:349
msgid "``<image_id>`` is the `SHA1 hash`_ of the image url"
msgstr ""

#: ../../topics/media-pipeline.rst:353
msgid "Example of image files stored using ``small`` and ``big`` thumbnail names::"
msgstr ""

#: ../../topics/media-pipeline.rst:359
msgid "The first one is the full image, as downloaded from the site."
msgstr ""

#: ../../topics/media-pipeline.rst:362
msgid "Filtering out small images"
msgstr ""

#: ../../topics/media-pipeline.rst:368
msgid "When using the Images Pipeline, you can drop images which are too small, by specifying the minimum allowed size in the :setting:`IMAGES_MIN_HEIGHT` and :setting:`IMAGES_MIN_WIDTH` settings."
msgstr ""

#: ../../topics/media-pipeline.rst:378
msgid "The size constraints don't affect thumbnail generation at all."
msgstr ""

#: ../../topics/media-pipeline.rst:380
msgid "It is possible to set just one size constraint or both. When setting both of them, only images that satisfy both minimum sizes will be saved. For the above example, images of sizes (105 x 105) or (105 x 200) or (200 x 105) will all be dropped because at least one dimension is shorter than the constraint."
msgstr ""

#: ../../topics/media-pipeline.rst:385
msgid "By default, there are no size constraints, so all images are processed."
msgstr ""

#: ../../topics/media-pipeline.rst:388
msgid "Allowing redirections"
msgstr ""

#: ../../topics/media-pipeline.rst:392
msgid "By default media pipelines ignore redirects, i.e. an HTTP redirection to a media file URL request will mean the media download is considered failed."
msgstr ""

#: ../../topics/media-pipeline.rst:395
msgid "To handle media redirections, set this setting to ``True``::"
msgstr ""

#: ../../topics/media-pipeline.rst:402
msgid "Extending the Media Pipelines"
msgstr ""

#: ../../topics/media-pipeline.rst:407
msgid "See here the methods that you can override in your custom Files Pipeline:"
msgstr ""

#: ../../topics/media-pipeline.rst:413
#: ../../topics/media-pipeline.rst:527
msgid "This method is called once per downloaded item. It returns the download path of the file originating from the specified :class:`response <scrapy.http.Response>`."
msgstr ""

#: ../../topics/media-pipeline.rst:417
#: ../../topics/media-pipeline.rst:531
msgid "In addition to ``response``, this method receives the original :class:`request <scrapy.Request>` and :class:`info <scrapy.pipelines.media.MediaPipeline.SpiderInfo>`."
msgstr ""

#: ../../topics/media-pipeline.rst:421
#: ../../topics/media-pipeline.rst:535
msgid "You can override this method to customize the download path of each file."
msgstr ""

#: ../../topics/media-pipeline.rst:423
#: ../../topics/media-pipeline.rst:537
msgid "For example, if file URLs end like regular paths (e.g. ``https://example.com/a/b/c/foo.png``), you can use the following approach to download all files into the ``files`` folder with their original filenames (e.g. ``files/foo.png``)::"
msgstr ""

#: ../../topics/media-pipeline.rst:438
#: ../../topics/media-pipeline.rst:552
msgid "By default the :meth:`file_path` method returns ``full/<request URL hash>.<extension>``."
msgstr ""

#: ../../topics/media-pipeline.rst:443
msgid "As seen on the workflow, the pipeline will get the URLs of the images to download from the item. In order to do this, you can override the :meth:`~get_media_requests` method and return a Request for each file URL::"
msgstr ""

#: ../../topics/media-pipeline.rst:452
msgid "Those requests will be processed by the pipeline and, when they have finished downloading, the results will be sent to the :meth:`~item_completed` method, as a list of 2-element tuples. Each tuple will contain ``(success, file_info_or_error)`` where:"
msgstr ""

#: ../../topics/media-pipeline.rst:457
msgid "``success`` is a boolean which is ``True`` if the image was downloaded successfully or ``False`` if it failed for some reason"
msgstr ""

#: ../../topics/media-pipeline.rst:460
msgid "``file_info_or_error`` is a dict containing the following keys (if success is ``True``) or a :exc:`~twisted.python.failure.Failure` if there was a problem."
msgstr ""

#: ../../topics/media-pipeline.rst:464
msgid "``url`` - the url where the file was downloaded from. This is the url of the request returned from the :meth:`~get_media_requests` method."
msgstr ""

#: ../../topics/media-pipeline.rst:468
msgid "``path`` - the path (relative to :setting:`FILES_STORE`) where the file was stored"
msgstr ""

#: ../../topics/media-pipeline.rst:471
msgid "``checksum`` - a `MD5 hash`_ of the image contents"
msgstr ""

#: ../../topics/media-pipeline.rst:473
msgid "The list of tuples received by :meth:`~item_completed` is guaranteed to retain the same order of the requests returned from the :meth:`~get_media_requests` method."
msgstr ""

#: ../../topics/media-pipeline.rst:477
msgid "Here's a typical value of the ``results`` argument::"
msgstr ""

#: ../../topics/media-pipeline.rst:486
msgid "By default the :meth:`get_media_requests` method returns ``None`` which means there are no files to download for the item."
msgstr ""

#: ../../topics/media-pipeline.rst:491
msgid "The :meth:`FilesPipeline.item_completed` method called when all file requests for a single item have completed (either finished downloading, or failed for some reason)."
msgstr ""

#: ../../topics/media-pipeline.rst:495
msgid "The :meth:`~item_completed` method must return the output that will be sent to subsequent item pipeline stages, so you must return (or drop) the item, as you would in any pipeline."
msgstr ""

#: ../../topics/media-pipeline.rst:499
msgid "Here is an example of the :meth:`~item_completed` method where we store the downloaded file paths (passed in results) in the ``file_paths`` item field, and we drop the item if it doesn't contain any files::"
msgstr ""

#: ../../topics/media-pipeline.rst:512
#: ../../topics/media-pipeline.rst:571
msgid "By default, the :meth:`item_completed` method returns the item."
msgstr ""

#: ../../topics/media-pipeline.rst:518
msgid "See here the methods that you can override in your custom Images Pipeline:"
msgstr ""

#: ../../topics/media-pipeline.rst:522
msgid "The :class:`ImagesPipeline` is an extension of the :class:`FilesPipeline`, customizing the field names and adding custom behavior for images."
msgstr ""

#: ../../topics/media-pipeline.rst:557
msgid "Works the same way as :meth:`FilesPipeline.get_media_requests` method, but using a different field name for image urls."
msgstr ""

#: ../../topics/media-pipeline.rst:560
msgid "Must return a Request for each image URL."
msgstr ""

#: ../../topics/media-pipeline.rst:564
msgid "The :meth:`ImagesPipeline.item_completed` method is called when all image requests for a single item have completed (either finished downloading, or failed for some reason)."
msgstr ""

#: ../../topics/media-pipeline.rst:568
msgid "Works the same way as :meth:`FilesPipeline.item_completed` method, but using a different field names for storing image downloading results."
msgstr ""

#: ../../topics/media-pipeline.rst:577
msgid "Custom Images pipeline example"
msgstr ""

#: ../../topics/media-pipeline.rst:579
msgid "Here is a full example of the Images Pipeline whose methods are exemplified above::"
msgstr ""

#: ../../topics/media-pipeline.rst:600
msgid "To enable your custom media pipeline component you must add its class import path to the :setting:`ITEM_PIPELINES` setting, like in the following example::"
msgstr ""

#: ../../topics/practices.rst:5
msgid "Common Practices"
msgstr ""

#: ../../topics/practices.rst:7
msgid "This section documents common practices when using Scrapy. These are things that cover many topics and don't often fall into any other specific section."
msgstr ""

#: ../../topics/practices.rst:13
msgid "Run Scrapy from a script"
msgstr ""

#: ../../topics/practices.rst:15
msgid "You can use the :ref:`API <topics-api>` to run Scrapy from a script, instead of the typical way of running Scrapy via ``scrapy crawl``."
msgstr ""

#: ../../topics/practices.rst:18
msgid "Remember that Scrapy is built on top of the Twisted asynchronous networking library, so you need to run it inside the Twisted reactor."
msgstr ""

#: ../../topics/practices.rst:21
msgid "The first utility you can use to run your spiders is :class:`scrapy.crawler.CrawlerProcess`. This class will start a Twisted reactor for you, configuring the logging and setting shutdown handlers. This class is the one used by all Scrapy commands."
msgstr ""

#: ../../topics/practices.rst:26
msgid "Here's an example showing how to run a single spider with it."
msgstr ""

#: ../../topics/practices.rst:46
msgid "Define settings within dictionary in CrawlerProcess. Make sure to check :class:`~scrapy.crawler.CrawlerProcess` documentation to get acquainted with its usage details."
msgstr ""

#: ../../topics/practices.rst:49
msgid "If you are inside a Scrapy project there are some additional helpers you can use to import those components within the project. You can automatically import your spiders passing their name to :class:`~scrapy.crawler.CrawlerProcess`, and use ``get_project_settings`` to get a :class:`~scrapy.settings.Settings` instance with your project settings."
msgstr ""

#: ../../topics/practices.rst:55
msgid "What follows is a working example of how to do that, using the `testspiders`_ project as example."
msgstr ""

#: ../../topics/practices.rst:69
msgid "There's another Scrapy utility that provides more control over the crawling process: :class:`scrapy.crawler.CrawlerRunner`. This class is a thin wrapper that encapsulates some simple helpers to run multiple crawlers, but it won't start or interfere with existing reactors in any way."
msgstr ""

#: ../../topics/practices.rst:74
msgid "Using this class the reactor should be explicitly run after scheduling your spiders. It's recommended you use :class:`~scrapy.crawler.CrawlerRunner` instead of :class:`~scrapy.crawler.CrawlerProcess` if your application is already using Twisted and you want to run Scrapy in the same reactor."
msgstr ""

#: ../../topics/practices.rst:79
msgid "Note that you will also have to shutdown the Twisted reactor yourself after the spider is finished. This can be achieved by adding callbacks to the deferred returned by the :meth:`CrawlerRunner.crawl <scrapy.crawler.CrawlerRunner.crawl>` method."
msgstr ""

#: ../../topics/practices.rst:84
msgid "Here's an example of its usage, along with a callback to manually stop the reactor after ``MySpider`` has finished running."
msgstr ""

#: ../../topics/practices.rst:105
msgid ":doc:`twisted:core/howto/reactor-basics`"
msgstr ""

#: ../../topics/practices.rst:110
msgid "Running multiple spiders in the same process"
msgstr ""

#: ../../topics/practices.rst:112
msgid "By default, Scrapy runs a single spider per process when you run ``scrapy crawl``. However, Scrapy supports running multiple spiders per process using the :ref:`internal API <topics-api>`."
msgstr ""

#: ../../topics/practices.rst:116
msgid "Here is an example that runs multiple spiders simultaneously:"
msgstr ""

#: ../../topics/practices.rst:136
msgid "Same example using :class:`~scrapy.crawler.CrawlerRunner`:"
msgstr ""

#: ../../topics/practices.rst:162
msgid "Same example but running the spiders sequentially by chaining the deferreds:"
msgstr ""

#: ../../topics/practices.rst:190
msgid ":ref:`run-from-script`."
msgstr ""

#: ../../topics/practices.rst:195
msgid "Distributed crawls"
msgstr ""

#: ../../topics/practices.rst:197
msgid "Scrapy doesn't provide any built-in facility for running crawls in a distribute (multi-server) manner. However, there are some ways to distribute crawls, which vary depending on how you plan to distribute them."
msgstr ""

#: ../../topics/practices.rst:201
msgid "If you have many spiders, the obvious way to distribute the load is to setup many Scrapyd instances and distribute spider runs among those."
msgstr ""

#: ../../topics/practices.rst:204
msgid "If you instead want to run a single (big) spider through many machines, what you usually do is partition the urls to crawl and send them to each separate spider. Here is a concrete example:"
msgstr ""

#: ../../topics/practices.rst:208
msgid "First, you prepare the list of urls to crawl and put them into separate files/urls::"
msgstr ""

#: ../../topics/practices.rst:215
msgid "Then you fire a spider run on 3 different Scrapyd servers. The spider would receive a (spider) argument ``part`` with the number of the partition to crawl::"
msgstr ""

#: ../../topics/practices.rst:226
msgid "Avoiding getting banned"
msgstr ""

#: ../../topics/practices.rst:228
msgid "Some websites implement certain measures to prevent bots from crawling them, with varying degrees of sophistication. Getting around those measures can be difficult and tricky, and may sometimes require special infrastructure. Please consider contacting `commercial support`_ if in doubt."
msgstr ""

#: ../../topics/practices.rst:233
msgid "Here are some tips to keep in mind when dealing with these kinds of sites:"
msgstr ""

#: ../../topics/practices.rst:235
msgid "rotate your user agent from a pool of well-known ones from browsers (google around to get a list of them)"
msgstr ""

#: ../../topics/practices.rst:237
msgid "disable cookies (see :setting:`COOKIES_ENABLED`) as some sites may use cookies to spot bot behaviour"
msgstr ""

#: ../../topics/practices.rst:239
msgid "use download delays (2 or higher). See :setting:`DOWNLOAD_DELAY` setting."
msgstr ""

#: ../../topics/practices.rst:240
msgid "if possible, use `Google cache`_ to fetch pages, instead of hitting the sites directly"
msgstr ""

#: ../../topics/practices.rst:242
msgid "use a pool of rotating IPs. For example, the free `Tor project`_ or paid services like `ProxyMesh`_. An open source alternative is `scrapoxy`_, a super proxy that you can attach your own proxies to."
msgstr ""

#: ../../topics/practices.rst:245
msgid "use a highly distributed downloader that circumvents bans internally, so you can just focus on parsing clean pages. One example of such downloaders is `Crawlera`_"
msgstr ""

#: ../../topics/practices.rst:249
msgid "If you are still unable to prevent your bot getting banned, consider contacting `commercial support`_."
msgstr ""

#: ../../topics/request-response.rst:5
msgid "Requests and Responses"
msgstr ""

#: ../../topics/request-response.rst:10
msgid "Scrapy uses :class:`Request` and :class:`Response` objects for crawling web sites."
msgstr ""

#: ../../topics/request-response.rst:13
msgid "Typically, :class:`Request` objects are generated in the spiders and pass across the system until they reach the Downloader, which executes the request and returns a :class:`Response` object which travels back to the spider that issued the request."
msgstr ""

#: ../../topics/request-response.rst:18
msgid "Both :class:`Request` and :class:`Response` classes have subclasses which add functionality not required in the base classes. These are described below in :ref:`topics-request-response-ref-request-subclasses` and :ref:`topics-request-response-ref-response-subclasses`."
msgstr ""

#: ../../topics/request-response.rst:25
msgid "Request objects"
msgstr ""

#: ../../topics/request-response.rst:29
msgid "A :class:`Request` object represents an HTTP request, which is usually generated in the Spider and executed by the Downloader, and thus generating a :class:`Response`."
msgstr ""

#: ../../topics/request-response.rst:33
msgid "the URL of this request  If the URL is invalid, a :exc:`ValueError` exception is raised."
msgstr ""

#: ../../topics/request-response.rst:33
msgid "the URL of this request"
msgstr ""

#: ../../topics/request-response.rst:35
msgid "If the URL is invalid, a :exc:`ValueError` exception is raised."
msgstr ""

#: ../../topics/request-response.rst:38
msgid "the function that will be called with the response of this request (once it's downloaded) as its first parameter. For more information see :ref:`topics-request-response-ref-request-callback-arguments` below. If a Request doesn't specify a callback, the spider's :meth:`~scrapy.spiders.Spider.parse` method will be used. Note that if exceptions are raised during processing, errback is called instead."
msgstr ""

#: ../../topics/request-response.rst:47
msgid "the HTTP method of this request. Defaults to ``'GET'``."
msgstr ""

#: ../../topics/request-response.rst:50
msgid "the initial values for the :attr:`Request.meta` attribute. If given, the dict passed in this parameter will be shallow copied."
msgstr ""

#: ../../topics/request-response.rst:54
msgid "the request body. If a ``unicode`` is passed, then it's encoded to ``str`` using the ``encoding`` passed (which defaults to ``utf-8``). If ``body`` is not given, an empty string is stored. Regardless of the type of this argument, the final value stored will be a ``str`` (never ``unicode`` or ``None``)."
msgstr ""

#: ../../topics/request-response.rst:61
msgid "the headers of this request. The dict values can be strings (for single valued headers) or lists (for multi-valued headers). If ``None`` is passed as value, the HTTP header will not be sent at all."
msgstr ""

#: ../../topics/request-response.rst:66
msgid "the request cookies. These can be sent in two forms.  1. Using a dict::      request_with_cookies = Request(url=\"http://www.example.com\",                                    cookies={'currency': 'USD', 'country': 'UY'})  2. Using a list of dicts::      request_with_cookies = Request(url=\"http://www.example.com\",                                    cookies=[{'name': 'currency',                                             'value': 'USD',                                             'domain': 'example.com',                                             'path': '/currency'}])  The latter form allows for customizing the ``domain`` and ``path`` attributes of the cookie. This is only useful if the cookies are saved for later requests.  .. reqmeta:: dont_merge_cookies  When some site returns cookies (in a response) those are stored in the cookies for that domain and will be sent again in future requests. That's the typical behaviour of any regular web browser.  To create a request that does not send stored cookies and does not store received cookies, set the ``dont_merge_cookies`` key to ``True`` in :attr:`request.meta <scrapy.http.Request.meta>`.  Example of a request that sends manually-defined cookies and ignores cookie storage::      Request(         url=\"http://www.example.com\",         cookies={'currency': 'USD', 'country': 'UY'},         meta={'dont_merge_cookies': True},     )  For more info see :ref:`cookies-mw`."
msgstr ""

#: ../../topics/request-response.rst:66
msgid "the request cookies. These can be sent in two forms."
msgstr ""

#: ../../topics/request-response.rst:68
msgid "Using a dict::"
msgstr ""

#: ../../topics/request-response.rst:73
msgid "Using a list of dicts::"
msgstr ""

#: ../../topics/request-response.rst:81
msgid "The latter form allows for customizing the ``domain`` and ``path`` attributes of the cookie. This is only useful if the cookies are saved for later requests."
msgstr ""

#: ../../topics/request-response.rst:87
msgid "When some site returns cookies (in a response) those are stored in the cookies for that domain and will be sent again in future requests. That's the typical behaviour of any regular web browser."
msgstr ""

#: ../../topics/request-response.rst:91
msgid "To create a request that does not send stored cookies and does not store received cookies, set the ``dont_merge_cookies`` key to ``True`` in :attr:`request.meta <scrapy.http.Request.meta>`."
msgstr ""

#: ../../topics/request-response.rst:95
msgid "Example of a request that sends manually-defined cookies and ignores cookie storage::"
msgstr ""

#: ../../topics/request-response.rst:104
msgid "For more info see :ref:`cookies-mw`."
msgstr ""

#: ../../topics/request-response.rst:107
msgid "the encoding of this request (defaults to ``'utf-8'``). This encoding will be used to percent-encode the URL and to convert the body to ``str`` (if given as ``unicode``)."
msgstr ""

#: ../../topics/request-response.rst:112
msgid "the priority of this request (defaults to ``0``). The priority is used by the scheduler to define the order used to process requests.  Requests with a higher priority value will execute earlier. Negative values are allowed in order to indicate relatively low-priority."
msgstr ""

#: ../../topics/request-response.rst:118
msgid "indicates that this request should not be filtered by the scheduler. This is used when you want to perform an identical request multiple times, to ignore the duplicates filter. Use it with care, or you will get into crawling loops. Default to ``False``."
msgstr ""

#: ../../topics/request-response.rst:124
msgid "a function that will be called if any exception was raised while processing the request. This includes pages that failed with 404 HTTP errors and such. It receives a :exc:`~twisted.python.failure.Failure` as first parameter. For more information, see :ref:`topics-request-response-ref-errbacks` below.  .. versionchanged:: 2.0    The *callback* parameter is no longer required when the *errback*    parameter is specified."
msgstr ""

#: ../../topics/request-response.rst:124
msgid "a function that will be called if any exception was raised while processing the request. This includes pages that failed with 404 HTTP errors and such. It receives a :exc:`~twisted.python.failure.Failure` as first parameter. For more information, see :ref:`topics-request-response-ref-errbacks` below."
msgstr ""

#: ../../topics/request-response.rst:131
msgid "The *callback* parameter is no longer required when the *errback* parameter is specified."
msgstr ""

#: ../../topics/request-response.rst:136
msgid "Flags sent to the request, can be used for logging or similar purposes."
msgstr ""

#: ../../topics/request-response.rst:139
msgid "A dict with arbitrary data that will be passed as keyword arguments to the Request's callback."
msgstr ""

#: ../../topics/request-response.rst:144
msgid "A string containing the URL of this request. Keep in mind that this attribute contains the escaped URL, so it can differ from the URL passed in the ``__init__`` method."
msgstr ""

#: ../../topics/request-response.rst:148
msgid "This attribute is read-only. To change the URL of a Request use :meth:`replace`."
msgstr ""

#: ../../topics/request-response.rst:153
msgid "A string representing the HTTP method in the request. This is guaranteed to be uppercase. Example: ``\"GET\"``, ``\"POST\"``, ``\"PUT\"``, etc"
msgstr ""

#: ../../topics/request-response.rst:158
msgid "A dictionary-like object which contains the request headers."
msgstr ""

#: ../../topics/request-response.rst:162
msgid "A str that contains the request body."
msgstr ""

#: ../../topics/request-response.rst:164
msgid "This attribute is read-only. To change the body of a Request use :meth:`replace`."
msgstr ""

#: ../../topics/request-response.rst:169
msgid "A dict that contains arbitrary metadata for this request. This dict is empty for new Requests, and is usually  populated by different Scrapy components (extensions, middlewares, etc). So the data contained in this dict depends on the extensions you have enabled."
msgstr ""

#: ../../topics/request-response.rst:174
msgid "See :ref:`topics-request-meta` for a list of special meta keys recognized by Scrapy."
msgstr ""

#: ../../topics/request-response.rst:177
msgid "This dict is :doc:`shallow copied <library/copy>` when the request is cloned using the ``copy()`` or ``replace()`` methods, and can also be accessed, in your spider, from the ``response.meta`` attribute."
msgstr ""

#: ../../topics/request-response.rst:183
msgid "A dictionary that contains arbitrary metadata for this request. Its contents will be passed to the Request's callback as keyword arguments. It is empty for new Requests, which means by default callbacks only get a :class:`Response` object as argument."
msgstr ""

#: ../../topics/request-response.rst:188
msgid "This dict is :doc:`shallow copied <library/copy>` when the request is cloned using the ``copy()`` or ``replace()`` methods, and can also be accessed, in your spider, from the ``response.cb_kwargs`` attribute."
msgstr ""

#: ../../topics/request-response.rst:194
msgid "Return a new Request which is a copy of this Request. See also: :ref:`topics-request-response-ref-request-callback-arguments`."
msgstr ""

#: ../../topics/request-response.rst:199
msgid "Return a Request object with the same members, except for those members given new values by whichever keyword arguments are specified. The :attr:`Request.cb_kwargs` and :attr:`Request.meta` attributes are shallow copied by default (unless new values are given as arguments). See also :ref:`topics-request-response-ref-request-callback-arguments`."
msgstr ""

#: ../../../scrapy/http/request/__init__.py:docstring of scrapy.http.Request.from_curl:1
msgid "Create a Request object from a string containing a `cURL <https://curl.haxx.se/>`_ command. It populates the HTTP method, the URL, the headers, the cookies and the body. It accepts the same arguments as the :class:`Request` class, taking preference and overriding the values of the same arguments contained in the cURL command."
msgstr ""

#: ../../../scrapy/http/request/__init__.py:docstring of scrapy.http.Request.from_curl:8
msgid "Unrecognized options are ignored by default. To raise an error when finding unknown options call this method by passing ``ignore_unknown_options=False``."
msgstr ""

#: ../../../scrapy/http/request/__init__.py:docstring of scrapy.http.Request.from_curl:12
msgid "Using :meth:`from_curl` from :class:`~scrapy.http.Request` subclasses, such as :class:`~scrapy.http.JSONRequest`, or :class:`~scrapy.http.XmlRpcRequest`, as well as having :ref:`downloader middlewares <topics-downloader-middleware>` and :ref:`spider middlewares <topics-spider-middleware>` enabled, such as :class:`~scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware`, :class:`~scrapy.downloadermiddlewares.useragent.UserAgentMiddleware`, or :class:`~scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware`, may modify the :class:`~scrapy.http.Request` object."
msgstr ""

#: ../../../scrapy/http/request/__init__.py:docstring of scrapy.http.Request.from_curl:25
msgid "To translate a cURL command into a Scrapy request, you may use `curl2scrapy <https://michael-shub.github.io/curl2scrapy/>`_."
msgstr ""

#: ../../topics/request-response.rst:210
msgid "Passing additional data to callback functions"
msgstr ""

#: ../../topics/request-response.rst:212
msgid "The callback of a request is a function that will be called when the response of that request is downloaded. The callback function will be called with the downloaded :class:`Response` object as its first argument."
msgstr ""

#: ../../topics/request-response.rst:226
msgid "In some cases you may be interested in passing arguments to those callback functions so you can receive the arguments later, in the second callback. The following example shows how to achieve this by using the :attr:`Request.cb_kwargs` attribute:"
msgstr ""

#: ../../topics/request-response.rst:247
msgid ":attr:`Request.cb_kwargs` was introduced in version ``1.7``. Prior to that, using :attr:`Request.meta` was recommended for passing information around callbacks. After ``1.7``, :attr:`Request.cb_kwargs` became the preferred way for handling user information, leaving :attr:`Request.meta` for communication with components like middlewares and extensions."
msgstr ""

#: ../../topics/request-response.rst:256
msgid "Using errbacks to catch exceptions in request processing"
msgstr ""

#: ../../topics/request-response.rst:258
msgid "The errback of a request is a function that will be called when an exception is raise while processing it."
msgstr ""

#: ../../topics/request-response.rst:261
msgid "It receives a :exc:`~twisted.python.failure.Failure` as first parameter and can be used to track connection establishment timeouts, DNS errors etc."
msgstr ""

#: ../../topics/request-response.rst:264
msgid "Here's an example spider logging all errors and catching some specific errors if needed::"
msgstr ""

#: ../../topics/request-response.rst:318
msgid "Request.meta special keys"
msgstr ""

#: ../../topics/request-response.rst:320
msgid "The :attr:`Request.meta` attribute can contain any arbitrary data, but there are some special keys recognized by Scrapy and its built-in extensions."
msgstr ""

#: ../../topics/request-response.rst:323
msgid "Those are:"
msgstr ""

#: ../../topics/request-response.rst:325
msgid ":reqmeta:`dont_redirect`"
msgstr ""

#: ../../topics/request-response.rst:326
msgid ":reqmeta:`dont_retry`"
msgstr ""

#: ../../topics/request-response.rst:327
msgid ":reqmeta:`handle_httpstatus_list`"
msgstr ""

#: ../../topics/request-response.rst:328
msgid ":reqmeta:`handle_httpstatus_all`"
msgstr ""

#: ../../topics/request-response.rst:329
msgid ":reqmeta:`dont_merge_cookies`"
msgstr ""

#: ../../topics/request-response.rst:330
msgid ":reqmeta:`cookiejar`"
msgstr ""

#: ../../topics/request-response.rst:331
msgid ":reqmeta:`dont_cache`"
msgstr ""

#: ../../topics/request-response.rst:332
msgid ":reqmeta:`redirect_reasons`"
msgstr ""

#: ../../topics/request-response.rst:333
msgid ":reqmeta:`redirect_urls`"
msgstr ""

#: ../../topics/request-response.rst:334
msgid ":reqmeta:`bindaddress`"
msgstr ""

#: ../../topics/request-response.rst:335
msgid ":reqmeta:`dont_obey_robotstxt`"
msgstr ""

#: ../../topics/request-response.rst:336
msgid ":reqmeta:`download_timeout`"
msgstr ""

#: ../../topics/request-response.rst:337
msgid ":reqmeta:`download_maxsize`"
msgstr ""

#: ../../topics/request-response.rst:338
msgid ":reqmeta:`download_latency`"
msgstr ""

#: ../../topics/request-response.rst:339
msgid ":reqmeta:`download_fail_on_dataloss`"
msgstr ""

#: ../../topics/request-response.rst:340
msgid ":reqmeta:`proxy`"
msgstr ""

#: ../../topics/request-response.rst:341
msgid "``ftp_user`` (See :setting:`FTP_USER` for more info)"
msgstr ""

#: ../../topics/request-response.rst:342
msgid "``ftp_password`` (See :setting:`FTP_PASSWORD` for more info)"
msgstr ""

#: ../../topics/request-response.rst:343
msgid ":reqmeta:`referrer_policy`"
msgstr ""

#: ../../topics/request-response.rst:344
msgid ":reqmeta:`max_retry_times`"
msgstr ""

#: ../../topics/request-response.rst:349
msgid "bindaddress"
msgstr ""

#: ../../topics/request-response.rst:351
msgid "The IP of the outgoing IP address to use for the performing the request."
msgstr ""

#: ../../topics/request-response.rst:356
msgid "download_timeout"
msgstr ""

#: ../../topics/request-response.rst:358
msgid "The amount of time (in secs) that the downloader will wait before timing out. See also: :setting:`DOWNLOAD_TIMEOUT`."
msgstr ""

#: ../../topics/request-response.rst:364
msgid "download_latency"
msgstr ""

#: ../../topics/request-response.rst:366
msgid "The amount of time spent to fetch the response, since the request has been started, i.e. HTTP message sent over the network. This meta key only becomes available when the response has been downloaded. While most other meta keys are used to control Scrapy behavior, this one is supposed to be read-only."
msgstr ""

#: ../../topics/request-response.rst:374
msgid "download_fail_on_dataloss"
msgstr ""

#: ../../topics/request-response.rst:376
msgid "Whether or not to fail on broken responses. See: :setting:`DOWNLOAD_FAIL_ON_DATALOSS`."
msgstr ""

#: ../../topics/request-response.rst:382
msgid "max_retry_times"
msgstr ""

#: ../../topics/request-response.rst:384
msgid "The meta key is used set retry times per request. When initialized, the :reqmeta:`max_retry_times` meta key takes higher precedence over the :setting:`RETRY_TIMES` setting."
msgstr ""

#: ../../topics/request-response.rst:391
msgid "Request subclasses"
msgstr ""

#: ../../topics/request-response.rst:393
msgid "Here is the list of built-in :class:`Request` subclasses. You can also subclass it to implement your own custom functionality."
msgstr ""

#: ../../topics/request-response.rst:397
msgid "FormRequest objects"
msgstr ""

#: ../../topics/request-response.rst:399
msgid "The FormRequest class extends the base :class:`Request` with functionality for dealing with HTML forms. It uses `lxml.html forms`_  to pre-populate form fields with form data from :class:`Response` objects."
msgstr ""

#: ../../topics/request-response.rst:407
msgid "The :class:`FormRequest` class adds a new keyword parameter to the ``__init__`` method. The remaining arguments are the same as for the :class:`Request` class and are not documented here."
msgstr ""

#: ../../topics/request-response.rst:411
msgid "is a dictionary (or iterable of (key, value) tuples) containing HTML Form data which will be url-encoded and assigned to the body of the request."
msgstr ""

#: ../../topics/request-response.rst:416
msgid "The :class:`FormRequest` objects support the following class method in addition to the standard :class:`Request` methods:"
msgstr ""

#: ../../topics/request-response.rst:421
msgid "Returns a new :class:`FormRequest` object with its form field values pre-populated with those found in the HTML ``<form>`` element contained in the given response. For an example see :ref:`topics-request-response-ref-request-userlogin`."
msgstr ""

#: ../../topics/request-response.rst:426
msgid "The policy is to automatically simulate a click, by default, on any form control that looks clickable, like a ``<input type=\"submit\">``.  Even though this is quite convenient, and often the desired behaviour, sometimes it can cause problems which could be hard to debug. For example, when working with forms that are filled and/or submitted using javascript, the default :meth:`from_response` behaviour may not be the most appropriate. To disable this behaviour you can set the ``dont_click`` argument to ``True``. Also, if you want to change the control clicked (instead of disabling it) you can also use the ``clickdata`` argument."
msgstr ""

#: ../../topics/request-response.rst:437
msgid "Using this method with select elements which have leading or trailing whitespace in the option values will not work due to a `bug in lxml`_, which should be fixed in lxml 3.8 and above."
msgstr ""

#: ../../topics/request-response.rst:441
msgid "the response containing a HTML form which will be used to pre-populate the form fields"
msgstr ""

#: ../../topics/request-response.rst:445
msgid "if given, the form with name attribute set to this value will be used."
msgstr ""

#: ../../topics/request-response.rst:448
msgid "if given, the form with id attribute set to this value will be used."
msgstr ""

#: ../../topics/request-response.rst:451
msgid "if given, the first form that matches the xpath will be used."
msgstr ""

#: ../../topics/request-response.rst:454
msgid "if given, the first form that matches the css selector will be used."
msgstr ""

#: ../../topics/request-response.rst:457
msgid "the number of form to use, when the response contains multiple forms. The first one (and also the default) is ``0``."
msgstr ""

#: ../../topics/request-response.rst:461
msgid "fields to override in the form data. If a field was already present in the response ``<form>`` element, its value is overridden by the one passed in this parameter. If a value passed in this parameter is ``None``, the field will not be included in the request, even if it was present in the response ``<form>`` element."
msgstr ""

#: ../../topics/request-response.rst:468
msgid "attributes to lookup the control clicked. If it's not given, the form data will be submitted simulating a click on the first clickable element. In addition to html attributes, the control can be identified by its zero-based index relative to other submittable inputs inside the form, via the ``nr`` attribute."
msgstr ""

#: ../../topics/request-response.rst:475
msgid "If True, the form data will be submitted without clicking in any element."
msgstr ""

#: ../../topics/request-response.rst:479
msgid "The other parameters of this class method are passed directly to the :class:`FormRequest` ``__init__`` method."
msgstr ""

#: ../../topics/request-response.rst:482
msgid "The ``formname`` parameter."
msgstr ""

#: ../../topics/request-response.rst:485
msgid "The ``formxpath`` parameter."
msgstr ""

#: ../../topics/request-response.rst:488
msgid "The ``formcss`` parameter."
msgstr ""

#: ../../topics/request-response.rst:491
msgid "The ``formid`` parameter."
msgstr ""

#: ../../topics/request-response.rst:495
msgid "Request usage examples"
msgstr ""

#: ../../topics/request-response.rst:498
msgid "Using FormRequest to send data via HTTP POST"
msgstr ""

#: ../../topics/request-response.rst:500
msgid "If you want to simulate a HTML Form POST in your spider and send a couple of key-value fields, you can return a :class:`FormRequest` object (from your spider) like this::"
msgstr ""

#: ../../topics/request-response.rst:511
msgid "Using FormRequest.from_response() to simulate a user login"
msgstr ""

#: ../../topics/request-response.rst:513
msgid "It is usual for web sites to provide pre-populated form fields through ``<input type=\"hidden\">`` elements, such as session related data or authentication tokens (for login pages). When scraping, you'll want these fields to be automatically pre-populated and only override a couple of them, such as the user name and password. You can use the :meth:`FormRequest.from_response` method for this job. Here's an example spider which uses it::"
msgstr ""

#: ../../topics/request-response.rst:547
msgid "JsonRequest"
msgstr ""

#: ../../topics/request-response.rst:549
msgid "The JsonRequest class extends the base :class:`Request` class with functionality for dealing with JSON requests."
msgstr ""

#: ../../topics/request-response.rst:554
msgid "The :class:`JsonRequest` class adds two new keyword parameters to the ``__init__`` method. The remaining arguments are the same as for the :class:`Request` class and are not documented here."
msgstr ""

#: ../../topics/request-response.rst:558
msgid "Using the :class:`JsonRequest` will set the ``Content-Type`` header to ``application/json`` and ``Accept`` header to ``application/json, text/javascript, */*; q=0.01``"
msgstr ""

#: ../../topics/request-response.rst:561
msgid "is any JSON serializable object that needs to be JSON encoded and assigned to body. if :attr:`Request.body` argument is provided this parameter will be ignored. if :attr:`Request.body` argument is not provided and data argument is provided :attr:`Request.method` will be set to ``'POST'`` automatically."
msgstr ""

#: ../../topics/request-response.rst:567
msgid "Parameters that will be passed to underlying :func:`json.dumps` method which is used to serialize data into JSON format."
msgstr ""

#: ../../topics/request-response.rst:572
msgid "JsonRequest usage example"
msgstr ""

#: ../../topics/request-response.rst:574
msgid "Sending a JSON POST request with a JSON payload::"
msgstr ""

#: ../../topics/request-response.rst:584
msgid "Response objects"
msgstr ""

#: ../../topics/request-response.rst:588
msgid "A :class:`Response` object represents an HTTP response, which is usually downloaded (by the Downloader) and fed to the Spiders for processing."
msgstr ""

#: ../../topics/request-response.rst:591
msgid "the URL of this response"
msgstr ""

#: ../../topics/request-response.rst:594
msgid "the HTTP status of the response. Defaults to ``200``."
msgstr ""

#: ../../topics/request-response.rst:597
msgid "the headers of this response. The dict values can be strings (for single valued headers) or lists (for multi-valued headers)."
msgstr ""

#: ../../topics/request-response.rst:601
msgid "the response body. To access the decoded text as str you can use ``response.text`` from an encoding-aware :ref:`Response subclass <topics-request-response-ref-response-subclasses>`, such as :class:`TextResponse`."
msgstr ""

#: ../../topics/request-response.rst:607
msgid "is a list containing the initial values for the :attr:`Response.flags` attribute. If given, the list will be shallow copied."
msgstr ""

#: ../../topics/request-response.rst:612
msgid "the initial value of the :attr:`Response.request` attribute. This represents the :class:`Request` that generated this response."
msgstr ""

#: ../../topics/request-response.rst:616
msgid "an object representing the server's SSL certificate."
msgstr ""

#: ../../topics/request-response.rst:619
#: ../../topics/request-response.rst:718
msgid "The IP address of the server from which the Response originated."
msgstr ""

#: ../../topics/request-response.rst:622
msgid "The ``ip_address`` parameter."
msgstr ""

#: ../../topics/request-response.rst:627
msgid "A string containing the URL of the response."
msgstr ""

#: ../../topics/request-response.rst:629
msgid "This attribute is read-only. To change the URL of a Response use :meth:`replace`."
msgstr ""

#: ../../topics/request-response.rst:634
msgid "An integer representing the HTTP status of the response. Example: ``200``, ``404``."
msgstr ""

#: ../../topics/request-response.rst:639
msgid "A dictionary-like object which contains the response headers. Values can be accessed using :meth:`get` to return the first header value with the specified name or :meth:`getlist` to return all header values with the specified name. For example, this call will give you all cookies in the headers::"
msgstr ""

#: ../../topics/request-response.rst:649
msgid "The body of this Response. Keep in mind that Response.body is always a bytes object. If you want the unicode version use :attr:`TextResponse.text` (only available in :class:`TextResponse` and subclasses)."
msgstr ""

#: ../../topics/request-response.rst:654
msgid "This attribute is read-only. To change the body of a Response use :meth:`replace`."
msgstr ""

#: ../../topics/request-response.rst:659
msgid "The :class:`Request` object that generated this response. This attribute is assigned in the Scrapy engine, after the response and the request have passed through all :ref:`Downloader Middlewares <topics-downloader-middleware>`. In particular, this means that:"
msgstr ""

#: ../../topics/request-response.rst:664
msgid "HTTP redirections will cause the original request (to the URL before redirection) to be assigned to the redirected response (with the final URL after redirection)."
msgstr ""

#: ../../topics/request-response.rst:668
msgid "Response.request.url doesn't always equal Response.url"
msgstr ""

#: ../../topics/request-response.rst:670
msgid "This attribute is only available in the spider code, and in the :ref:`Spider Middlewares <topics-spider-middleware>`, but not in Downloader Middlewares (although you have the Request available there by other means) and handlers of the :signal:`response_downloaded` signal."
msgstr ""

#: ../../topics/request-response.rst:677
msgid "A shortcut to the :attr:`Request.meta` attribute of the :attr:`Response.request` object (i.e. ``self.request.meta``)."
msgstr ""

#: ../../topics/request-response.rst:680
msgid "Unlike the :attr:`Response.request` attribute, the :attr:`Response.meta` attribute is propagated along redirects and retries, so you will get the original :attr:`Request.meta` sent from your spider."
msgstr ""

#: ../../topics/request-response.rst:684
msgid ":attr:`Request.meta` attribute"
msgstr ""

#: ../../topics/request-response.rst:690
msgid "A shortcut to the :attr:`Request.cb_kwargs` attribute of the :attr:`Response.request` object (i.e. ``self.request.cb_kwargs``)."
msgstr ""

#: ../../topics/request-response.rst:693
msgid "Unlike the :attr:`Response.request` attribute, the :attr:`Response.cb_kwargs` attribute is propagated along redirects and retries, so you will get the original :attr:`Request.cb_kwargs` sent from your spider."
msgstr ""

#: ../../topics/request-response.rst:698
msgid ":attr:`Request.cb_kwargs` attribute"
msgstr ""

#: ../../topics/request-response.rst:702
msgid "A list that contains flags for this response. Flags are labels used for tagging Responses. For example: ``'cached'``, ``'redirected``', etc. And they're shown on the string representation of the Response (`__str__` method) which is used by the engine for logging."
msgstr ""

#: ../../topics/request-response.rst:709
msgid "A :class:`twisted.internet.ssl.Certificate` object representing the server's SSL certificate."
msgstr ""

#: ../../topics/request-response.rst:712
msgid "Only populated for ``https`` responses, ``None`` otherwise."
msgstr ""

#: ../../topics/request-response.rst:720
msgid "This attribute is currently only populated by the HTTP 1.1 download handler, i.e. for ``http(s)`` responses. For other handlers, :attr:`ip_address` is always ``None``."
msgstr ""

#: ../../topics/request-response.rst:726
msgid "Returns a new Response which is a copy of this Response."
msgstr ""

#: ../../topics/request-response.rst:730
msgid "Returns a Response object with the same members, except for those members given new values by whichever keyword arguments are specified. The attribute :attr:`Response.meta` is copied by default."
msgstr ""

#: ../../topics/request-response.rst:736
msgid "Constructs an absolute url by combining the Response's :attr:`url` with a possible relative url."
msgstr ""

#: ../../topics/request-response.rst:739
msgid "This is a wrapper over :func:`~urllib.parse.urljoin`, it's merely an alias for making this call::"
msgstr ""

#: ../../../scrapy/http/response/__init__.py:docstring of scrapy.http.Response.follow:1
msgid "Return a :class:`~.Request` instance to follow a link ``url``. It accepts the same arguments as ``Request.__init__`` method, but ``url`` can be a relative URL or a ``scrapy.link.Link`` object, not only an absolute URL."
msgstr ""

#: ../../../scrapy/http/response/__init__.py:docstring of scrapy.http.Response.follow:6
msgid ":class:`~.TextResponse` provides a :meth:`~.TextResponse.follow` method which supports selectors in addition to absolute/relative URLs and Link objects."
msgstr ""

#: ../../../scrapy/http/response/__init__.py:docstring of scrapy.http.Response.follow:10
msgid "The *flags* parameter."
msgstr ""

#: ../../../scrapy/http/response/__init__.py:docstring of scrapy.http.Response.follow_all:3
msgid "Return an iterable of :class:`~.Request` instances to follow all links in ``urls``. It accepts the same arguments as ``Request.__init__`` method, but elements of ``urls`` can be relative URLs or :class:`~scrapy.link.Link` objects, not only absolute URLs."
msgstr ""

#: ../../../scrapy/http/response/__init__.py:docstring of scrapy.http.Response.follow_all:8
msgid ":class:`~.TextResponse` provides a :meth:`~.TextResponse.follow_all` method which supports selectors in addition to absolute/relative URLs and Link objects."
msgstr ""

#: ../../topics/request-response.rst:752
msgid "Response subclasses"
msgstr ""

#: ../../topics/request-response.rst:754
msgid "Here is the list of available built-in Response subclasses. You can also subclass the Response class to implement your own functionality."
msgstr ""

#: ../../topics/request-response.rst:758
msgid "TextResponse objects"
msgstr ""

#: ../../topics/request-response.rst:762
msgid ":class:`TextResponse` objects adds encoding capabilities to the base :class:`Response` class, which is meant to be used only for binary data, such as images, sounds or any media file."
msgstr ""

#: ../../topics/request-response.rst:766
msgid ":class:`TextResponse` objects support a new ``__init__`` method argument, in addition to the base :class:`Response` objects. The remaining functionality is the same as for the :class:`Response` class and is not documented here."
msgstr ""

#: ../../topics/request-response.rst:770
msgid "is a string which contains the encoding to use for this response. If you create a :class:`TextResponse` object with a unicode body, it will be encoded using this encoding (remember the body attribute is always a string). If ``encoding`` is ``None`` (default value), the encoding will be looked up in the response headers and body instead."
msgstr ""

#: ../../topics/request-response.rst:777
msgid ":class:`TextResponse` objects support the following attributes in addition to the standard :class:`Response` ones:"
msgstr ""

#: ../../topics/request-response.rst:782
msgid "Response body, as unicode."
msgstr ""

#: ../../topics/request-response.rst:784
msgid "The same as ``response.body.decode(response.encoding)``, but the result is cached after the first call, so you can access ``response.text`` multiple times without extra overhead."
msgstr ""

#: ../../topics/request-response.rst:790
msgid "``unicode(response.body)`` is not a correct way to convert response body to unicode: you would be using the system default encoding (typically ``ascii``) instead of the response encoding."
msgstr ""

#: ../../topics/request-response.rst:797
msgid "A string with the encoding of this response. The encoding is resolved by trying the following mechanisms, in order:"
msgstr ""

#: ../../topics/request-response.rst:800
msgid "the encoding passed in the ``__init__`` method ``encoding`` argument"
msgstr ""

#: ../../topics/request-response.rst:802
msgid "the encoding declared in the Content-Type HTTP header. If this encoding is not valid (i.e. unknown), it is ignored and the next resolution mechanism is tried."
msgstr ""

#: ../../topics/request-response.rst:806
msgid "the encoding declared in the response body. The TextResponse class doesn't provide any special functionality for this. However, the :class:`HtmlResponse` and :class:`XmlResponse` classes do."
msgstr ""

#: ../../topics/request-response.rst:810
msgid "the encoding inferred by looking at the response body. This is the more fragile method but also the last one tried."
msgstr ""

#: ../../topics/request-response.rst:815
msgid "A :class:`~scrapy.selector.Selector` instance using the response as target. The selector is lazily instantiated on first access."
msgstr ""

#: ../../topics/request-response.rst:818
msgid ":class:`TextResponse` objects support the following methods in addition to the standard :class:`Response` ones:"
msgstr ""

#: ../../topics/request-response.rst:823
msgid "A shortcut to ``TextResponse.selector.xpath(query)``::"
msgstr ""

#: ../../topics/request-response.rst:829
msgid "A shortcut to ``TextResponse.selector.css(query)``::"
msgstr ""

#: ../../../scrapy/http/response/text.py:docstring of scrapy.http.TextResponse.follow:1
msgid "Return a :class:`~.Request` instance to follow a link ``url``. It accepts the same arguments as ``Request.__init__`` method, but ``url`` can be not only an absolute URL, but also"
msgstr ""

#: ../../../scrapy/http/response/text.py:docstring of scrapy.http.TextResponse.follow:5
#: ../../../scrapy/http/response/text.py:docstring of scrapy.http.TextResponse.follow_all:6
msgid "a relative URL"
msgstr ""

#: ../../../scrapy/http/response/text.py:docstring of scrapy.http.TextResponse.follow:6
#: ../../../scrapy/http/response/text.py:docstring of scrapy.http.TextResponse.follow_all:7
msgid "a :class:`~scrapy.link.Link` object, e.g. the result of :ref:`topics-link-extractors`"
msgstr ""

#: ../../../scrapy/http/response/text.py:docstring of scrapy.http.TextResponse.follow:8
#: ../../../scrapy/http/response/text.py:docstring of scrapy.http.TextResponse.follow_all:9
msgid "a :class:`~scrapy.selector.Selector` object for a ``<link>`` or ``<a>`` element, e.g. ``response.css('a.my_link')[0]``"
msgstr ""

#: ../../../scrapy/http/response/text.py:docstring of scrapy.http.TextResponse.follow:10
#: ../../../scrapy/http/response/text.py:docstring of scrapy.http.TextResponse.follow_all:11
msgid "an attribute :class:`~scrapy.selector.Selector` (not SelectorList), e.g. ``response.css('a::attr(href)')[0]`` or ``response.xpath('//img/@src')[0]``"
msgstr ""

#: ../../../scrapy/http/response/text.py:docstring of scrapy.http.TextResponse.follow:14
msgid "See :ref:`response-follow-example` for usage examples."
msgstr ""

#: ../../../scrapy/http/response/text.py:docstring of scrapy.http.TextResponse.follow_all:1
msgid "A generator that produces :class:`~.Request` instances to follow all links in ``urls``. It accepts the same arguments as the :class:`~.Request`'s ``__init__`` method, except that each ``urls`` element does not need to be an absolute URL, it can be any of the following:"
msgstr ""

#: ../../../scrapy/http/response/text.py:docstring of scrapy.http.TextResponse.follow_all:15
msgid "In addition, ``css`` and ``xpath`` arguments are accepted to perform the link extraction within the ``follow_all`` method (only one of ``urls``, ``css`` and ``xpath`` is accepted)."
msgstr ""

#: ../../../scrapy/http/response/text.py:docstring of scrapy.http.TextResponse.follow_all:18
msgid "Note that when passing a ``SelectorList`` as argument for the ``urls`` parameter or using the ``css`` or ``xpath`` parameters, this method will not produce requests for selectors from which links cannot be obtained (for instance, anchor tags without an ``href`` attribute)"
msgstr ""

#: ../../topics/request-response.rst:839
msgid "The same as :attr:`text`, but available as a method. This method is kept for backward compatibility; please prefer ``response.text``."
msgstr ""

#: ../../topics/request-response.rst:844
msgid "HtmlResponse objects"
msgstr ""

#: ../../topics/request-response.rst:848
msgid "The :class:`HtmlResponse` class is a subclass of :class:`TextResponse` which adds encoding auto-discovering support by looking into the HTML `meta http-equiv`_ attribute.  See :attr:`TextResponse.encoding`."
msgstr ""

#: ../../topics/request-response.rst:855
msgid "XmlResponse objects"
msgstr ""

#: ../../topics/request-response.rst:859
msgid "The :class:`XmlResponse` class is a subclass of :class:`TextResponse` which adds encoding auto-discovering support by looking into the XML declaration line.  See :attr:`TextResponse.encoding`."
msgstr ""

#: ../../topics/scrapyd.rst:7
msgid "Scrapyd"
msgstr ""

#: ../../topics/scrapyd.rst:9
msgid "Scrapyd has been moved into a separate project."
msgstr ""

#: ../../topics/scrapyd.rst:11
msgid "Its documentation is now hosted at:"
msgstr ""

#: ../../topics/scrapyd.rst:13
msgid "https://scrapyd.readthedocs.io/en/latest/"
msgstr ""

#: ../../topics/selectors.rst:5
msgid "Selectors"
msgstr ""

#: ../../topics/selectors.rst:7
msgid "When you're scraping web pages, the most common task you need to perform is to extract data from the HTML source. There are several libraries available to achieve this, such as:"
msgstr ""

#: ../../topics/selectors.rst:11
msgid "`BeautifulSoup`_ is a very popular web scraping library among Python programmers which constructs a Python object based on the structure of the HTML code and also deals with bad markup reasonably well, but it has one drawback: it's slow."
msgstr ""

#: ../../topics/selectors.rst:16
msgid "`lxml`_ is an XML parsing library (which also parses HTML) with a pythonic API based on :mod:`~xml.etree.ElementTree`. (lxml is not part of the Python standard library.)"
msgstr ""

#: ../../topics/selectors.rst:20
msgid "Scrapy comes with its own mechanism for extracting data. They're called selectors because they \"select\" certain parts of the HTML document specified either by `XPath`_ or `CSS`_ expressions."
msgstr ""

#: ../../topics/selectors.rst:24
msgid "`XPath`_ is a language for selecting nodes in XML documents, which can also be used with HTML. `CSS`_ is a language for applying styles to HTML documents. It defines selectors to associate those styles with specific HTML elements."
msgstr ""

#: ../../topics/selectors.rst:29
msgid "Scrapy Selectors is a thin wrapper around `parsel`_ library; the purpose of this wrapper is to provide better integration with Scrapy Response objects."
msgstr ""

#: ../../topics/selectors.rst:32
msgid "`parsel`_ is a stand-alone web scraping library which can be used without Scrapy. It uses `lxml`_ library under the hood, and implements an easy API on top of lxml API. It means Scrapy selectors are very similar in speed and parsing accuracy to lxml."
msgstr ""

#: ../../topics/selectors.rst:44
#: ../../topics/selectors.rst:95
msgid "Using selectors"
msgstr ""

#: ../../topics/selectors.rst:47
msgid "Constructing selectors"
msgstr ""

#: ../../topics/selectors.rst:51
msgid "Response objects expose a :class:`~scrapy.selector.Selector` instance on ``.selector`` attribute:"
msgstr ""

#: ../../topics/selectors.rst:57
msgid "Querying responses using XPath and CSS is so common that responses include two more shortcuts: ``response.xpath()`` and ``response.css()``:"
msgstr ""

#: ../../topics/selectors.rst:65
msgid "Scrapy selectors are instances of :class:`~scrapy.selector.Selector` class constructed by passing either :class:`~scrapy.http.TextResponse` object or markup as an unicode string (in ``text`` argument). Usually there is no need to construct Scrapy selectors manually: ``response`` object is available in Spider callbacks, so in most cases it is more convenient to use ``response.css()`` and ``response.xpath()`` shortcuts. By using ``response.selector`` or one of these shortcuts you can also ensure the response body is parsed only once."
msgstr ""

#: ../../topics/selectors.rst:74
msgid "But if required, it is possible to use ``Selector`` directly. Constructing from text:"
msgstr ""

#: ../../topics/selectors.rst:82
msgid "Constructing from response - :class:`~scrapy.http.HtmlResponse` is one of :class:`~scrapy.http.TextResponse` subclasses:"
msgstr ""

#: ../../topics/selectors.rst:91
msgid "``Selector`` automatically chooses the best parsing rules (XML vs HTML) based on input type."
msgstr ""

#: ../../topics/selectors.rst:97
msgid "To explain how to use the selectors we'll use the ``Scrapy shell`` (which provides interactive testing) and an example page located in the Scrapy documentation server:"
msgstr ""

#: ../../topics/selectors.rst:101
msgid "https://docs.scrapy.org/en/latest/_static/selectors-sample1.html"
msgstr ""

#: ../../topics/selectors.rst:105
msgid "For the sake of completeness, here's its full HTML code:"
msgstr ""

#: ../../topics/selectors.rst:112
msgid "First, let's open the shell::"
msgstr ""

#: ../../topics/selectors.rst:116
msgid "Then, after the shell loads, you'll have the response available as ``response`` shell variable, and its attached selector in ``response.selector`` attribute."
msgstr ""

#: ../../topics/selectors.rst:119
msgid "Since we're dealing with HTML, the selector will automatically use an HTML parser."
msgstr ""

#: ../../topics/selectors.rst:123
msgid "So, by looking at the :ref:`HTML code <topics-selectors-htmlcode>` of that page, let's construct an XPath for selecting the text inside the title tag:"
msgstr ""

#: ../../topics/selectors.rst:129
msgid "To actually extract the textual data, you must call the selector ``.get()`` or ``.getall()`` methods, as follows:"
msgstr ""

#: ../../topics/selectors.rst:137
msgid "``.get()`` always returns a single result; if there are several matches, content of a first match is returned; if there are no matches, None is returned. ``.getall()`` returns a list with all results."
msgstr ""

#: ../../topics/selectors.rst:141
msgid "Notice that CSS selectors can select text or attribute nodes using CSS3 pseudo-elements:"
msgstr ""

#: ../../topics/selectors.rst:147
msgid "As you can see, ``.xpath()`` and ``.css()`` methods return a :class:`~scrapy.selector.SelectorList` instance, which is a list of new selectors. This API can be used for quickly selecting nested data:"
msgstr ""

#: ../../topics/selectors.rst:158
msgid "If you want to extract only the first matched element, you can call the selector ``.get()`` (or its alias ``.extract_first()`` commonly used in previous Scrapy versions):"
msgstr ""

#: ../../topics/selectors.rst:165
msgid "It returns ``None`` if no element was found:"
msgstr ""

#: ../../topics/selectors.rst:170
msgid "A default return value can be provided as an argument, to be used instead of ``None``:"
msgstr ""

#: ../../topics/selectors.rst:176
msgid "Instead of using e.g. ``'@src'`` XPath it is possible to query for attributes using ``.attrib`` property of a :class:`~scrapy.selector.Selector`:"
msgstr ""

#: ../../topics/selectors.rst:186
msgid "As a shortcut, ``.attrib`` is also available on SelectorList directly; it returns attributes for the first matching element:"
msgstr ""

#: ../../topics/selectors.rst:192
msgid "This is most useful when only a single result is expected, e.g. when selecting by id, or selecting unique elements on a web page:"
msgstr ""

#: ../../topics/selectors.rst:198
msgid "Now we're going to get the base URL and some image links:"
msgstr ""

#: ../../topics/selectors.rst:240
msgid "Extensions to CSS Selectors"
msgstr ""

#: ../../topics/selectors.rst:242
msgid "Per W3C standards, `CSS selectors`_ do not support selecting text nodes or attribute values. But selecting these is so essential in a web scraping context that Scrapy (parsel) implements a couple of **non-standard pseudo-elements**:"
msgstr ""

#: ../../topics/selectors.rst:247
msgid "to select text nodes, use ``::text``"
msgstr ""

#: ../../topics/selectors.rst:248
msgid "to select attribute values, use ``::attr(name)`` where *name* is the name of the attribute that you want the value of"
msgstr ""

#: ../../topics/selectors.rst:252
msgid "These pseudo-elements are Scrapy-/Parsel-specific. They will most probably not work with other libraries like `lxml`_ or `PyQuery`_."
msgstr ""

#: ../../topics/selectors.rst:260
msgid "``title::text`` selects children text nodes of a descendant ``<title>`` element:"
msgstr ""

#: ../../topics/selectors.rst:265
msgid "``*::text`` selects all descendant text nodes of the current selector context:"
msgstr ""

#: ../../topics/selectors.rst:280
msgid "``foo::text`` returns no results if ``foo`` element exists, but contains no text (i.e. text is empty):"
msgstr ""

#: ../../topics/selectors.rst:286
msgid "This means ``.css('foo::text').get()`` could return None even if an element exists. Use ``default=''`` if you always want a string:"
msgstr ""

#: ../../topics/selectors.rst:293
msgid "``a::attr(href)`` selects the *href* attribute value of descendant links:"
msgstr ""

#: ../../topics/selectors.rst:303
#: ../../topics/selectors.rst:928
#: ../../topics/selectors.rst:968
msgid "See also: :ref:`selecting-attributes`."
msgstr ""

#: ../../topics/selectors.rst:306
msgid "You cannot chain these pseudo-elements. But in practice it would not make much sense: text nodes do not have attributes, and attribute values are string values already and do not have children nodes."
msgstr ""

#: ../../topics/selectors.rst:315
msgid "Nesting selectors"
msgstr ""

#: ../../topics/selectors.rst:317
msgid "The selection methods (``.xpath()`` or ``.css()``) return a list of selectors of the same type, so you can call the selection methods for those selectors too. Here's an example:"
msgstr ""

#: ../../topics/selectors.rst:341
msgid "Selecting element attributes"
msgstr ""

#: ../../topics/selectors.rst:343
msgid "There are several ways to get a value of an attribute. First, one can use XPath syntax:"
msgstr ""

#: ../../topics/selectors.rst:349
msgid "XPath syntax has a few advantages: it is a standard XPath feature, and ``@attributes`` can be used in other parts of an XPath expression - e.g. it is possible to filter by attribute value."
msgstr ""

#: ../../topics/selectors.rst:353
msgid "Scrapy also provides an extension to CSS selectors (``::attr(...)``) which allows to get attribute values:"
msgstr ""

#: ../../topics/selectors.rst:359
msgid "In addition to that, there is a ``.attrib`` property of Selector. You can use it if you prefer to lookup attributes in Python code, without using XPaths or CSS extensions:"
msgstr ""

#: ../../topics/selectors.rst:366
msgid "This property is also available on SelectorList; it returns a dictionary with attributes of a first matching element. It is convenient to use when a selector is expected to give a single result (e.g. when selecting by element ID, or when selecting an unique element on a page):"
msgstr ""

#: ../../topics/selectors.rst:376
msgid "``.attrib`` property of an empty SelectorList is empty:"
msgstr ""

#: ../../topics/selectors.rst:382
msgid "Using selectors with regular expressions"
msgstr ""

#: ../../topics/selectors.rst:384
msgid ":class:`~scrapy.selector.Selector` also has a ``.re()`` method for extracting data using regular expressions. However, unlike using ``.xpath()`` or ``.css()`` methods, ``.re()`` returns a list of unicode strings. So you can't construct nested ``.re()`` calls."
msgstr ""

#: ../../topics/selectors.rst:389
msgid "Here's an example used to extract image names from the :ref:`HTML code <topics-selectors-htmlcode>` above:"
msgstr ""

#: ../../topics/selectors.rst:399
msgid "There's an additional helper reciprocating ``.get()`` (and its alias ``.extract_first()``) for ``.re()``, named ``.re_first()``. Use it to extract just the first matching string:"
msgstr ""

#: ../../topics/selectors.rst:409
msgid "extract() and extract_first()"
msgstr ""

#: ../../topics/selectors.rst:411
msgid "If you're a long-time Scrapy user, you're probably familiar with ``.extract()`` and ``.extract_first()`` selector methods. Many blog posts and tutorials are using them as well. These methods are still supported by Scrapy, there are **no plans** to deprecate them."
msgstr ""

#: ../../topics/selectors.rst:416
msgid "However, Scrapy usage docs are now written using ``.get()`` and ``.getall()`` methods. We feel that these new methods result in a more concise and readable code."
msgstr ""

#: ../../topics/selectors.rst:420
msgid "The following examples show how these methods map to each other."
msgstr ""

#: ../../topics/selectors.rst:422
msgid "``SelectorList.get()`` is the same as ``SelectorList.extract_first()``:"
msgstr ""

#: ../../topics/selectors.rst:429
msgid "``SelectorList.getall()`` is the same as ``SelectorList.extract()``:"
msgstr ""

#: ../../topics/selectors.rst:436
msgid "``Selector.get()`` is the same as ``Selector.extract()``:"
msgstr ""

#: ../../topics/selectors.rst:443
msgid "For consistency, there is also ``Selector.getall()``, which returns a list:"
msgstr ""

#: ../../topics/selectors.rst:448
msgid "So, the main difference is that output of ``.get()`` and ``.getall()`` methods is more predictable: ``.get()`` always returns a single result, ``.getall()`` always returns a list of all extracted results. With ``.extract()`` method it was not always obvious if a result is a list or not; to get a single result either ``.extract()`` or ``.extract_first()`` should be called."
msgstr ""

#: ../../topics/selectors.rst:458
msgid "Working with XPaths"
msgstr ""

#: ../../topics/selectors.rst:460
msgid "Here are some tips which may help you to use XPath with Scrapy selectors effectively. If you are not much familiar with XPath yet, you may want to take a look first at this `XPath tutorial`_."
msgstr ""

#: ../../topics/selectors.rst:465
msgid "Some of the tips are based on `this post from ScrapingHub's blog`_."
msgstr ""

#: ../../topics/selectors.rst:474
msgid "Working with relative XPaths"
msgstr ""

#: ../../topics/selectors.rst:476
msgid "Keep in mind that if you are nesting selectors and use an XPath that starts with ``/``, that XPath will be absolute to the document and not relative to the ``Selector`` you're calling it from."
msgstr ""

#: ../../topics/selectors.rst:480
msgid "For example, suppose you want to extract all ``<p>`` elements inside ``<div>`` elements. First, you would get all ``<div>`` elements:"
msgstr ""

#: ../../topics/selectors.rst:485
msgid "At first, you may be tempted to use the following approach, which is wrong, as it actually extracts all ``<p>`` elements from the document, not only those inside ``<div>`` elements:"
msgstr ""

#: ../../topics/selectors.rst:492
msgid "This is the proper way to do it (note the dot prefixing the ``.//p`` XPath):"
msgstr ""

#: ../../topics/selectors.rst:497
msgid "Another common case would be to extract all direct ``<p>`` children:"
msgstr ""

#: ../../topics/selectors.rst:502
msgid "For more details about relative XPaths see the `Location Paths`_ section in the XPath specification."
msgstr ""

#: ../../topics/selectors.rst:508
msgid "When querying by class, consider using CSS"
msgstr ""

#: ../../topics/selectors.rst:510
msgid "Because an element can contain multiple CSS classes, the XPath way to select elements by class is the rather verbose::"
msgstr ""

#: ../../topics/selectors.rst:515
msgid "If you use ``@class='someclass'`` you may end up missing elements that have other classes, and if you just use ``contains(@class, 'someclass')`` to make up for that you may end up with more elements that you want, if they have a different class name that shares the string ``someclass``."
msgstr ""

#: ../../topics/selectors.rst:520
msgid "As it turns out, Scrapy selectors allow you to chain selectors, so most of the time you can just select by class using CSS and then switch to XPath when needed:"
msgstr ""

#: ../../topics/selectors.rst:528
msgid "This is cleaner than using the verbose XPath trick shown above. Just remember to use the ``.`` in the XPath expressions that will follow."
msgstr ""

#: ../../topics/selectors.rst:532
msgid "Beware of the difference between //node[1] and (//node)[1]"
msgstr ""

#: ../../topics/selectors.rst:534
msgid "``//node[1]`` selects all the nodes occurring first under their respective parents."
msgstr ""

#: ../../topics/selectors.rst:536
msgid "``(//node)[1]`` selects all the nodes in the document, and then gets only the first of them."
msgstr ""

#: ../../topics/selectors.rst:554
msgid "This gets all first ``<li>``  elements under whatever it is its parent:"
msgstr ""

#: ../../topics/selectors.rst:559
msgid "And this gets the first ``<li>``  element in the whole document:"
msgstr ""

#: ../../topics/selectors.rst:564
msgid "This gets all first ``<li>``  elements under an ``<ul>``  parent:"
msgstr ""

#: ../../topics/selectors.rst:569
msgid "And this gets the first ``<li>``  element under an ``<ul>``  parent in the whole document:"
msgstr ""

#: ../../topics/selectors.rst:575
msgid "Using text nodes in a condition"
msgstr ""

#: ../../topics/selectors.rst:577
msgid "When you need to use the text content as argument to an `XPath string function`_, avoid using ``.//text()`` and use just ``.`` instead."
msgstr ""

#: ../../topics/selectors.rst:580
msgid "This is because the expression ``.//text()`` yields a collection of text elements -- a *node-set*. And when a node-set is converted to a string, which happens when it is passed as argument to a string function like ``contains()`` or ``starts-with()``, it results in the text for the first element only."
msgstr ""

#: ../../topics/selectors.rst:589
msgid "Converting a *node-set* to string:"
msgstr ""

#: ../../topics/selectors.rst:596
msgid "A *node* converted to a string, however, puts together the text of itself plus of all its descendants:"
msgstr ""

#: ../../topics/selectors.rst:603
msgid "So, using the ``.//text()`` node-set won't select anything in this case:"
msgstr ""

#: ../../topics/selectors.rst:608
msgid "But using the ``.`` to mean the node, works:"
msgstr ""

#: ../../topics/selectors.rst:618
msgid "Variables in XPath expressions"
msgstr ""

#: ../../topics/selectors.rst:620
msgid "XPath allows you to reference variables in your XPath expressions, using the ``$somevariable`` syntax. This is somewhat similar to parameterized queries or prepared statements in the SQL world where you replace some arguments in your queries with placeholders like ``?``, which are then substituted with values passed with the query."
msgstr ""

#: ../../topics/selectors.rst:626
msgid "Here's an example to match an element based on its \"id\" attribute value, without hard-coding it (that was shown previously):"
msgstr ""

#: ../../topics/selectors.rst:633
msgid "Here's another example, to find the \"id\" attribute of a ``<div>`` tag containing five ``<a>`` children (here we pass the value ``5`` as an integer):"
msgstr ""

#: ../../topics/selectors.rst:639
msgid "All variable references must have a binding value when calling ``.xpath()`` (otherwise you'll get a ``ValueError: XPath error:`` exception). This is done by passing as many named arguments as necessary."
msgstr ""

#: ../../topics/selectors.rst:643
msgid "`parsel`_, the library powering Scrapy selectors, has more details and examples on `XPath variables`_."
msgstr ""

#: ../../topics/selectors.rst:652
msgid "Removing namespaces"
msgstr ""

#: ../../topics/selectors.rst:654
msgid "When dealing with scraping projects, it is often quite convenient to get rid of namespaces altogether and just work with element names, to write more simple/convenient XPaths. You can use the :meth:`Selector.remove_namespaces` method for that."
msgstr ""

#: ../../topics/selectors.rst:659
msgid "Let's show an example that illustrates this with the Python Insider blog atom feed."
msgstr ""

#: ../../topics/selectors.rst:663
msgid "First, we open the shell with the url we want to scrape::"
msgstr ""

#: ../../topics/selectors.rst:667
msgid "This is how the file starts::"
msgstr ""

#: ../../topics/selectors.rst:680
msgid "You can see several namespace declarations including a default \"http://www.w3.org/2005/Atom\" and another one using the \"gd:\" prefix for \"http://schemas.google.com/g/2005\"."
msgstr ""

#: ../../topics/selectors.rst:686
msgid "Once in the shell we can try selecting all ``<link>`` objects and see that it doesn't work (because the Atom XML namespace is obfuscating those nodes):"
msgstr ""

#: ../../topics/selectors.rst:692
msgid "But once we call the :meth:`Selector.remove_namespaces` method, all nodes can be accessed directly by their names:"
msgstr ""

#: ../../topics/selectors.rst:701
msgid "If you wonder why the namespace removal procedure isn't always called by default instead of having to call it manually, this is because of two reasons, which, in order of relevance, are:"
msgstr ""

#: ../../topics/selectors.rst:705
msgid "Removing namespaces requires to iterate and modify all nodes in the document, which is a reasonably expensive operation to perform by default for all documents crawled by Scrapy"
msgstr ""

#: ../../topics/selectors.rst:709
msgid "There could be some cases where using namespaces is actually required, in case some element names clash between namespaces. These cases are very rare though."
msgstr ""

#: ../../topics/selectors.rst:715
msgid "Using EXSLT extensions"
msgstr ""

#: ../../topics/selectors.rst:717
msgid "Being built atop `lxml`_, Scrapy selectors support some `EXSLT`_ extensions and come with these pre-registered namespaces to use in XPath expressions:"
msgstr ""

#: ../../topics/selectors.rst:722
msgid "prefix"
msgstr ""

#: ../../topics/selectors.rst:722
msgid "namespace"
msgstr ""

#: ../../topics/selectors.rst:722
msgid "usage"
msgstr ""

#: ../../topics/selectors.rst:724
msgid "re"
msgstr ""

#: ../../topics/selectors.rst:724
msgid "\\http://exslt.org/regular-expressions"
msgstr ""

#: ../../topics/selectors.rst:724
msgid "`regular expressions`_"
msgstr ""

#: ../../topics/selectors.rst:725
msgid "set"
msgstr ""

#: ../../topics/selectors.rst:725
msgid "\\http://exslt.org/sets"
msgstr ""

#: ../../topics/selectors.rst:725
msgid "`set manipulation`_"
msgstr ""

#: ../../topics/selectors.rst:729
msgid "Regular expressions"
msgstr ""

#: ../../topics/selectors.rst:731
msgid "The ``test()`` function, for example, can prove quite useful when XPath's ``starts-with()`` or ``contains()`` are not sufficient."
msgstr ""

#: ../../topics/selectors.rst:734
msgid "Example selecting links in list item with a \"class\" attribute ending with a digit:"
msgstr ""

#: ../../topics/selectors.rst:754
msgid "C library ``libxslt`` doesn't natively support EXSLT regular expressions so `lxml`_'s implementation uses hooks to Python's ``re`` module. Thus, using regexp functions in your XPath expressions may add a small performance penalty."
msgstr ""

#: ../../topics/selectors.rst:760
msgid "Set operations"
msgstr ""

#: ../../topics/selectors.rst:762
msgid "These can be handy for excluding parts of a document tree before extracting text elements for example."
msgstr ""

#: ../../topics/selectors.rst:765
msgid "Example extracting microdata (sample content taken from https://schema.org/Product) with groups of itemscopes and corresponding itemprops::"
msgstr ""

#: ../../topics/selectors.rst:849
msgid "Here we first iterate over ``itemscope`` elements, and for each one, we look for all ``itemprops`` elements and exclude those that are themselves inside another ``itemscope``."
msgstr ""

#: ../../topics/selectors.rst:858
msgid "Other XPath extensions"
msgstr ""

#: ../../topics/selectors.rst:860
msgid "Scrapy selectors also provide a sorely missed XPath extension function ``has-class`` that returns ``True`` for nodes that have all of the specified HTML classes."
msgstr ""

#: ../../topics/selectors.rst:866
msgid "For the following HTML::"
msgstr ""

#: ../../topics/selectors.rst:875
msgid "You can use it like this:"
msgstr ""

#: ../../topics/selectors.rst:885
msgid "So XPath ``//p[has-class(\"foo\", \"bar-baz\")]`` is roughly equivalent to CSS ``p.foo.bar-baz``.  Please note, that it is slower in most of the cases, because it's a pure-Python function that's invoked for every node in question whereas the CSS lookup is translated into XPath and thus runs more efficiently, so performance-wise its uses are limited to situations that are not easily described with CSS selectors."
msgstr ""

#: ../../topics/selectors.rst:892
msgid "Parsel also simplifies adding your own XPath extensions."
msgstr ""

#: ../../../.tox/docs-i18n/lib/python3.8/site-packages/parsel/xpathfuncs.py:docstring of parsel.xpathfuncs.set_xpathfunc:1
msgid "Register a custom extension function to use in XPath expressions."
msgstr ""

#: ../../../.tox/docs-i18n/lib/python3.8/site-packages/parsel/xpathfuncs.py:docstring of parsel.xpathfuncs.set_xpathfunc:3
msgid "The function ``func`` registered under ``fname`` identifier will be called for every matching node, being passed a ``context`` parameter as well as any parameters passed from the corresponding XPath expression."
msgstr ""

#: ../../../.tox/docs-i18n/lib/python3.8/site-packages/parsel/xpathfuncs.py:docstring of parsel.xpathfuncs.set_xpathfunc:7
msgid "If ``func`` is ``None``, the extension function will be removed."
msgstr ""

#: ../../../.tox/docs-i18n/lib/python3.8/site-packages/parsel/xpathfuncs.py:docstring of parsel.xpathfuncs.set_xpathfunc:9
msgid "See more `in lxml documentation`_."
msgstr ""

#: ../../topics/selectors.rst:900
msgid "Built-in Selectors reference"
msgstr ""

#: ../../topics/selectors.rst:906
msgid "Selector objects"
msgstr ""

#: ../../../scrapy/selector/unified.py:docstring of scrapy.selector.Selector:1
msgid "An instance of :class:`Selector` is a wrapper over response to select certain parts of its content."
msgstr ""

#: ../../../scrapy/selector/unified.py:docstring of scrapy.selector.Selector:4
msgid "``response`` is an :class:`~scrapy.http.HtmlResponse` or an :class:`~scrapy.http.XmlResponse` object that will be used for selecting and extracting data."
msgstr ""

#: ../../../scrapy/selector/unified.py:docstring of scrapy.selector.Selector:8
msgid "``text`` is a unicode string or utf-8 encoded text for cases when a ``response`` isn't available. Using ``text`` and ``response`` together is undefined behavior."
msgstr ""

#: ../../../scrapy/selector/unified.py:docstring of scrapy.selector.Selector:12
msgid "``type`` defines the selector type, it can be ``\"html\"``, ``\"xml\"`` or ``None`` (default)."
msgstr ""

#: ../../../scrapy/selector/unified.py:docstring of scrapy.selector.Selector:15
msgid "If ``type`` is ``None``, the selector automatically chooses the best type based on ``response`` type (see below), or defaults to ``\"html\"`` in case it is used together with ``text``."
msgstr ""

#: ../../../scrapy/selector/unified.py:docstring of scrapy.selector.Selector:19
msgid "If ``type`` is ``None`` and a ``response`` is passed, the selector type is inferred from the response type as follows:"
msgstr ""

#: ../../../scrapy/selector/unified.py:docstring of scrapy.selector.Selector:22
msgid "``\"html\"`` for :class:`~scrapy.http.HtmlResponse` type"
msgstr ""

#: ../../../scrapy/selector/unified.py:docstring of scrapy.selector.Selector:23
msgid "``\"xml\"`` for :class:`~scrapy.http.XmlResponse` type"
msgstr ""

#: ../../../scrapy/selector/unified.py:docstring of scrapy.selector.Selector:24
msgid "``\"html\"`` for anything else"
msgstr ""

#: ../../../scrapy/selector/unified.py:docstring of scrapy.selector.Selector:26
msgid "Otherwise, if ``type`` is set, the selector type will be forced and no detection will occur."
msgstr ""

#: ../../../.tox/docs-i18n/lib/python3.8/site-packages/parsel/selector.py:docstring of scrapy.selector.Selector.xpath:1
msgid "Find nodes matching the xpath ``query`` and return the result as a :class:`SelectorList` instance with all elements flattened. List elements implement :class:`Selector` interface too."
msgstr ""

#: ../../../.tox/docs-i18n/lib/python3.8/site-packages/parsel/selector.py:docstring of scrapy.selector.Selector.xpath:5
msgid "``query`` is a string containing the XPATH query to apply."
msgstr ""

#: ../../../.tox/docs-i18n/lib/python3.8/site-packages/parsel/selector.py:docstring of scrapy.selector.Selector.xpath:7
#: ../../../.tox/docs-i18n/lib/python3.8/site-packages/parsel/selector.py:docstring of scrapy.selector.SelectorList.xpath:6
msgid "``namespaces`` is an optional ``prefix: namespace-uri`` mapping (dict) for additional prefixes to those registered with ``register_namespace(prefix, uri)``. Contrary to ``register_namespace()``, these prefixes are not saved for future calls."
msgstr ""

#: ../../../.tox/docs-i18n/lib/python3.8/site-packages/parsel/selector.py:docstring of scrapy.selector.Selector.xpath:12
#: ../../../.tox/docs-i18n/lib/python3.8/site-packages/parsel/selector.py:docstring of scrapy.selector.SelectorList.xpath:11
msgid "Any additional named arguments can be used to pass values for XPath variables in the XPath expression, e.g.::"
msgstr ""

#: ../../topics/selectors.rst:914
msgid "For convenience, this method can be called as ``response.xpath()``"
msgstr ""

#: ../../../.tox/docs-i18n/lib/python3.8/site-packages/parsel/selector.py:docstring of scrapy.selector.Selector.css:1
msgid "Apply the given CSS selector and return a :class:`SelectorList` instance."
msgstr ""

#: ../../../.tox/docs-i18n/lib/python3.8/site-packages/parsel/selector.py:docstring of scrapy.selector.Selector.css:3
msgid "``query`` is a string containing the CSS selector to apply."
msgstr ""

#: ../../../.tox/docs-i18n/lib/python3.8/site-packages/parsel/selector.py:docstring of scrapy.selector.Selector.css:5
msgid "In the background, CSS queries are translated into XPath queries using `cssselect`_ library and run ``.xpath()`` method."
msgstr ""

#: ../../topics/selectors.rst:920
msgid "For convenience, this method can be called as ``response.css()``"
msgstr ""

#: ../../../.tox/docs-i18n/lib/python3.8/site-packages/parsel/selector.py:docstring of scrapy.selector.Selector.get:1
msgid "Serialize and return the matched nodes in a single unicode string. Percent encoded content is unquoted."
msgstr ""

#: ../../topics/selectors.rst:924
#: ../../topics/selectors.rst:956
#: ../../topics/selectors.rst:960
msgid "See also: :ref:`old-extraction-api`"
msgstr ""

#: ../../../scrapy/selector/unified.py:docstring of scrapy.selector.Selector.attrib:1
msgid "Return the attributes dictionary for underlying element."
msgstr ""

#: ../../../.tox/docs-i18n/lib/python3.8/site-packages/parsel/selector.py:docstring of scrapy.selector.Selector.re:1
msgid "Apply the given regex and return a list of unicode strings with the matches."
msgstr ""

#: ../../../.tox/docs-i18n/lib/python3.8/site-packages/parsel/selector.py:docstring of scrapy.selector.Selector.re:4
msgid "``regex`` can be either a compiled regular expression or a string which will be compiled to a regular expression using ``re.compile(regex)``."
msgstr ""

#: ../../../.tox/docs-i18n/lib/python3.8/site-packages/parsel/selector.py:docstring of scrapy.selector.Selector.re:7
#: ../../../.tox/docs-i18n/lib/python3.8/site-packages/parsel/selector.py:docstring of scrapy.selector.Selector.re_first:5
msgid "By default, character entity references are replaced by their corresponding character (except for ``&amp;`` and ``&lt;``). Passing ``replace_entities`` as ``False`` switches off these replacements."
msgstr ""

#: ../../../.tox/docs-i18n/lib/python3.8/site-packages/parsel/selector.py:docstring of scrapy.selector.Selector.re_first:1
msgid "Apply the given regex and return the first unicode string which matches. If there is no match, return the default value (``None`` if the argument is not provided)."
msgstr ""

#: ../../../.tox/docs-i18n/lib/python3.8/site-packages/parsel/selector.py:docstring of scrapy.selector.Selector.register_namespace:1
msgid "Register the given namespace to be used in this :class:`Selector`. Without registering namespaces you can't select or extract data from non-standard namespaces. See :ref:`selector-examples-xml`."
msgstr ""

#: ../../../.tox/docs-i18n/lib/python3.8/site-packages/parsel/selector.py:docstring of scrapy.selector.Selector.remove_namespaces:1
msgid "Remove all namespaces, allowing to traverse the document using namespace-less xpaths. See :ref:`removing-namespaces`."
msgstr ""

#: ../../../.tox/docs-i18n/lib/python3.8/site-packages/parsel/selector.py:docstring of scrapy.selector.Selector.__bool__:1
msgid "Return ``True`` if there is any real content selected or ``False`` otherwise.  In other words, the boolean value of a :class:`Selector` is given by the contents it selects."
msgstr ""

#: ../../../.tox/docs-i18n/lib/python3.8/site-packages/parsel/selector.py:docstring of scrapy.selector.Selector.getall:1
msgid "Serialize and return the matched node in a 1-element list of unicode strings."
msgstr ""

#: ../../topics/selectors.rst:942
msgid "This method is added to Selector for consistency; it is more useful with SelectorList. See also: :ref:`old-extraction-api`"
msgstr ""

#: ../../topics/selectors.rst:946
msgid "SelectorList objects"
msgstr ""

#: ../../../scrapy/selector/unified.py:docstring of scrapy.selector.SelectorList:1
msgid "The :class:`SelectorList` class is a subclass of the builtin ``list`` class, which provides a few additional methods."
msgstr ""

#: ../../../.tox/docs-i18n/lib/python3.8/site-packages/parsel/selector.py:docstring of scrapy.selector.SelectorList.xpath:1
msgid "Call the ``.xpath()`` method for each element in this list and return their results flattened as another :class:`SelectorList`."
msgstr ""

#: ../../../.tox/docs-i18n/lib/python3.8/site-packages/parsel/selector.py:docstring of scrapy.selector.SelectorList.xpath:4
msgid "``query`` is the same argument as the one in :meth:`Selector.xpath`"
msgstr ""

#: ../../../.tox/docs-i18n/lib/python3.8/site-packages/parsel/selector.py:docstring of scrapy.selector.SelectorList.css:1
msgid "Call the ``.css()`` method for each element in this list and return their results flattened as another :class:`SelectorList`."
msgstr ""

#: ../../../.tox/docs-i18n/lib/python3.8/site-packages/parsel/selector.py:docstring of scrapy.selector.SelectorList.css:4
msgid "``query`` is the same argument as the one in :meth:`Selector.css`"
msgstr ""

#: ../../../.tox/docs-i18n/lib/python3.8/site-packages/parsel/selector.py:docstring of scrapy.selector.SelectorList.getall:1
msgid "Call the ``.get()`` method for each element is this list and return their results flattened, as a list of unicode strings."
msgstr ""

#: ../../../.tox/docs-i18n/lib/python3.8/site-packages/parsel/selector.py:docstring of scrapy.selector.SelectorList.get:1
msgid "Return the result of ``.get()`` for the first element in this list. If the list is empty, return the default value."
msgstr ""

#: ../../../.tox/docs-i18n/lib/python3.8/site-packages/parsel/selector.py:docstring of scrapy.selector.SelectorList.re:1
msgid "Call the ``.re()`` method for each element in this list and return their results flattened, as a list of unicode strings."
msgstr ""

#: ../../../.tox/docs-i18n/lib/python3.8/site-packages/parsel/selector.py:docstring of scrapy.selector.SelectorList.re:4
#: ../../../.tox/docs-i18n/lib/python3.8/site-packages/parsel/selector.py:docstring of scrapy.selector.SelectorList.re_first:6
msgid "By default, character entity references are replaced by their corresponding character (except for ``&amp;`` and ``&lt;``. Passing ``replace_entities`` as ``False`` switches off these replacements."
msgstr ""

#: ../../../.tox/docs-i18n/lib/python3.8/site-packages/parsel/selector.py:docstring of scrapy.selector.SelectorList.re_first:1
msgid "Call the ``.re()`` method for the first element in this list and return the result in an unicode string. If the list is empty or the regex doesn't match anything, return the default value (``None`` if the argument is not provided)."
msgstr ""

#: ../../../scrapy/selector/unified.py:docstring of scrapy.selector.SelectorList.attrib:1
msgid "Return the attributes dictionary for the first element. If the list is empty, return an empty dict."
msgstr ""

#: ../../topics/selectors.rst:973
msgid "Examples"
msgstr ""

#: ../../topics/selectors.rst:978
msgid "Selector examples on HTML response"
msgstr ""

#: ../../topics/selectors.rst:980
msgid "Here are some :class:`Selector` examples to illustrate several concepts. In all cases, we assume there is already a :class:`Selector` instantiated with a :class:`~scrapy.http.HtmlResponse` object like this::"
msgstr ""

#: ../../topics/selectors.rst:986
msgid "Select all ``<h1>`` elements from an HTML response body, returning a list of :class:`Selector` objects (i.e. a :class:`SelectorList` object)::"
msgstr ""

#: ../../topics/selectors.rst:991
msgid "Extract the text of all ``<h1>`` elements from an HTML response body, returning a list of unicode strings::"
msgstr ""

#: ../../topics/selectors.rst:997
msgid "Iterate over all ``<p>`` tags and print their class attribute::"
msgstr ""

#: ../../topics/selectors.rst:1006
msgid "Selector examples on XML response"
msgstr ""

#: ../../topics/selectors.rst:1008
msgid "Here are some examples to illustrate concepts for :class:`Selector` objects instantiated with an :class:`~scrapy.http.XmlResponse` object::"
msgstr ""

#: ../../topics/selectors.rst:1013
msgid "Select all ``<product>`` elements from an XML response body, returning a list of :class:`Selector` objects (i.e. a :class:`SelectorList` object)::"
msgstr ""

#: ../../topics/selectors.rst:1018
msgid "Extract all prices from a `Google Base XML feed`_ which requires registering a namespace::"
msgstr ""

#: ../../topics/settings.rst:7
msgid "The Scrapy settings allows you to customize the behaviour of all Scrapy components, including the core, extensions, pipelines and spiders themselves."
msgstr ""

#: ../../topics/settings.rst:10
msgid "The infrastructure of the settings provides a global namespace of key-value mappings that the code can use to pull configuration values from. The settings can be populated through different mechanisms, which are described below."
msgstr ""

#: ../../topics/settings.rst:14
msgid "The settings are also the mechanism for selecting the currently active Scrapy project (in case you have many)."
msgstr ""

#: ../../topics/settings.rst:17
#: ../../topics/spiders.rst:98
msgid "For a list of available built-in settings see: :ref:`topics-settings-ref`."
msgstr ""

#: ../../topics/settings.rst:22
msgid "Designating the settings"
msgstr ""

#: ../../topics/settings.rst:24
msgid "When you use Scrapy, you have to tell it which settings you're using. You can do this by using an environment variable, ``SCRAPY_SETTINGS_MODULE``."
msgstr ""

#: ../../topics/settings.rst:27
msgid "The value of ``SCRAPY_SETTINGS_MODULE`` should be in Python path syntax, e.g. ``myproject.settings``. Note that the settings module should be on the Python :ref:`import search path <tut-searchpath>`."
msgstr ""

#: ../../topics/settings.rst:34
msgid "Populating the settings"
msgstr ""

#: ../../topics/settings.rst:36
msgid "Settings can be populated using different mechanisms, each of which having a different precedence. Here is the list of them in decreasing order of precedence:"
msgstr ""

#: ../../topics/settings.rst:40
msgid "Command line options (most precedence)"
msgstr ""

#: ../../topics/settings.rst:41
msgid "Settings per-spider"
msgstr ""

#: ../../topics/settings.rst:42
msgid "Project settings module"
msgstr ""

#: ../../topics/settings.rst:43
msgid "Default settings per-command"
msgstr ""

#: ../../topics/settings.rst:44
msgid "Default global settings (less precedence)"
msgstr ""

#: ../../topics/settings.rst:46
msgid "The population of these settings sources is taken care of internally, but a manual handling is possible using API calls. See the :ref:`topics-api-settings` topic for reference."
msgstr ""

#: ../../topics/settings.rst:50
msgid "These mechanisms are described in more detail below."
msgstr ""

#: ../../topics/settings.rst:53
msgid "1. Command line options"
msgstr ""

#: ../../topics/settings.rst:55
msgid "Arguments provided by the command line are the ones that take most precedence, overriding any other options. You can explicitly override one (or more) settings using the ``-s`` (or ``--set``) command line option."
msgstr ""

#: ../../topics/settings.rst:66
msgid "2. Settings per-spider"
msgstr ""

#: ../../topics/settings.rst:68
msgid "Spiders (See the :ref:`topics-spiders` chapter for reference) can define their own settings that will take precedence and override the project ones. They can do so by setting their :attr:`~scrapy.spiders.Spider.custom_settings` attribute::"
msgstr ""

#: ../../topics/settings.rst:80
msgid "3. Project settings module"
msgstr ""

#: ../../topics/settings.rst:82
msgid "The project settings module is the standard configuration file for your Scrapy project, it's where most of your custom settings will be populated. For a standard Scrapy project, this means you'll be adding or changing the settings in the ``settings.py`` file created for your project."
msgstr ""

#: ../../topics/settings.rst:88
msgid "4. Default settings per-command"
msgstr ""

#: ../../topics/settings.rst:90
msgid "Each :doc:`Scrapy tool </topics/commands>` command can have its own default settings, which override the global default settings. Those custom command settings are specified in the ``default_settings`` attribute of the command class."
msgstr ""

#: ../../topics/settings.rst:96
msgid "5. Default global settings"
msgstr ""

#: ../../topics/settings.rst:98
msgid "The global defaults are located in the ``scrapy.settings.default_settings`` module and documented in the :ref:`topics-settings-ref` section."
msgstr ""

#: ../../topics/settings.rst:102
msgid "How to access settings"
msgstr ""

#: ../../topics/settings.rst:106
msgid "In a spider, the settings are available through ``self.settings``::"
msgstr ""

#: ../../topics/settings.rst:116
msgid "The ``settings`` attribute is set in the base Spider class after the spider is initialized.  If you want to use the settings before the initialization (e.g., in your spider's ``__init__()`` method), you'll need to override the :meth:`~scrapy.spiders.Spider.from_crawler` method."
msgstr ""

#: ../../topics/settings.rst:121
msgid "Settings can be accessed through the :attr:`scrapy.crawler.Crawler.settings` attribute of the Crawler that is passed to ``from_crawler`` method in extensions, middlewares and item pipelines::"
msgstr ""

#: ../../topics/settings.rst:135
msgid "The settings object can be used like a dict (e.g., ``settings['LOG_ENABLED']``), but it's usually preferred to extract the setting in the format you need it to avoid type errors, using one of the methods provided by the :class:`~scrapy.settings.Settings` API."
msgstr ""

#: ../../topics/settings.rst:141
msgid "Rationale for setting names"
msgstr ""

#: ../../topics/settings.rst:143
msgid "Setting names are usually prefixed with the component that they configure. For example, proper setting names for a fictional robots.txt extension would be ``ROBOTSTXT_ENABLED``, ``ROBOTSTXT_OBEY``, ``ROBOTSTXT_CACHEDIR``, etc."
msgstr ""

#: ../../topics/settings.rst:151
msgid "Built-in settings reference"
msgstr ""

#: ../../topics/settings.rst:153
msgid "Here's a list of all available Scrapy settings, in alphabetical order, along with their default values and the scope where they apply."
msgstr ""

#: ../../topics/settings.rst:156
msgid "The scope, where available, shows where the setting is being used, if it's tied to any particular component. In that case the module of that component will be shown, typically an extension, middleware or pipeline. It also means that the component must be enabled in order for the setting to have any effect."
msgstr ""

#: ../../topics/settings.rst:164
msgid "AWS_ACCESS_KEY_ID"
msgstr ""

#: ../../topics/settings.rst:168
msgid "The AWS access key used by code that requires access to `Amazon Web services`_, such as the :ref:`S3 feed storage backend <topics-feed-storage-s3>`."
msgstr ""

#: ../../topics/settings.rst:174
msgid "AWS_SECRET_ACCESS_KEY"
msgstr ""

#: ../../topics/settings.rst:178
msgid "The AWS secret key used by code that requires access to `Amazon Web services`_, such as the :ref:`S3 feed storage backend <topics-feed-storage-s3>`."
msgstr ""

#: ../../topics/settings.rst:184
msgid "AWS_ENDPOINT_URL"
msgstr ""

#: ../../topics/settings.rst:188
msgid "Endpoint URL used for S3-like storage, for example Minio or s3.scality."
msgstr ""

#: ../../topics/settings.rst:193
msgid "AWS_USE_SSL"
msgstr ""

#: ../../topics/settings.rst:197
msgid "Use this option if you want to disable SSL connection for communication with S3 or S3-like storage. By default SSL will be used."
msgstr ""

#: ../../topics/settings.rst:203
msgid "AWS_VERIFY"
msgstr ""

#: ../../topics/settings.rst:207
msgid "Verify SSL connection between Scrapy and S3 or S3-like storage. By default SSL verification will occur."
msgstr ""

#: ../../topics/settings.rst:213
msgid "AWS_REGION_NAME"
msgstr ""

#: ../../topics/settings.rst:217
msgid "The name of the region associated with the AWS client."
msgstr ""

#: ../../topics/settings.rst:222
msgid "BOT_NAME"
msgstr ""

#: ../../topics/settings.rst:224
msgid "Default: ``'scrapybot'``"
msgstr ""

#: ../../topics/settings.rst:226
msgid "The name of the bot implemented by this Scrapy project (also known as the project name). This name will be used for the logging too."
msgstr ""

#: ../../topics/settings.rst:229
msgid "It's automatically populated with your project name when you create your project with the :command:`startproject` command."
msgstr ""

#: ../../topics/settings.rst:235
msgid "CONCURRENT_ITEMS"
msgstr ""

#: ../../topics/settings.rst:239
msgid "Maximum number of concurrent items (per response) to process in parallel in the Item Processor (also known as the :ref:`Item Pipeline <topics-item-pipeline>`)."
msgstr ""

#: ../../topics/settings.rst:245
msgid "CONCURRENT_REQUESTS"
msgstr ""

#: ../../topics/settings.rst:247
msgid "Default: ``16``"
msgstr ""

#: ../../topics/settings.rst:249
msgid "The maximum number of concurrent (i.e. simultaneous) requests that will be performed by the Scrapy downloader."
msgstr ""

#: ../../topics/settings.rst:255
msgid "CONCURRENT_REQUESTS_PER_DOMAIN"
msgstr ""

#: ../../topics/settings.rst:257
msgid "Default: ``8``"
msgstr ""

#: ../../topics/settings.rst:259
msgid "The maximum number of concurrent (i.e. simultaneous) requests that will be performed to any single domain."
msgstr ""

#: ../../topics/settings.rst:262
msgid "See also: :ref:`topics-autothrottle` and its :setting:`AUTOTHROTTLE_TARGET_CONCURRENCY` option."
msgstr ""

#: ../../topics/settings.rst:269
msgid "CONCURRENT_REQUESTS_PER_IP"
msgstr ""

#: ../../topics/settings.rst:273
msgid "The maximum number of concurrent (i.e. simultaneous) requests that will be performed to any single IP. If non-zero, the :setting:`CONCURRENT_REQUESTS_PER_DOMAIN` setting is ignored, and this one is used instead. In other words, concurrency limits will be applied per IP, not per domain."
msgstr ""

#: ../../topics/settings.rst:279
msgid "This setting also affects :setting:`DOWNLOAD_DELAY` and :ref:`topics-autothrottle`: if :setting:`CONCURRENT_REQUESTS_PER_IP` is non-zero, download delay is enforced per IP, not per domain."
msgstr ""

#: ../../topics/settings.rst:287
msgid "DEFAULT_ITEM_CLASS"
msgstr ""

#: ../../topics/settings.rst:289
msgid "Default: ``'scrapy.item.Item'``"
msgstr ""

#: ../../topics/settings.rst:291
msgid "The default class that will be used for instantiating items in the :ref:`the Scrapy shell <topics-shell>`."
msgstr ""

#: ../../topics/settings.rst:297
msgid "DEFAULT_REQUEST_HEADERS"
msgstr ""

#: ../../topics/settings.rst:306
msgid "The default headers used for Scrapy HTTP Requests. They're populated in the :class:`~scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware`."
msgstr ""

#: ../../topics/settings.rst:312
msgid "DEPTH_LIMIT"
msgstr ""

#: ../../topics/settings.rst:316
#: ../../topics/settings.rst:328
#: ../../topics/settings.rst:354
msgid "Scope: ``scrapy.spidermiddlewares.depth.DepthMiddleware``"
msgstr ""

#: ../../topics/settings.rst:318
msgid "The maximum depth that will be allowed to crawl for any site. If zero, no limit will be imposed."
msgstr ""

#: ../../topics/settings.rst:324
msgid "DEPTH_PRIORITY"
msgstr ""

#: ../../topics/settings.rst:330
msgid "An integer that is used to adjust the :attr:`~scrapy.http.Request.priority` of a :class:`~scrapy.http.Request` based on its depth."
msgstr ""

#: ../../topics/settings.rst:333
msgid "The priority of a request is adjusted as follows::"
msgstr ""

#: ../../topics/settings.rst:337
msgid "As depth increases, positive values of ``DEPTH_PRIORITY`` decrease request priority (BFO), while negative values increase request priority (DFO). See also :ref:`faq-bfo-dfo`."
msgstr ""

#: ../../topics/settings.rst:343
msgid "This setting adjusts priority **in the opposite way** compared to other priority settings :setting:`REDIRECT_PRIORITY_ADJUST` and :setting:`RETRY_PRIORITY_ADJUST`."
msgstr ""

#: ../../topics/settings.rst:350
msgid "DEPTH_STATS_VERBOSE"
msgstr ""

#: ../../topics/settings.rst:356
msgid "Whether to collect verbose depth stats. If this is enabled, the number of requests for each depth is collected in the stats."
msgstr ""

#: ../../topics/settings.rst:362
msgid "DNSCACHE_ENABLED"
msgstr ""

#: ../../topics/settings.rst:366
msgid "Whether to enable DNS in-memory cache."
msgstr ""

#: ../../topics/settings.rst:371
msgid "DNSCACHE_SIZE"
msgstr ""

#: ../../topics/settings.rst:373
msgid "Default: ``10000``"
msgstr ""

#: ../../topics/settings.rst:375
msgid "DNS in-memory cache size."
msgstr ""

#: ../../topics/settings.rst:380
msgid "DNS_RESOLVER"
msgstr ""

#: ../../topics/settings.rst:384
msgid "Default: ``'scrapy.resolver.CachingThreadedResolver'``"
msgstr ""

#: ../../topics/settings.rst:386
msgid "The class to be used to resolve DNS names. The default ``scrapy.resolver.CachingThreadedResolver`` supports specifying a timeout for DNS requests via the :setting:`DNS_TIMEOUT` setting, but works only with IPv4 addresses. Scrapy provides an alternative resolver, ``scrapy.resolver.CachingHostnameResolver``, which supports IPv4/IPv6 addresses but does not take the :setting:`DNS_TIMEOUT` setting into account."
msgstr ""

#: ../../topics/settings.rst:395
msgid "DNS_TIMEOUT"
msgstr ""

#: ../../topics/settings.rst:397
msgid "Default: ``60``"
msgstr ""

#: ../../topics/settings.rst:399
msgid "Timeout for processing of DNS queries in seconds. Float is supported."
msgstr ""

#: ../../topics/settings.rst:404
msgid "DOWNLOADER"
msgstr ""

#: ../../topics/settings.rst:406
msgid "Default: ``'scrapy.core.downloader.Downloader'``"
msgstr ""

#: ../../topics/settings.rst:408
msgid "The downloader to use for crawling."
msgstr ""

#: ../../topics/settings.rst:413
msgid "DOWNLOADER_HTTPCLIENTFACTORY"
msgstr ""

#: ../../topics/settings.rst:415
msgid "Default: ``'scrapy.core.downloader.webclient.ScrapyHTTPClientFactory'``"
msgstr ""

#: ../../topics/settings.rst:417
msgid "Defines a Twisted ``protocol.ClientFactory``  class to use for HTTP/1.0 connections (for ``HTTP10DownloadHandler``)."
msgstr ""

#: ../../topics/settings.rst:422
msgid "HTTP/1.0 is rarely used nowadays so you can safely ignore this setting, unless you really want to use HTTP/1.0 and override :setting:`DOWNLOAD_HANDLERS` for ``http(s)`` scheme accordingly, i.e. to ``'scrapy.core.downloader.handlers.http.HTTP10DownloadHandler'``."
msgstr ""

#: ../../topics/settings.rst:430
msgid "DOWNLOADER_CLIENTCONTEXTFACTORY"
msgstr ""

#: ../../topics/settings.rst:432
msgid "Default: ``'scrapy.core.downloader.contextfactory.ScrapyClientContextFactory'``"
msgstr ""

#: ../../topics/settings.rst:434
msgid "Represents the classpath to the ContextFactory to use."
msgstr ""

#: ../../topics/settings.rst:436
msgid "Here, \"ContextFactory\" is a Twisted term for SSL/TLS contexts, defining the TLS/SSL protocol version to use, whether to do certificate verification, or even enable client-side authentication (and various other things)."
msgstr ""

#: ../../topics/settings.rst:442
msgid "Scrapy default context factory **does NOT perform remote server certificate verification**. This is usually fine for web scraping."
msgstr ""

#: ../../topics/settings.rst:445
msgid "If you do need remote server certificate verification enabled, Scrapy also has another context factory class that you can set, ``'scrapy.core.downloader.contextfactory.BrowserLikeContextFactory'``, which uses the platform's certificates to validate remote endpoints."
msgstr ""

#: ../../topics/settings.rst:450
msgid "If you do use a custom ContextFactory, make sure its ``__init__`` method accepts a ``method`` parameter (this is the ``OpenSSL.SSL`` method mapping :setting:`DOWNLOADER_CLIENT_TLS_METHOD`), a ``tls_verbose_logging`` parameter (``bool``) and a ``tls_ciphers`` parameter (see :setting:`DOWNLOADER_CLIENT_TLS_CIPHERS`)."
msgstr ""

#: ../../topics/settings.rst:459
msgid "DOWNLOADER_CLIENT_TLS_CIPHERS"
msgstr ""

#: ../../topics/settings.rst:461
msgid "Default: ``'DEFAULT'``"
msgstr ""

#: ../../topics/settings.rst:463
msgid "Use  this setting to customize the TLS/SSL ciphers used by the default HTTP/1.1 downloader."
msgstr ""

#: ../../topics/settings.rst:466
msgid "The setting should contain a string in the `OpenSSL cipher list format`_, these ciphers will be used as client ciphers. Changing this setting may be necessary to access certain HTTPS websites: for example, you may need to use ``'DEFAULT:!DH'`` for a website with weak DH parameters or enable a specific cipher that is not included in ``DEFAULT`` if a website requires it."
msgstr ""

#: ../../topics/settings.rst:477
msgid "DOWNLOADER_CLIENT_TLS_METHOD"
msgstr ""

#: ../../topics/settings.rst:479
msgid "Default: ``'TLS'``"
msgstr ""

#: ../../topics/settings.rst:481
msgid "Use this setting to customize the TLS/SSL method used by the default HTTP/1.1 downloader."
msgstr ""

#: ../../topics/settings.rst:484
msgid "This setting must be one of these string values:"
msgstr ""

#: ../../topics/settings.rst:486
msgid "``'TLS'``: maps to OpenSSL's ``TLS_method()`` (a.k.a ``SSLv23_method()``), which allows protocol negotiation, starting from the highest supported by the platform; **default, recommended**"
msgstr ""

#: ../../topics/settings.rst:489
msgid "``'TLSv1.0'``: this value forces HTTPS connections to use TLS version 1.0 ; set this if you want the behavior of Scrapy<1.1"
msgstr ""

#: ../../topics/settings.rst:491
msgid "``'TLSv1.1'``: forces TLS version 1.1"
msgstr ""

#: ../../topics/settings.rst:492
msgid "``'TLSv1.2'``: forces TLS version 1.2"
msgstr ""

#: ../../topics/settings.rst:493
msgid "``'SSLv3'``: forces SSL version 3 (**not recommended**)"
msgstr ""

#: ../../topics/settings.rst:499
msgid "DOWNLOADER_CLIENT_TLS_VERBOSE_LOGGING"
msgstr ""

#: ../../topics/settings.rst:503
msgid "Setting this to ``True`` will enable DEBUG level messages about TLS connection parameters after establishing HTTPS connections. The kind of information logged depends on the versions of OpenSSL and pyOpenSSL."
msgstr ""

#: ../../topics/settings.rst:507
msgid "This setting is only used for the default :setting:`DOWNLOADER_CLIENTCONTEXTFACTORY`."
msgstr ""

#: ../../topics/settings.rst:513
msgid "DOWNLOADER_MIDDLEWARES"
msgstr ""

#: ../../topics/settings.rst:515
#: ../../topics/settings.rst:749
#: ../../topics/settings.rst:1253
#: ../../topics/settings.rst:1321
msgid "Default:: ``{}``"
msgstr ""

#: ../../topics/settings.rst:517
msgid "A dict containing the downloader middlewares enabled in your project, and their orders. For more info see :ref:`topics-downloader-middleware-setting`."
msgstr ""

#: ../../topics/settings.rst:523
msgid "DOWNLOADER_MIDDLEWARES_BASE"
msgstr ""

#: ../../topics/settings.rst:544
msgid "A dict containing the downloader middlewares enabled by default in Scrapy. Low orders are closer to the engine, high orders are closer to the downloader. You should never modify this setting in your project, modify :setting:`DOWNLOADER_MIDDLEWARES` instead.  For more info see :ref:`topics-downloader-middleware-setting`."
msgstr ""

#: ../../topics/settings.rst:553
msgid "DOWNLOADER_STATS"
msgstr ""

#: ../../topics/settings.rst:557
msgid "Whether to enable downloader stats collection."
msgstr ""

#: ../../topics/settings.rst:562
msgid "DOWNLOAD_DELAY"
msgstr ""

#: ../../topics/settings.rst:566
msgid "The amount of time (in secs) that the downloader should wait before downloading consecutive pages from the same website. This can be used to throttle the crawling speed to avoid hitting servers too hard. Decimal numbers are supported.  Example::"
msgstr ""

#: ../../topics/settings.rst:573
msgid "This setting is also affected by the :setting:`RANDOMIZE_DOWNLOAD_DELAY` setting (which is enabled by default). By default, Scrapy doesn't wait a fixed amount of time between requests, but uses a random interval between 0.5 * :setting:`DOWNLOAD_DELAY` and 1.5 * :setting:`DOWNLOAD_DELAY`."
msgstr ""

#: ../../topics/settings.rst:577
msgid "When :setting:`CONCURRENT_REQUESTS_PER_IP` is non-zero, delays are enforced per ip address instead of per domain."
msgstr ""

#: ../../topics/settings.rst:582
msgid "You can also change this setting per spider by setting ``download_delay`` spider attribute."
msgstr ""

#: ../../topics/settings.rst:588
msgid "DOWNLOAD_HANDLERS"
msgstr ""

#: ../../topics/settings.rst:592
msgid "A dict containing the request downloader handlers enabled in your project. See :setting:`DOWNLOAD_HANDLERS_BASE` for example format."
msgstr ""

#: ../../topics/settings.rst:598
msgid "DOWNLOAD_HANDLERS_BASE"
msgstr ""

#: ../../topics/settings.rst:611
msgid "A dict containing the request download handlers enabled by default in Scrapy. You should never modify this setting in your project, modify :setting:`DOWNLOAD_HANDLERS` instead."
msgstr ""

#: ../../topics/settings.rst:615
msgid "You can disable any of these download handlers by assigning ``None`` to their URI scheme in :setting:`DOWNLOAD_HANDLERS`. E.g., to disable the built-in FTP handler (without replacement), place this in your ``settings.py``::"
msgstr ""

#: ../../topics/settings.rst:626
msgid "DOWNLOAD_TIMEOUT"
msgstr ""

#: ../../topics/settings.rst:628
msgid "Default: ``180``"
msgstr ""

#: ../../topics/settings.rst:630
msgid "The amount of time (in secs) that the downloader will wait before timing out."
msgstr ""

#: ../../topics/settings.rst:634
msgid "This timeout can be set per spider using :attr:`download_timeout` spider attribute and per-request using :reqmeta:`download_timeout` Request.meta key."
msgstr ""

#: ../../topics/settings.rst:641
msgid "DOWNLOAD_MAXSIZE"
msgstr ""

#: ../../topics/settings.rst:643
msgid "Default: ``1073741824`` (1024MB)"
msgstr ""

#: ../../topics/settings.rst:645
msgid "The maximum response size (in bytes) that downloader will download."
msgstr ""

#: ../../topics/settings.rst:647
#: ../../topics/settings.rst:666
msgid "If you want to disable it set to 0."
msgstr ""

#: ../../topics/settings.rst:653
msgid "This size can be set per spider using :attr:`download_maxsize` spider attribute and per-request using :reqmeta:`download_maxsize` Request.meta key."
msgstr ""

#: ../../topics/settings.rst:660
msgid "DOWNLOAD_WARNSIZE"
msgstr ""

#: ../../topics/settings.rst:662
msgid "Default: ``33554432`` (32MB)"
msgstr ""

#: ../../topics/settings.rst:664
msgid "The response size (in bytes) that downloader will start to warn."
msgstr ""

#: ../../topics/settings.rst:670
msgid "This size can be set per spider using :attr:`download_warnsize` spider attribute and per-request using :reqmeta:`download_warnsize` Request.meta key."
msgstr ""

#: ../../topics/settings.rst:677
msgid "DOWNLOAD_FAIL_ON_DATALOSS"
msgstr ""

#: ../../topics/settings.rst:681
msgid "Whether or not to fail on broken responses, that is, declared ``Content-Length`` does not match content sent by the server or chunked response was not properly finish. If ``True``, these responses raise a ``ResponseFailed([_DataLoss])`` error. If ``False``, these responses are passed through and the flag ``dataloss`` is added to the response, i.e.: ``'dataloss' in response.flags`` is ``True``."
msgstr ""

#: ../../topics/settings.rst:688
msgid "Optionally, this can be set per-request basis by using the :reqmeta:`download_fail_on_dataloss` Request.meta key to ``False``."
msgstr ""

#: ../../topics/settings.rst:693
msgid "A broken response, or data loss error, may happen under several circumstances, from server misconfiguration to network errors to data corruption. It is up to the user to decide if it makes sense to process broken responses considering they may contain partial or incomplete content. If :setting:`RETRY_ENABLED` is ``True`` and this setting is set to ``True``, the ``ResponseFailed([_DataLoss])`` failure will be retried as usual."
msgstr ""

#: ../../topics/settings.rst:703
msgid "DUPEFILTER_CLASS"
msgstr ""

#: ../../topics/settings.rst:705
msgid "Default: ``'scrapy.dupefilters.RFPDupeFilter'``"
msgstr ""

#: ../../topics/settings.rst:707
msgid "The class used to detect and filter duplicate requests."
msgstr ""

#: ../../topics/settings.rst:709
msgid "The default (``RFPDupeFilter``) filters based on request fingerprint using the ``scrapy.utils.request.request_fingerprint`` function. In order to change the way duplicates are checked you could subclass ``RFPDupeFilter`` and override its ``request_fingerprint`` method. This method should accept scrapy :class:`~scrapy.http.Request` object and return its fingerprint (a string)."
msgstr ""

#: ../../topics/settings.rst:716
msgid "You can disable filtering of duplicate requests by setting :setting:`DUPEFILTER_CLASS` to ``'scrapy.dupefilters.BaseDupeFilter'``. Be very careful about this however, because you can get into crawling loops. It's usually a better idea to set the ``dont_filter`` parameter to ``True`` on the specific :class:`~scrapy.http.Request` that should not be filtered."
msgstr ""

#: ../../topics/settings.rst:726
msgid "DUPEFILTER_DEBUG"
msgstr ""

#: ../../topics/settings.rst:730
msgid "By default, ``RFPDupeFilter`` only logs the first duplicate request. Setting :setting:`DUPEFILTER_DEBUG` to ``True`` will make it log all duplicate requests."
msgstr ""

#: ../../topics/settings.rst:736
msgid "EDITOR"
msgstr ""

#: ../../topics/settings.rst:738
msgid "Default: ``vi`` (on Unix systems) or the IDLE editor (on Windows)"
msgstr ""

#: ../../topics/settings.rst:740
msgid "The editor to use for editing spiders with the :command:`edit` command. Additionally, if the ``EDITOR`` environment variable is set, the :command:`edit` command will prefer it over the default setting."
msgstr ""

#: ../../topics/settings.rst:747
msgid "EXTENSIONS"
msgstr ""

#: ../../topics/settings.rst:751
msgid "A dict containing the extensions enabled in your project, and their orders."
msgstr ""

#: ../../topics/settings.rst:756
msgid "EXTENSIONS_BASE"
msgstr ""

#: ../../topics/settings.rst:772
msgid "A dict containing the extensions available by default in Scrapy, and their orders. This setting contains all stable built-in extensions. Keep in mind that some of them need to be enabled through a setting."
msgstr ""

#: ../../topics/settings.rst:776
msgid "For more information See the :ref:`extensions user guide  <topics-extensions>` and the :ref:`list of available extensions <topics-extensions-ref>`."
msgstr ""

#: ../../topics/settings.rst:783
msgid "FEED_TEMPDIR"
msgstr ""

#: ../../topics/settings.rst:785
msgid "The Feed Temp dir allows you to set a custom folder to save crawler temporary files before uploading with :ref:`FTP feed storage <topics-feed-storage-ftp>` and :ref:`Amazon S3 <topics-feed-storage-s3>`."
msgstr ""

#: ../../topics/settings.rst:792
msgid "FTP_PASSIVE_MODE"
msgstr ""

#: ../../topics/settings.rst:796
msgid "Whether or not to use passive mode when initiating FTP transfers."
msgstr ""

#: ../../topics/settings.rst:802
msgid "FTP_PASSWORD"
msgstr ""

#: ../../topics/settings.rst:804
msgid "Default: ``\"guest\"``"
msgstr ""

#: ../../topics/settings.rst:806
msgid "The password to use for FTP connections when there is no ``\"ftp_password\"`` in ``Request`` meta."
msgstr ""

#: ../../topics/settings.rst:810
msgid "Paraphrasing `RFC 1635`_, although it is common to use either the password \"guest\" or one's e-mail address for anonymous FTP, some FTP servers explicitly ask for the user's e-mail address and will not allow login with the \"guest\" password."
msgstr ""

#: ../../topics/settings.rst:821
msgid "FTP_USER"
msgstr ""

#: ../../topics/settings.rst:823
msgid "Default: ``\"anonymous\"``"
msgstr ""

#: ../../topics/settings.rst:825
msgid "The username to use for FTP connections when there is no ``\"ftp_user\"`` in ``Request`` meta."
msgstr ""

#: ../../topics/settings.rst:831
msgid "ITEM_PIPELINES"
msgstr ""

#: ../../topics/settings.rst:835
msgid "A dict containing the item pipelines to use, and their orders. Order values are arbitrary, but it is customary to define them in the 0-1000 range. Lower orders process before higher orders."
msgstr ""

#: ../../topics/settings.rst:849
msgid "ITEM_PIPELINES_BASE"
msgstr ""

#: ../../topics/settings.rst:853
msgid "A dict containing the pipelines enabled by default in Scrapy. You should never modify this setting in your project, modify :setting:`ITEM_PIPELINES` instead."
msgstr ""

#: ../../topics/settings.rst:859
msgid "LOG_ENABLED"
msgstr ""

#: ../../topics/settings.rst:863
msgid "Whether to enable logging."
msgstr ""

#: ../../topics/settings.rst:868
msgid "LOG_ENCODING"
msgstr ""

#: ../../topics/settings.rst:870
msgid "Default: ``'utf-8'``"
msgstr ""

#: ../../topics/settings.rst:872
msgid "The encoding to use for logging."
msgstr ""

#: ../../topics/settings.rst:877
msgid "LOG_FILE"
msgstr ""

#: ../../topics/settings.rst:881
msgid "File name to use for logging output. If ``None``, standard error will be used."
msgstr ""

#: ../../topics/settings.rst:886
msgid "LOG_FORMAT"
msgstr ""

#: ../../topics/settings.rst:888
msgid "Default: ``'%(asctime)s [%(name)s] %(levelname)s: %(message)s'``"
msgstr ""

#: ../../topics/settings.rst:890
msgid "String for formatting log messages. Refer to the :ref:`Python logging documentation <logrecord-attributes>` for the qwhole list of available placeholders."
msgstr ""

#: ../../topics/settings.rst:897
msgid "LOG_DATEFORMAT"
msgstr ""

#: ../../topics/settings.rst:899
msgid "Default: ``'%Y-%m-%d %H:%M:%S'``"
msgstr ""

#: ../../topics/settings.rst:901
msgid "String for formatting date/time, expansion of the ``%(asctime)s`` placeholder in :setting:`LOG_FORMAT`. Refer to the :ref:`Python datetime documentation <strftime-strptime-behavior>` for the whole list of available directives."
msgstr ""

#: ../../topics/settings.rst:909
msgid "LOG_FORMATTER"
msgstr ""

#: ../../topics/settings.rst:911
msgid "Default: :class:`scrapy.logformatter.LogFormatter`"
msgstr ""

#: ../../topics/settings.rst:913
msgid "The class to use for :ref:`formatting log messages <custom-log-formats>` for different actions."
msgstr ""

#: ../../topics/settings.rst:918
msgid "LOG_LEVEL"
msgstr ""

#: ../../topics/settings.rst:920
msgid "Default: ``'DEBUG'``"
msgstr ""

#: ../../topics/settings.rst:922
msgid "Minimum level to log. Available levels are: CRITICAL, ERROR, WARNING, INFO, DEBUG. For more info see :ref:`topics-logging`."
msgstr ""

#: ../../topics/settings.rst:928
msgid "LOG_STDOUT"
msgstr ""

#: ../../topics/settings.rst:932
msgid "If ``True``, all standard output (and error) of your process will be redirected to the log. For example if you ``print('hello')`` it will appear in the Scrapy log."
msgstr ""

#: ../../topics/settings.rst:939
msgid "LOG_SHORT_NAMES"
msgstr ""

#: ../../topics/settings.rst:943
msgid "If ``True``, the logs will just contain the root path. If it is set to ``False`` then it displays the component responsible for the log output"
msgstr ""

#: ../../topics/settings.rst:949
msgid "LOGSTATS_INTERVAL"
msgstr ""

#: ../../topics/settings.rst:953
msgid "The interval (in seconds) between each logging printout of the stats by :class:`~scrapy.extensions.logstats.LogStats`."
msgstr ""

#: ../../topics/settings.rst:959
msgid "MEMDEBUG_ENABLED"
msgstr ""

#: ../../topics/settings.rst:963
msgid "Whether to enable memory debugging."
msgstr ""

#: ../../topics/settings.rst:968
msgid "MEMDEBUG_NOTIFY"
msgstr ""

#: ../../topics/settings.rst:972
msgid "When memory debugging is enabled a memory report will be sent to the specified addresses if this setting is not empty, otherwise the report will be written to the log."
msgstr ""

#: ../../topics/settings.rst:983
msgid "MEMUSAGE_ENABLED"
msgstr ""

#: ../../topics/settings.rst:987
#: ../../topics/settings.rst:1004
#: ../../topics/settings.rst:1020
#: ../../topics/settings.rst:1038
#: ../../topics/settings.rst:1055
msgid "Scope: ``scrapy.extensions.memusage``"
msgstr ""

#: ../../topics/settings.rst:989
msgid "Whether to enable the memory usage extension. This extension keeps track of a peak memory used by the process (it writes it to stats). It can also optionally shutdown the Scrapy process when it exceeds a memory limit (see :setting:`MEMUSAGE_LIMIT_MB`), and notify by email when that happened (see :setting:`MEMUSAGE_NOTIFY_MAIL`)."
msgstr ""

#: ../../topics/settings.rst:995
#: ../../topics/settings.rst:1009
#: ../../topics/settings.rst:1029
#: ../../topics/settings.rst:1046
msgid "See :ref:`topics-extensions-ref-memusage`."
msgstr ""

#: ../../topics/settings.rst:1000
msgid "MEMUSAGE_LIMIT_MB"
msgstr ""

#: ../../topics/settings.rst:1006
msgid "The maximum amount of memory to allow (in megabytes) before shutting down Scrapy  (if MEMUSAGE_ENABLED is True). If zero, no check will be performed."
msgstr ""

#: ../../topics/settings.rst:1014
msgid "MEMUSAGE_CHECK_INTERVAL_SECONDS"
msgstr ""

#: ../../topics/settings.rst:1022
msgid "The :ref:`Memory usage extension <topics-extensions-ref-memusage>` checks the current memory usage, versus the limits set by :setting:`MEMUSAGE_LIMIT_MB` and :setting:`MEMUSAGE_WARNING_MB`, at fixed time intervals."
msgstr ""

#: ../../topics/settings.rst:1027
msgid "This sets the length of these intervals, in seconds."
msgstr ""

#: ../../topics/settings.rst:1034
msgid "MEMUSAGE_NOTIFY_MAIL"
msgstr ""

#: ../../topics/settings.rst:1040
msgid "A list of emails to notify if the memory limit has been reached."
msgstr ""

#: ../../topics/settings.rst:1051
msgid "MEMUSAGE_WARNING_MB"
msgstr ""

#: ../../topics/settings.rst:1057
msgid "The maximum amount of memory to allow (in megabytes) before sending a warning email notifying about it. If zero, no warning will be produced."
msgstr ""

#: ../../topics/settings.rst:1063
msgid "NEWSPIDER_MODULE"
msgstr ""

#: ../../topics/settings.rst:1065
msgid "Default: ``''``"
msgstr ""

#: ../../topics/settings.rst:1067
msgid "Module where to create new spiders using the :command:`genspider` command."
msgstr ""

#: ../../topics/settings.rst:1076
msgid "RANDOMIZE_DOWNLOAD_DELAY"
msgstr ""

#: ../../topics/settings.rst:1080
msgid "If enabled, Scrapy will wait a random amount of time (between 0.5 * :setting:`DOWNLOAD_DELAY` and 1.5 * :setting:`DOWNLOAD_DELAY`) while fetching requests from the same website."
msgstr ""

#: ../../topics/settings.rst:1083
msgid "This randomization decreases the chance of the crawler being detected (and subsequently blocked) by sites which analyze requests looking for statistically significant similarities in the time between their requests."
msgstr ""

#: ../../topics/settings.rst:1087
msgid "The randomization policy is the same used by `wget`_ ``--random-wait`` option."
msgstr ""

#: ../../topics/settings.rst:1089
msgid "If :setting:`DOWNLOAD_DELAY` is zero (default) this option has no effect."
msgstr ""

#: ../../topics/settings.rst:1096
msgid "REACTOR_THREADPOOL_MAXSIZE"
msgstr ""

#: ../../topics/settings.rst:1098
msgid "Default: ``10``"
msgstr ""

#: ../../topics/settings.rst:1100
msgid "The maximum limit for Twisted Reactor thread pool size. This is common multi-purpose thread pool used by various Scrapy components. Threaded DNS Resolver, BlockingFeedStorage, S3FilesStore just to name a few. Increase this value if you're experiencing problems with insufficient blocking IO."
msgstr ""

#: ../../topics/settings.rst:1108
msgid "REDIRECT_PRIORITY_ADJUST"
msgstr ""

#: ../../topics/settings.rst:1110
msgid "Default: ``+2``"
msgstr ""

#: ../../topics/settings.rst:1112
msgid "Scope: ``scrapy.downloadermiddlewares.redirect.RedirectMiddleware``"
msgstr ""

#: ../../topics/settings.rst:1114
msgid "Adjust redirect request priority relative to original request:"
msgstr ""

#: ../../topics/settings.rst:1116
msgid "**a positive priority adjust (default) means higher priority.**"
msgstr ""

#: ../../topics/settings.rst:1117
msgid "a negative priority adjust means lower priority."
msgstr ""

#: ../../topics/settings.rst:1122
msgid "RETRY_PRIORITY_ADJUST"
msgstr ""

#: ../../topics/settings.rst:1124
msgid "Default: ``-1``"
msgstr ""

#: ../../topics/settings.rst:1126
msgid "Scope: ``scrapy.downloadermiddlewares.retry.RetryMiddleware``"
msgstr ""

#: ../../topics/settings.rst:1128
msgid "Adjust retry request priority relative to original request:"
msgstr ""

#: ../../topics/settings.rst:1130
msgid "a positive priority adjust means higher priority."
msgstr ""

#: ../../topics/settings.rst:1131
msgid "**a negative priority adjust (default) means lower priority.**"
msgstr ""

#: ../../topics/settings.rst:1136
msgid "ROBOTSTXT_OBEY"
msgstr ""

#: ../../topics/settings.rst:1140
msgid "Scope: ``scrapy.downloadermiddlewares.robotstxt``"
msgstr ""

#: ../../topics/settings.rst:1142
msgid "If enabled, Scrapy will respect robots.txt policies. For more information see :ref:`topics-dlmw-robots`."
msgstr ""

#: ../../topics/settings.rst:1147
msgid "While the default value is ``False`` for historical reasons, this option is enabled by default in settings.py file generated by ``scrapy startproject`` command."
msgstr ""

#: ../../topics/settings.rst:1154
msgid "ROBOTSTXT_PARSER"
msgstr ""

#: ../../topics/settings.rst:1156
msgid "Default: ``'scrapy.robotstxt.ProtegoRobotParser'``"
msgstr ""

#: ../../topics/settings.rst:1158
msgid "The parser backend to use for parsing ``robots.txt`` files. For more information see :ref:`topics-dlmw-robots`."
msgstr ""

#: ../../topics/settings.rst:1164
msgid "ROBOTSTXT_USER_AGENT"
msgstr ""

#: ../../topics/settings.rst:1168
msgid "The user agent string to use for matching in the robots.txt file. If ``None``, the User-Agent header you are sending with the request or the :setting:`USER_AGENT` setting (in that order) will be used for determining the user agent to use in the robots.txt file."
msgstr ""

#: ../../topics/settings.rst:1176
msgid "SCHEDULER"
msgstr ""

#: ../../topics/settings.rst:1178
msgid "Default: ``'scrapy.core.scheduler.Scheduler'``"
msgstr ""

#: ../../topics/settings.rst:1180
msgid "The scheduler to use for crawling."
msgstr ""

#: ../../topics/settings.rst:1185
msgid "SCHEDULER_DEBUG"
msgstr ""

#: ../../topics/settings.rst:1189
msgid "Setting to ``True`` will log debug information about the requests scheduler. This currently logs (only once) if the requests cannot be serialized to disk. Stats counter (``scheduler/unserializable``) tracks the number of times this happens."
msgstr ""

#: ../../topics/settings.rst:1193
msgid "Example entry in logs::"
msgstr ""

#: ../../topics/settings.rst:1204
msgid "SCHEDULER_DISK_QUEUE"
msgstr ""

#: ../../topics/settings.rst:1206
msgid "Default: ``'scrapy.squeues.PickleLifoDiskQueue'``"
msgstr ""

#: ../../topics/settings.rst:1208
msgid "Type of disk queue that will be used by scheduler. Other available types are ``scrapy.squeues.PickleFifoDiskQueue``, ``scrapy.squeues.MarshalFifoDiskQueue``, ``scrapy.squeues.MarshalLifoDiskQueue``."
msgstr ""

#: ../../topics/settings.rst:1215
msgid "SCHEDULER_MEMORY_QUEUE"
msgstr ""

#: ../../topics/settings.rst:1216
msgid "Default: ``'scrapy.squeues.LifoMemoryQueue'``"
msgstr ""

#: ../../topics/settings.rst:1218
msgid "Type of in-memory queue used by scheduler. Other available type is: ``scrapy.squeues.FifoMemoryQueue``."
msgstr ""

#: ../../topics/settings.rst:1224
msgid "SCHEDULER_PRIORITY_QUEUE"
msgstr ""

#: ../../topics/settings.rst:1225
msgid "Default: ``'scrapy.pqueues.ScrapyPriorityQueue'``"
msgstr ""

#: ../../topics/settings.rst:1227
msgid "Type of priority queue used by the scheduler. Another available type is ``scrapy.pqueues.DownloaderAwarePriorityQueue``. ``scrapy.pqueues.DownloaderAwarePriorityQueue`` works better than ``scrapy.pqueues.ScrapyPriorityQueue`` when you crawl many different domains in parallel. But currently ``scrapy.pqueues.DownloaderAwarePriorityQueue`` does not work together with :setting:`CONCURRENT_REQUESTS_PER_IP`."
msgstr ""

#: ../../topics/settings.rst:1237
msgid "SCRAPER_SLOT_MAX_ACTIVE_SIZE"
msgstr ""

#: ../../topics/settings.rst:1241
msgid "Default: ``5_000_000``"
msgstr ""

#: ../../topics/settings.rst:1243
msgid "Soft limit (in bytes) for response data being processed."
msgstr ""

#: ../../topics/settings.rst:1245
msgid "While the sum of the sizes of all responses being processed is above this value, Scrapy does not process new requests."
msgstr ""

#: ../../topics/settings.rst:1251
msgid "SPIDER_CONTRACTS"
msgstr ""

#: ../../topics/settings.rst:1255
msgid "A dict containing the spider contracts enabled in your project, used for testing spiders. For more info see :ref:`topics-contracts`."
msgstr ""

#: ../../topics/settings.rst:1261
msgid "SPIDER_CONTRACTS_BASE"
msgstr ""

#: ../../topics/settings.rst:1271
msgid "A dict containing the Scrapy contracts enabled by default in Scrapy. You should never modify this setting in your project, modify :setting:`SPIDER_CONTRACTS` instead. For more info see :ref:`topics-contracts`."
msgstr ""

#: ../../topics/settings.rst:1275
msgid "You can disable any of these contracts by assigning ``None`` to their class path in :setting:`SPIDER_CONTRACTS`. E.g., to disable the built-in ``ScrapesContract``, place this in your ``settings.py``::"
msgstr ""

#: ../../topics/settings.rst:1286
msgid "SPIDER_LOADER_CLASS"
msgstr ""

#: ../../topics/settings.rst:1288
msgid "Default: ``'scrapy.spiderloader.SpiderLoader'``"
msgstr ""

#: ../../topics/settings.rst:1290
msgid "The class that will be used for loading spiders, which must implement the :ref:`topics-api-spiderloader`."
msgstr ""

#: ../../topics/settings.rst:1296
msgid "SPIDER_LOADER_WARN_ONLY"
msgstr ""

#: ../../topics/settings.rst:1302
msgid "By default, when Scrapy tries to import spider classes from :setting:`SPIDER_MODULES`, it will fail loudly if there is any ``ImportError`` exception. But you can choose to silence this exception and turn it into a simple warning by setting ``SPIDER_LOADER_WARN_ONLY = True``."
msgstr ""

#: ../../topics/settings.rst:1308
msgid "Some :ref:`scrapy commands <topics-commands>` run with this setting to ``True`` already (i.e. they will only issue a warning and will not fail) since they do not actually need to load spider classes to work: :command:`scrapy runspider <runspider>`, :command:`scrapy settings <settings>`, :command:`scrapy startproject <startproject>`, :command:`scrapy version <version>`."
msgstr ""

#: ../../topics/settings.rst:1319
msgid "SPIDER_MIDDLEWARES"
msgstr ""

#: ../../topics/settings.rst:1323
msgid "A dict containing the spider middlewares enabled in your project, and their orders. For more info see :ref:`topics-spider-middleware-setting`."
msgstr ""

#: ../../topics/settings.rst:1329
msgid "SPIDER_MIDDLEWARES_BASE"
msgstr ""

#: ../../topics/settings.rst:1341
msgid "A dict containing the spider middlewares enabled by default in Scrapy, and their orders. Low orders are closer to the engine, high orders are closer to the spider. For more info see :ref:`topics-spider-middleware-setting`."
msgstr ""

#: ../../topics/settings.rst:1348
msgid "SPIDER_MODULES"
msgstr ""

#: ../../topics/settings.rst:1352
msgid "A list of modules where Scrapy will look for spiders."
msgstr ""

#: ../../topics/settings.rst:1361
msgid "STATS_CLASS"
msgstr ""

#: ../../topics/settings.rst:1363
msgid "Default: ``'scrapy.statscollectors.MemoryStatsCollector'``"
msgstr ""

#: ../../topics/settings.rst:1365
msgid "The class to use for collecting stats, who must implement the :ref:`topics-api-stats`."
msgstr ""

#: ../../topics/settings.rst:1371
msgid "STATS_DUMP"
msgstr ""

#: ../../topics/settings.rst:1375
msgid "Dump the :ref:`Scrapy stats <topics-stats>` (to the Scrapy log) once the spider finishes."
msgstr ""

#: ../../topics/settings.rst:1378
msgid "For more info see: :ref:`topics-stats`."
msgstr ""

#: ../../topics/settings.rst:1383
msgid "STATSMAILER_RCPTS"
msgstr ""

#: ../../topics/settings.rst:1385
msgid "Default: ``[]`` (empty list)"
msgstr ""

#: ../../topics/settings.rst:1387
msgid "Send Scrapy stats after spiders finish scraping. See :class:`~scrapy.extensions.statsmailer.StatsMailer` for more info."
msgstr ""

#: ../../topics/settings.rst:1393
msgid "TELNETCONSOLE_ENABLED"
msgstr ""

#: ../../topics/settings.rst:1397
msgid "A boolean which specifies if the :ref:`telnet console <topics-telnetconsole>` will be enabled (provided its extension is also enabled)."
msgstr ""

#: ../../topics/settings.rst:1403
msgid "TEMPLATES_DIR"
msgstr ""

#: ../../topics/settings.rst:1405
msgid "Default: ``templates`` dir inside scrapy module"
msgstr ""

#: ../../topics/settings.rst:1407
msgid "The directory where to look for templates when creating new projects with :command:`startproject` command and new spiders with :command:`genspider` command."
msgstr ""

#: ../../topics/settings.rst:1411
msgid "The project name must not conflict with the name of custom files or directories in the ``project`` subdirectory."
msgstr ""

#: ../../topics/settings.rst:1417
msgid "TWISTED_REACTOR"
msgstr ""

#: ../../topics/settings.rst:1423
msgid "Import path of a given :mod:`~twisted.internet.reactor`."
msgstr ""

#: ../../topics/settings.rst:1425
msgid "Scrapy will install this reactor if no other reactor is installed yet, such as when the ``scrapy`` CLI program is invoked or when using the :class:`~scrapy.crawler.CrawlerProcess` class."
msgstr ""

#: ../../topics/settings.rst:1429
msgid "If you are using the :class:`~scrapy.crawler.CrawlerRunner` class, you also need to install the correct reactor manually. You can do that using :func:`~scrapy.utils.reactor.install_reactor`:"
msgstr ""

#: ../../../scrapy/utils/reactor.py:docstring of scrapy.utils.reactor.install_reactor:1
msgid "Installs the :mod:`~twisted.internet.reactor` with the specified import path."
msgstr ""

#: ../../topics/settings.rst:1435
msgid "If a reactor is already installed, :func:`~scrapy.utils.reactor.install_reactor` has no effect."
msgstr ""

#: ../../topics/settings.rst:1438
msgid ":meth:`CrawlerRunner.__init__ <scrapy.crawler.CrawlerRunner.__init__>` raises :exc:`Exception` if the installed reactor does not match the :setting:`TWISTED_REACTOR` setting; therfore, having top-level :mod:`~twisted.internet.reactor` imports in project files and imported third-party libraries will make Scrapy raise :exc:`Exception` when it checks which reactor is installed."
msgstr ""

#: ../../topics/settings.rst:1445
msgid "In order to use the reactor installed by Scrapy::"
msgstr ""

#: ../../topics/settings.rst:1473
msgid "which raises :exc:`Exception`, becomes::"
msgstr ""

#: ../../topics/settings.rst:1501
msgid "The default value of the :setting:`TWISTED_REACTOR` setting is ``None``, which means that Scrapy will not attempt to install any specific reactor, and the default reactor defined by Twisted for the current platform will be used. This is to maintain backward compatibility and avoid possible problems caused by using a non-default reactor."
msgstr ""

#: ../../topics/settings.rst:1507
msgid "For additional information, see :doc:`core/howto/choosing-reactor`."
msgstr ""

#: ../../topics/settings.rst:1513
msgid "URLLENGTH_LIMIT"
msgstr ""

#: ../../topics/settings.rst:1515
msgid "Default: ``2083``"
msgstr ""

#: ../../topics/settings.rst:1517
msgid "Scope: ``spidermiddlewares.urllength``"
msgstr ""

#: ../../topics/settings.rst:1519
msgid "The maximum URL length to allow for crawled URLs. For more information about the default value for this setting see: https://boutell.com/newfaq/misc/urllength.html"
msgstr ""

#: ../../topics/settings.rst:1525
msgid "USER_AGENT"
msgstr ""

#: ../../topics/settings.rst:1527
msgid "Default: ``\"Scrapy/VERSION (+https://scrapy.org)\"``"
msgstr ""

#: ../../topics/settings.rst:1529
msgid "The default User-Agent to use when crawling, unless overridden. This user agent is also used by :class:`~scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware` if :setting:`ROBOTSTXT_USER_AGENT` setting is ``None`` and there is no overridding User-Agent header specified for the request."
msgstr ""

#: ../../topics/settings.rst:1536
msgid "Settings documented elsewhere:"
msgstr ""

#: ../../topics/settings.rst:1538
msgid "The following settings are documented elsewhere, please check each specific case to see how to enable and use them."
msgstr ""

#: ../../topics/shell.rst:5
msgid "Scrapy shell"
msgstr ""

#: ../../topics/shell.rst:7
msgid "The Scrapy shell is an interactive shell where you can try and debug your scraping code very quickly, without having to run the spider. It's meant to be used for testing data extraction code, but you can actually use it for testing any kind of code as it is also a regular Python shell."
msgstr ""

#: ../../topics/shell.rst:12
msgid "The shell is used for testing XPath or CSS expressions and see how they work and what data they extract from the web pages you're trying to scrape. It allows you to interactively test your expressions while you're writing your spider, without having to run the spider to test every change."
msgstr ""

#: ../../topics/shell.rst:17
msgid "Once you get familiarized with the Scrapy shell, you'll see that it's an invaluable tool for developing and debugging your spiders."
msgstr ""

#: ../../topics/shell.rst:21
msgid "Configuring the shell"
msgstr ""

#: ../../topics/shell.rst:23
msgid "If you have `IPython`_ installed, the Scrapy shell will use it (instead of the standard Python console). The `IPython`_ console is much more powerful and provides smart auto-completion and colorized output, among other things."
msgstr ""

#: ../../topics/shell.rst:27
msgid "We highly recommend you install `IPython`_, specially if you're working on Unix systems (where `IPython`_ excels). See the `IPython installation guide`_ for more info."
msgstr ""

#: ../../topics/shell.rst:31
msgid "Scrapy also has support for `bpython`_, and will try to use it where `IPython`_ is unavailable."
msgstr ""

#: ../../topics/shell.rst:34
msgid "Through Scrapy's settings you can configure it to use any one of ``ipython``, ``bpython`` or the standard ``python`` shell, regardless of which are installed. This is done by setting the ``SCRAPY_PYTHON_SHELL`` environment variable; or by defining it in your :ref:`scrapy.cfg <topics-config-settings>`::"
msgstr ""

#: ../../topics/shell.rst:47
msgid "Launch the shell"
msgstr ""

#: ../../topics/shell.rst:49
msgid "To launch the Scrapy shell you can use the :command:`shell` command like this::"
msgstr ""

#: ../../topics/shell.rst:54
msgid "Where the ``<url>`` is the URL you want to scrape."
msgstr ""

#: ../../topics/shell.rst:56
msgid ":command:`shell` also works for local files. This can be handy if you want to play around with a local copy of a web page. :command:`shell` understands the following syntaxes for local files::"
msgstr ""

#: ../../topics/shell.rst:68
msgid "When using relative file paths, be explicit and prepend them with ``./`` (or ``../`` when relevant). ``scrapy shell index.html`` will not work as one might expect (and this is by design, not a bug)."
msgstr ""

#: ../../topics/shell.rst:73
msgid "Because :command:`shell` favors HTTP URLs over File URIs, and ``index.html`` being syntactically similar to ``example.com``, :command:`shell` will treat ``index.html`` as a domain name and trigger a DNS lookup error::"
msgstr ""

#: ../../topics/shell.rst:84
msgid ":command:`shell` will not test beforehand if a file called ``index.html`` exists in the current directory. Again, be explicit."
msgstr ""

#: ../../topics/shell.rst:89
msgid "Using the shell"
msgstr ""

#: ../../topics/shell.rst:91
msgid "The Scrapy shell is just a regular Python console (or `IPython`_ console if you have it available) which provides some additional shortcut functions for convenience."
msgstr ""

#: ../../topics/shell.rst:96
msgid "Available Shortcuts"
msgstr ""

#: ../../topics/shell.rst:98
msgid "``shelp()`` - print a help with the list of available objects and shortcuts"
msgstr ""

#: ../../topics/shell.rst:100
msgid "``fetch(url[, redirect=True])`` - fetch a new response from the given URL and update all related objects accordingly. You can optionaly ask for HTTP 3xx redirections to not be followed by passing ``redirect=False``"
msgstr ""

#: ../../topics/shell.rst:104
msgid "``fetch(request)`` - fetch a new response from the given request and update all related objects accordingly."
msgstr ""

#: ../../topics/shell.rst:107
msgid "``view(response)`` - open the given response in your local web browser, for inspection. This will add a `\\<base\\> tag`_ to the response body in order for external links (such as images and style sheets) to display properly. Note, however, that this will create a temporary file in your computer, which won't be removed automatically."
msgstr ""

#: ../../topics/shell.rst:116
msgid "Available Scrapy objects"
msgstr ""

#: ../../topics/shell.rst:118
msgid "The Scrapy shell automatically creates some convenient objects from the downloaded page, like the :class:`~scrapy.http.Response` object and the :class:`~scrapy.selector.Selector` objects (for both HTML and XML content)."
msgstr ""

#: ../../topics/shell.rst:123
msgid "Those objects are:"
msgstr ""

#: ../../topics/shell.rst:125
msgid "``crawler`` - the current :class:`~scrapy.crawler.Crawler` object."
msgstr ""

#: ../../topics/shell.rst:127
msgid "``spider`` - the Spider which is known to handle the URL, or a :class:`~scrapy.spiders.Spider` object if there is no spider found for the current URL"
msgstr ""

#: ../../topics/shell.rst:131
msgid "``request`` - a :class:`~scrapy.http.Request` object of the last fetched page. You can modify this request using :meth:`~scrapy.http.Request.replace` or fetch a new request (without leaving the shell) using the ``fetch`` shortcut."
msgstr ""

#: ../../topics/shell.rst:136
msgid "``response`` - a :class:`~scrapy.http.Response` object containing the last fetched page"
msgstr ""

#: ../../topics/shell.rst:139
msgid "``settings`` - the current :ref:`Scrapy settings <topics-settings>`"
msgstr ""

#: ../../topics/shell.rst:142
msgid "Example of shell session"
msgstr ""

#: ../../topics/shell.rst:144
msgid "Here's an example of a typical shell session where we start by scraping the https://scrapy.org page, and then proceed to scrape the https://old.reddit.com/ page. Finally, we modify the (Reddit) request method to POST and re-fetch it getting an error. We end the session by typing Ctrl-D (in Unix systems) or Ctrl-Z in Windows."
msgstr ""

#: ../../topics/shell.rst:150
msgid "Keep in mind that the data extracted here may not be the same when you try it, as those pages are not static and could have changed by the time you test this. The only purpose of this example is to get you familiarized with how the Scrapy shell works."
msgstr ""

#: ../../topics/shell.rst:155
msgid "First, we launch the shell::"
msgstr ""

#: ../../topics/shell.rst:161
msgid "Remember to always enclose URLs in quotes when running the Scrapy shell from the command line, otherwise URLs containing arguments (i.e. the ``&`` character) will not work."
msgstr ""

#: ../../topics/shell.rst:165
msgid "On Windows, use double quotes instead::"
msgstr ""

#: ../../topics/shell.rst:170
msgid "Then, the shell fetches the URL (using the Scrapy downloader) and prints the list of available objects and useful shortcuts (you'll notice that these lines all start with the ``[s]`` prefix)::"
msgstr ""

#: ../../topics/shell.rst:191
msgid "After that, we can start playing with the objects:"
msgstr ""

#: ../../topics/shell.rst:236
msgid "Invoking the shell from spiders to inspect responses"
msgstr ""

#: ../../topics/shell.rst:238
msgid "Sometimes you want to inspect the responses that are being processed in a certain point of your spider, if only to check that response you expect is getting there."
msgstr ""

#: ../../topics/shell.rst:242
msgid "This can be achieved by using the ``scrapy.shell.inspect_response`` function."
msgstr ""

#: ../../topics/shell.rst:244
msgid "Here's an example of how you would call it from your spider::"
msgstr ""

#: ../../topics/shell.rst:265
msgid "When you run the spider, you will get something similar to this::"
msgstr ""

#: ../../topics/shell.rst:276
msgid "Then, you can check if the extraction code is working:"
msgstr ""

#: ../../topics/shell.rst:281
msgid "Nope, it doesn't. So you can open the response in your web browser and see if it's the response you were expecting:"
msgstr ""

#: ../../topics/shell.rst:287
msgid "Finally you hit Ctrl-D (or Ctrl-Z in Windows) to exit the shell and resume the crawling::"
msgstr ""

#: ../../topics/shell.rst:294
msgid "Note that you can't use the ``fetch`` shortcut here since the Scrapy engine is blocked by the shell. However, after you leave the shell, the spider will continue crawling where it stopped, as shown above."
msgstr ""

#: ../../topics/signals.rst:5
msgid "Signals"
msgstr ""

#: ../../topics/signals.rst:7
msgid "Scrapy uses signals extensively to notify when certain events occur. You can catch some of those signals in your Scrapy project (using an :ref:`extension <topics-extensions>`, for example) to perform additional tasks or extend Scrapy to add functionality not provided out of the box."
msgstr ""

#: ../../topics/signals.rst:12
msgid "Even though signals provide several arguments, the handlers that catch them don't need to accept all of them - the signal dispatching mechanism will only deliver the arguments that the handler receives."
msgstr ""

#: ../../topics/signals.rst:16
msgid "You can connect to signals (or send your own) through the :ref:`topics-api-signals`."
msgstr ""

#: ../../topics/signals.rst:19
msgid "Here is a simple example showing how you can catch signals and perform some action::"
msgstr ""

#: ../../topics/signals.rst:51
msgid "Deferred signal handlers"
msgstr ""

#: ../../topics/signals.rst:53
msgid "Some signals support returning :class:`~twisted.internet.defer.Deferred` objects from their handlers, allowing you to run asynchronous code that does not block Scrapy. If a signal handler returns a :class:`~twisted.internet.defer.Deferred`, Scrapy waits for that :class:`~twisted.internet.defer.Deferred` to fire."
msgstr ""

#: ../../topics/signals.rst:59
msgid "Let's take an example::"
msgstr ""

#: ../../topics/signals.rst:91
msgid "See the :ref:`topics-signals-ref` below to know which signals support :class:`~twisted.internet.defer.Deferred`."
msgstr ""

#: ../../topics/signals.rst:97
msgid "Built-in signals reference"
msgstr ""

#: ../../topics/signals.rst:102
msgid "Here's the list of Scrapy built-in signals and their meaning."
msgstr ""

#: ../../topics/signals.rst:105
msgid "Engine signals"
msgstr ""

#: ../../topics/signals.rst:108
msgid "engine_started"
msgstr ""

#: ../../topics/signals.rst:113
msgid "Sent when the Scrapy engine has started crawling."
msgstr ""

#: ../../topics/signals.rst:115
#: ../../topics/signals.rst:130
#: ../../topics/signals.rst:152
#: ../../topics/signals.rst:172
#: ../../topics/signals.rst:197
#: ../../topics/signals.rst:223
#: ../../topics/signals.rst:247
msgid "This signal supports returning deferreds from their handlers."
msgstr ""

#: ../../topics/signals.rst:117
msgid "This signal may be fired *after* the :signal:`spider_opened` signal, depending on how the spider was started. So **don't** rely on this signal getting fired before :signal:`spider_opened`."
msgstr ""

#: ../../topics/signals.rst:122
msgid "engine_stopped"
msgstr ""

#: ../../topics/signals.rst:127
msgid "Sent when the Scrapy engine is stopped (for example, when a crawling process has finished)."
msgstr ""

#: ../../topics/signals.rst:133
msgid "Item signals"
msgstr ""

#: ../../topics/signals.rst:136
msgid "As at max :setting:`CONCURRENT_ITEMS` items are processed in parallel, many deferreds are fired together using :class:`~twisted.internet.defer.DeferredList`. Hence the next batch waits for the :class:`~twisted.internet.defer.DeferredList` to fire and then runs the respective item signal handler for the next batch of scraped items."
msgstr ""

#: ../../topics/signals.rst:144
msgid "item_scraped"
msgstr ""

#: ../../topics/signals.rst:149
msgid "Sent when an item has been scraped, after it has passed all the :ref:`topics-item-pipeline` stages (without being dropped)."
msgstr ""

#: ../../topics/signals.rst:160
msgid "the response from where the item was scraped"
msgstr ""

#: ../../topics/signals.rst:164
msgid "item_dropped"
msgstr ""

#: ../../topics/signals.rst:169
msgid "Sent after an item has been dropped from the :ref:`topics-item-pipeline` when some stage raised a :exc:`~scrapy.exceptions.DropItem` exception."
msgstr ""

#: ../../topics/signals.rst:174
#: ../../topics/signals.rst:199
msgid "the item dropped from the :ref:`topics-item-pipeline`"
msgstr ""

#: ../../topics/signals.rst:180
msgid "the response from where the item was dropped"
msgstr ""

#: ../../topics/signals.rst:183
msgid "the exception (which must be a :exc:`~scrapy.exceptions.DropItem` subclass) which caused the item to be dropped"
msgstr ""

#: ../../topics/signals.rst:189
msgid "item_error"
msgstr ""

#: ../../topics/signals.rst:194
msgid "Sent when a :ref:`topics-item-pipeline` generates an error (i.e. raises an exception), except :exc:`~scrapy.exceptions.DropItem` exception."
msgstr ""

#: ../../topics/signals.rst:202
#: ../../topics/signals.rst:295
#: ../../topics/spider-middleware.rst:138
msgid "the response being processed when the exception was raised"
msgstr ""

#: ../../topics/signals.rst:205
#: ../../topics/signals.rst:298
#: ../../topics/spider-middleware.rst:145
msgid "the spider which raised the exception"
msgstr ""

#: ../../topics/signals.rst:208
#: ../../topics/signals.rst:292
#: ../../topics/spider-middleware.rst:142
msgid "the exception raised"
msgstr ""

#: ../../topics/signals.rst:212
msgid "Spider signals"
msgstr ""

#: ../../topics/signals.rst:215
msgid "spider_closed"
msgstr ""

#: ../../topics/signals.rst:220
msgid "Sent after a spider has been closed. This can be used to release per-spider resources reserved on :signal:`spider_opened`."
msgstr ""

#: ../../topics/signals.rst:228
msgid "a string which describes the reason why the spider was closed. If it was closed because the spider has completed scraping, the reason is ``'finished'``. Otherwise, if the spider was manually closed by calling the ``close_spider`` engine method, then the reason is the one passed in the ``reason`` argument of that method (which defaults to ``'cancelled'``). If the engine was shutdown (for example, by hitting Ctrl-C to stop it) the reason will be ``'shutdown'``."
msgstr ""

#: ../../topics/signals.rst:238
msgid "spider_opened"
msgstr ""

#: ../../topics/signals.rst:243
msgid "Sent after a spider has been opened for crawling. This is typically used to reserve per-spider resources, but can be used for any task that needs to be performed when a spider is opened."
msgstr ""

#: ../../topics/signals.rst:253
msgid "spider_idle"
msgstr ""

#: ../../topics/signals.rst:258
msgid "Sent when a spider has gone idle, which means the spider has no further:"
msgstr ""

#: ../../topics/signals.rst:260
msgid "requests waiting to be downloaded"
msgstr ""

#: ../../topics/signals.rst:261
msgid "requests scheduled"
msgstr ""

#: ../../topics/signals.rst:262
msgid "items being processed in the item pipeline"
msgstr ""

#: ../../topics/signals.rst:264
msgid "If the idle state persists after all handlers of this signal have finished, the engine starts closing the spider. After the spider has finished closing, the :signal:`spider_closed` signal is sent."
msgstr ""

#: ../../topics/signals.rst:268
msgid "You may raise a :exc:`~scrapy.exceptions.DontCloseSpider` exception to prevent the spider from being closed."
msgstr ""

#: ../../topics/signals.rst:271
#: ../../topics/signals.rst:290
#: ../../topics/signals.rst:385
#: ../../topics/signals.rst:404
msgid "This signal does not support returning deferreds from their handlers."
msgstr ""

#: ../../topics/signals.rst:273
msgid "the spider which has gone idle"
msgstr ""

#: ../../topics/signals.rst:276
msgid "Scheduling some requests in your :signal:`spider_idle` handler does **not** guarantee that it can prevent the spider from being closed, although it sometimes can. That's because the spider may still remain idle if all the scheduled requests are rejected by the scheduler (e.g. filtered due to duplication)."
msgstr ""

#: ../../topics/signals.rst:283
msgid "spider_error"
msgstr ""

#: ../../topics/signals.rst:288
msgid "Sent when a spider callback generates an error (i.e. raises an exception)."
msgstr ""

#: ../../topics/signals.rst:302
msgid "Request signals"
msgstr ""

#: ../../topics/signals.rst:305
msgid "request_scheduled"
msgstr ""

#: ../../topics/signals.rst:310
msgid "Sent when the engine schedules a :class:`~scrapy.http.Request`, to be downloaded later."
msgstr ""

#: ../../topics/signals.rst:313
#: ../../topics/signals.rst:330
#: ../../topics/signals.rst:346
msgid "The signal does not support returning deferreds from their handlers."
msgstr ""

#: ../../topics/signals.rst:315
#: ../../topics/signals.rst:332
msgid "the request that reached the scheduler"
msgstr ""

#: ../../topics/signals.rst:318
#: ../../topics/signals.rst:335
#: ../../topics/signals.rst:351
#: ../../topics/signals.rst:370
msgid "the spider that yielded the request"
msgstr ""

#: ../../topics/signals.rst:322
msgid "request_dropped"
msgstr ""

#: ../../topics/signals.rst:327
msgid "Sent when a :class:`~scrapy.http.Request`, scheduled by the engine to be downloaded later, is rejected by the scheduler."
msgstr ""

#: ../../topics/signals.rst:339
msgid "request_reached_downloader"
msgstr ""

#: ../../topics/signals.rst:344
msgid "Sent when a :class:`~scrapy.http.Request` reached downloader."
msgstr ""

#: ../../topics/signals.rst:348
msgid "the request that reached downloader"
msgstr ""

#: ../../topics/signals.rst:355
msgid "request_left_downloader"
msgstr ""

#: ../../topics/signals.rst:362
msgid "Sent when a :class:`~scrapy.http.Request` leaves the downloader, even in case of failure."
msgstr ""

#: ../../topics/signals.rst:365
msgid "This signal does not support returning deferreds from its handlers."
msgstr ""

#: ../../topics/signals.rst:367
msgid "the request that reached the downloader"
msgstr ""

#: ../../topics/signals.rst:374
msgid "Response signals"
msgstr ""

#: ../../topics/signals.rst:377
msgid "response_received"
msgstr ""

#: ../../topics/signals.rst:382
msgid "Sent when the engine receives a new :class:`~scrapy.http.Response` from the downloader."
msgstr ""

#: ../../topics/signals.rst:387
msgid "the response received"
msgstr ""

#: ../../topics/signals.rst:390
#: ../../topics/signals.rst:409
msgid "the request that generated the response"
msgstr ""

#: ../../topics/signals.rst:397
msgid "response_downloaded"
msgstr ""

#: ../../topics/signals.rst:402
msgid "Sent by the downloader right after a ``HTTPResponse`` is downloaded."
msgstr ""

#: ../../topics/signals.rst:406
msgid "the response downloaded"
msgstr ""

#: ../../topics/spider-middleware.rst:5
msgid "Spider Middleware"
msgstr ""

#: ../../topics/spider-middleware.rst:7
msgid "The spider middleware is a framework of hooks into Scrapy's spider processing mechanism where you can plug custom functionality to process the responses that are sent to :ref:`topics-spiders` for processing and to process the requests and items that are generated from spiders."
msgstr ""

#: ../../topics/spider-middleware.rst:15
msgid "Activating a spider middleware"
msgstr ""

#: ../../topics/spider-middleware.rst:17
msgid "To activate a spider middleware component, add it to the :setting:`SPIDER_MIDDLEWARES` setting, which is a dict whose keys are the middleware class path and their values are the middleware orders."
msgstr ""

#: ../../topics/spider-middleware.rst:27
msgid "The :setting:`SPIDER_MIDDLEWARES` setting is merged with the :setting:`SPIDER_MIDDLEWARES_BASE` setting defined in Scrapy (and not meant to be overridden) and then sorted by order to get the final sorted list of enabled middlewares: the first middleware is the one closer to the engine and the last is the one closer to the spider. In other words, the :meth:`~scrapy.spidermiddlewares.SpiderMiddleware.process_spider_input` method of each middleware will be invoked in increasing middleware order (100, 200, 300, ...), and the :meth:`~scrapy.spidermiddlewares.SpiderMiddleware.process_spider_output` method of each middleware will be invoked in decreasing order."
msgstr ""

#: ../../topics/spider-middleware.rst:38
msgid "To decide which order to assign to your middleware see the :setting:`SPIDER_MIDDLEWARES_BASE` setting and pick a value according to where you want to insert the middleware. The order does matter because each middleware performs a different action and your middleware could depend on some previous (or subsequent) middleware being applied."
msgstr ""

#: ../../topics/spider-middleware.rst:44
msgid "If you want to disable a builtin middleware (the ones defined in :setting:`SPIDER_MIDDLEWARES_BASE`, and enabled by default) you must define it in your project :setting:`SPIDER_MIDDLEWARES` setting and assign ``None`` as its value.  For example, if you want to disable the off-site middleware::"
msgstr ""

#: ../../topics/spider-middleware.rst:60
msgid "Writing your own spider middleware"
msgstr ""

#: ../../topics/spider-middleware.rst:62
msgid "Each spider middleware is a Python class that defines one or more of the methods defined below."
msgstr ""

#: ../../topics/spider-middleware.rst:75
msgid "This method is called for each response that goes through the spider middleware and into the spider, for processing."
msgstr ""

#: ../../topics/spider-middleware.rst:78
msgid ":meth:`process_spider_input` should return ``None`` or raise an exception."
msgstr ""

#: ../../topics/spider-middleware.rst:81
msgid "If it returns ``None``, Scrapy will continue processing this response, executing all other middlewares until, finally, the response is handed to the spider for processing."
msgstr ""

#: ../../topics/spider-middleware.rst:85
msgid "If it raises an exception, Scrapy won't bother calling any other spider middleware :meth:`process_spider_input` and will call the request errback if there is one, otherwise it will start the :meth:`process_spider_exception` chain. The output of the errback is chained back in the other direction for :meth:`process_spider_output` to process it, or :meth:`process_spider_exception` if it raised an exception."
msgstr ""

#: ../../topics/spider-middleware.rst:101
msgid "This method is called with the results returned from the Spider, after it has processed the response."
msgstr ""

#: ../../topics/spider-middleware.rst:104
msgid ":meth:`process_spider_output` must return an iterable of :class:`~scrapy.http.Request`, dict or :class:`~scrapy.item.Item` objects."
msgstr ""

#: ../../topics/spider-middleware.rst:108
msgid "the response which generated this output from the spider"
msgstr ""

#: ../../topics/spider-middleware.rst:112
msgid "the result returned by the spider"
msgstr ""

#: ../../topics/spider-middleware.rst:116
msgid "the spider whose result is being processed"
msgstr ""

#: ../../topics/spider-middleware.rst:122
msgid "This method is called when a spider or :meth:`process_spider_output` method (from a previous spider middleware) raises an exception."
msgstr ""

#: ../../topics/spider-middleware.rst:125
msgid ":meth:`process_spider_exception` should return either ``None`` or an iterable of :class:`~scrapy.http.Request`, dict or :class:`~scrapy.item.Item` objects."
msgstr ""

#: ../../topics/spider-middleware.rst:129
msgid "If it returns ``None``, Scrapy will continue processing this exception, executing any other :meth:`process_spider_exception` in the following middleware components, until no middleware components are left and the exception reaches the engine (where it's logged and discarded)."
msgstr ""

#: ../../topics/spider-middleware.rst:134
msgid "If it returns an iterable the :meth:`process_spider_output` pipeline kicks in, starting from the next spider middleware, and no other :meth:`process_spider_exception` will be called."
msgstr ""

#: ../../topics/spider-middleware.rst:152
msgid "This method is called with the start requests of the spider, and works similarly to the :meth:`process_spider_output` method, except that it doesn't have a response associated and must return only requests (not items)."
msgstr ""

#: ../../topics/spider-middleware.rst:157
msgid "It receives an iterable (in the ``start_requests`` parameter) and must return another iterable of :class:`~scrapy.http.Request` objects."
msgstr ""

#: ../../topics/spider-middleware.rst:160
msgid "When implementing this method in your spider middleware, you should always return an iterable (that follows the input one) and not consume all ``start_requests`` iterator because it can be very large (or even unbounded) and cause a memory overflow. The Scrapy engine is designed to pull start requests while it has capacity to process them, so the start requests iterator can be effectively endless where there is some other condition for stopping the spider (like a time limit or item/page count)."
msgstr ""

#: ../../topics/spider-middleware.rst:169
msgid "the start requests"
msgstr ""

#: ../../topics/spider-middleware.rst:172
msgid "the spider to whom the start requests belong"
msgstr ""

#: ../../topics/spider-middleware.rst:189
msgid "Built-in spider middleware reference"
msgstr ""

#: ../../topics/spider-middleware.rst:191
msgid "This page describes all spider middleware components that come with Scrapy. For information on how to use them and how to write your own spider middleware, see the :ref:`spider middleware usage guide <topics-spider-middleware>`."
msgstr ""

#: ../../topics/spider-middleware.rst:195
msgid "For a list of the components enabled by default (and their orders) see the :setting:`SPIDER_MIDDLEWARES_BASE` setting."
msgstr ""

#: ../../topics/spider-middleware.rst:199
msgid "DepthMiddleware"
msgstr ""

#: ../../topics/spider-middleware.rst:206
msgid "DepthMiddleware is used for tracking the depth of each Request inside the site being scraped. It works by setting ``request.meta['depth'] = 0`` whenever there is no value previously set (usually just the first Request) and incrementing it by 1 otherwise."
msgstr ""

#: ../../topics/spider-middleware.rst:211
msgid "It can be used to limit the maximum depth to scrape, control Request priority based on their depth, and things like that."
msgstr ""

#: ../../topics/spider-middleware.rst:214
msgid "The :class:`DepthMiddleware` can be configured through the following settings (see the settings documentation for more info):"
msgstr ""

#: ../../topics/spider-middleware.rst:217
msgid ":setting:`DEPTH_LIMIT` - The maximum depth that will be allowed to crawl for any site. If zero, no limit will be imposed."
msgstr ""

#: ../../topics/spider-middleware.rst:219
msgid ":setting:`DEPTH_STATS_VERBOSE` - Whether to collect the number of requests for each depth."
msgstr ""

#: ../../topics/spider-middleware.rst:221
msgid ":setting:`DEPTH_PRIORITY` - Whether to prioritize the requests based on their depth."
msgstr ""

#: ../../topics/spider-middleware.rst:225
msgid "HttpErrorMiddleware"
msgstr ""

#: ../../topics/spider-middleware.rst:232
msgid "Filter out unsuccessful (erroneous) HTTP responses so that spiders don't have to deal with them, which (most of the time) imposes an overhead, consumes more resources, and makes the spider logic more complex."
msgstr ""

#: ../../topics/spider-middleware.rst:236
msgid "According to the `HTTP standard`_, successful responses are those whose status codes are in the 200-300 range."
msgstr ""

#: ../../topics/spider-middleware.rst:241
msgid "If you still want to process response codes outside that range, you can specify which response codes the spider is able to handle using the ``handle_httpstatus_list`` spider attribute or :setting:`HTTPERROR_ALLOWED_CODES` setting."
msgstr ""

#: ../../topics/spider-middleware.rst:246
msgid "For example, if you want your spider to handle 404 responses you can do this::"
msgstr ""

#: ../../topics/spider-middleware.rst:261
msgid "Keep in mind, however, that it's usually a bad idea to handle non-200 responses, unless you really know what you're doing."
msgstr ""

#: ../../topics/spider-middleware.rst:264
msgid "For more information see: `HTTP Status Code Definitions`_."
msgstr ""

#: ../../topics/spider-middleware.rst:269
msgid "HttpErrorMiddleware settings"
msgstr ""

#: ../../topics/spider-middleware.rst:274
msgid "HTTPERROR_ALLOWED_CODES"
msgstr ""

#: ../../topics/spider-middleware.rst:278
msgid "Pass all responses with non-200 status codes contained in this list."
msgstr ""

#: ../../topics/spider-middleware.rst:283
msgid "HTTPERROR_ALLOW_ALL"
msgstr ""

#: ../../topics/spider-middleware.rst:287
msgid "Pass all responses, regardless of its status code."
msgstr ""

#: ../../topics/spider-middleware.rst:290
msgid "OffsiteMiddleware"
msgstr ""

#: ../../topics/spider-middleware.rst:297
msgid "Filters out Requests for URLs outside the domains covered by the spider."
msgstr ""

#: ../../topics/spider-middleware.rst:299
msgid "This middleware filters out every request whose host names aren't in the spider's :attr:`~scrapy.spiders.Spider.allowed_domains` attribute. All subdomains of any domain in the list are also allowed. E.g. the rule ``www.example.org`` will also allow ``bob.www.example.org`` but not ``www2.example.com`` nor ``example.com``."
msgstr ""

#: ../../topics/spider-middleware.rst:305
msgid "When your spider returns a request for a domain not belonging to those covered by the spider, this middleware will log a debug message similar to this one::"
msgstr ""

#: ../../topics/spider-middleware.rst:311
msgid "To avoid filling the log with too much noise, it will only print one of these messages for each new domain filtered. So, for example, if another request for ``www.othersite.com`` is filtered, no log message will be printed. But if a request for ``someothersite.com`` is filtered, a message will be printed (but only for the first request filtered)."
msgstr ""

#: ../../topics/spider-middleware.rst:317
msgid "If the spider doesn't define an :attr:`~scrapy.spiders.Spider.allowed_domains` attribute, or the attribute is empty, the offsite middleware will allow all requests."
msgstr ""

#: ../../topics/spider-middleware.rst:321
msgid "If the request has the :attr:`~scrapy.http.Request.dont_filter` attribute set, the offsite middleware will allow the request even if its domain is not listed in allowed domains."
msgstr ""

#: ../../topics/spider-middleware.rst:327
msgid "RefererMiddleware"
msgstr ""

#: ../../topics/spider-middleware.rst:334
msgid "Populates Request ``Referer`` header, based on the URL of the Response which generated it."
msgstr ""

#: ../../topics/spider-middleware.rst:338
msgid "RefererMiddleware settings"
msgstr ""

#: ../../topics/spider-middleware.rst:343
msgid "REFERER_ENABLED"
msgstr ""

#: ../../topics/spider-middleware.rst:349
msgid "Whether to enable referer middleware."
msgstr ""

#: ../../topics/spider-middleware.rst:354
msgid "REFERRER_POLICY"
msgstr ""

#: ../../topics/spider-middleware.rst:358
msgid "Default: ``'scrapy.spidermiddlewares.referer.DefaultReferrerPolicy'``"
msgstr ""

#: ../../topics/spider-middleware.rst:362
msgid "`Referrer Policy`_ to apply when populating Request \"Referer\" header."
msgstr ""

#: ../../topics/spider-middleware.rst:365
msgid "You can also set the Referrer Policy per request, using the special ``\"referrer_policy\"`` :ref:`Request.meta <topics-request-meta>` key, with the same acceptable values as for the ``REFERRER_POLICY`` setting."
msgstr ""

#: ../../topics/spider-middleware.rst:370
msgid "Acceptable values for REFERRER_POLICY"
msgstr ""

#: ../../topics/spider-middleware.rst:372
msgid "either a path to a ``scrapy.spidermiddlewares.referer.ReferrerPolicy`` subclass â€” a custom policy or one of the built-in ones (see classes below),"
msgstr ""

#: ../../topics/spider-middleware.rst:374
msgid "or one of the standard W3C-defined string values,"
msgstr ""

#: ../../topics/spider-middleware.rst:375
msgid "or the special ``\"scrapy-default\"``."
msgstr ""

#: ../../topics/spider-middleware.rst:378
msgid "String value"
msgstr ""

#: ../../topics/spider-middleware.rst:378
msgid "Class name (as a string)"
msgstr ""

#: ../../topics/spider-middleware.rst:380
msgid "``\"scrapy-default\"`` (default)"
msgstr ""

#: ../../topics/spider-middleware.rst:380
msgid ":class:`scrapy.spidermiddlewares.referer.DefaultReferrerPolicy`"
msgstr ""

#: ../../topics/spider-middleware.rst:381
msgid "`\"no-referrer\"`_"
msgstr ""

#: ../../topics/spider-middleware.rst:381
msgid ":class:`scrapy.spidermiddlewares.referer.NoReferrerPolicy`"
msgstr ""

#: ../../topics/spider-middleware.rst:382
msgid "`\"no-referrer-when-downgrade\"`_"
msgstr ""

#: ../../topics/spider-middleware.rst:382
msgid ":class:`scrapy.spidermiddlewares.referer.NoReferrerWhenDowngradePolicy`"
msgstr ""

#: ../../topics/spider-middleware.rst:383
msgid "`\"same-origin\"`_"
msgstr ""

#: ../../topics/spider-middleware.rst:383
msgid ":class:`scrapy.spidermiddlewares.referer.SameOriginPolicy`"
msgstr ""

#: ../../topics/spider-middleware.rst:384
msgid "`\"origin\"`_"
msgstr ""

#: ../../topics/spider-middleware.rst:384
msgid ":class:`scrapy.spidermiddlewares.referer.OriginPolicy`"
msgstr ""

#: ../../topics/spider-middleware.rst:385
msgid "`\"strict-origin\"`_"
msgstr ""

#: ../../topics/spider-middleware.rst:385
msgid ":class:`scrapy.spidermiddlewares.referer.StrictOriginPolicy`"
msgstr ""

#: ../../topics/spider-middleware.rst:386
msgid "`\"origin-when-cross-origin\"`_"
msgstr ""

#: ../../topics/spider-middleware.rst:386
msgid ":class:`scrapy.spidermiddlewares.referer.OriginWhenCrossOriginPolicy`"
msgstr ""

#: ../../topics/spider-middleware.rst:387
msgid "`\"strict-origin-when-cross-origin\"`_"
msgstr ""

#: ../../topics/spider-middleware.rst:387
msgid ":class:`scrapy.spidermiddlewares.referer.StrictOriginWhenCrossOriginPolicy`"
msgstr ""

#: ../../topics/spider-middleware.rst:388
msgid "`\"unsafe-url\"`_"
msgstr ""

#: ../../topics/spider-middleware.rst:388
msgid ":class:`scrapy.spidermiddlewares.referer.UnsafeUrlPolicy`"
msgstr ""

#: ../../../scrapy/spidermiddlewares/referer.py:docstring of scrapy.spidermiddlewares.referer.DefaultReferrerPolicy:1
msgid "A variant of \"no-referrer-when-downgrade\", with the addition that \"Referer\" is not sent if the parent request was using ``file://`` or ``s3://`` scheme."
msgstr ""

#: ../../topics/spider-middleware.rst:393
msgid "Scrapy's default referrer policy â€” just like `\"no-referrer-when-downgrade\"`_, the W3C-recommended value for browsers â€” will send a non-empty \"Referer\" header from any ``http(s)://`` to any ``https://`` URL, even if the domain is different."
msgstr ""

#: ../../topics/spider-middleware.rst:398
msgid "`\"same-origin\"`_ may be a better choice if you want to remove referrer information for cross-domain requests."
msgstr ""

#: ../../../scrapy/spidermiddlewares/referer.py:docstring of scrapy.spidermiddlewares.referer.NoReferrerPolicy:1
msgid "https://www.w3.org/TR/referrer-policy/#referrer-policy-no-referrer"
msgstr ""

#: ../../../scrapy/spidermiddlewares/referer.py:docstring of scrapy.spidermiddlewares.referer.NoReferrerPolicy:3
msgid "The simplest policy is \"no-referrer\", which specifies that no referrer information is to be sent along with requests made from a particular request client to any origin. The header will be omitted entirely."
msgstr ""

#: ../../../scrapy/spidermiddlewares/referer.py:docstring of scrapy.spidermiddlewares.referer.NoReferrerWhenDowngradePolicy:1
msgid "https://www.w3.org/TR/referrer-policy/#referrer-policy-no-referrer-when-downgrade"
msgstr ""

#: ../../../scrapy/spidermiddlewares/referer.py:docstring of scrapy.spidermiddlewares.referer.NoReferrerWhenDowngradePolicy:3
msgid "The \"no-referrer-when-downgrade\" policy sends a full URL along with requests from a TLS-protected environment settings object to a potentially trustworthy URL, and requests from clients which are not TLS-protected to any origin."
msgstr ""

#: ../../../scrapy/spidermiddlewares/referer.py:docstring of scrapy.spidermiddlewares.referer.NoReferrerWhenDowngradePolicy:7
msgid "Requests from TLS-protected clients to non-potentially trustworthy URLs, on the other hand, will contain no referrer information. A Referer HTTP header will not be sent."
msgstr ""

#: ../../../scrapy/spidermiddlewares/referer.py:docstring of scrapy.spidermiddlewares.referer.NoReferrerWhenDowngradePolicy:11
msgid "This is a user agent's default behavior, if no policy is otherwise specified."
msgstr ""

#: ../../topics/spider-middleware.rst:405
msgid "\"no-referrer-when-downgrade\" policy is the W3C-recommended default, and is used by major web browsers."
msgstr ""

#: ../../topics/spider-middleware.rst:408
msgid "However, it is NOT Scrapy's default referrer policy (see :class:`DefaultReferrerPolicy`)."
msgstr ""

#: ../../../scrapy/spidermiddlewares/referer.py:docstring of scrapy.spidermiddlewares.referer.SameOriginPolicy:1
msgid "https://www.w3.org/TR/referrer-policy/#referrer-policy-same-origin"
msgstr ""

#: ../../../scrapy/spidermiddlewares/referer.py:docstring of scrapy.spidermiddlewares.referer.SameOriginPolicy:3
msgid "The \"same-origin\" policy specifies that a full URL, stripped for use as a referrer, is sent as referrer information when making same-origin requests from a particular request client."
msgstr ""

#: ../../../scrapy/spidermiddlewares/referer.py:docstring of scrapy.spidermiddlewares.referer.SameOriginPolicy:6
msgid "Cross-origin requests, on the other hand, will contain no referrer information. A Referer HTTP header will not be sent."
msgstr ""

#: ../../../scrapy/spidermiddlewares/referer.py:docstring of scrapy.spidermiddlewares.referer.OriginPolicy:1
msgid "https://www.w3.org/TR/referrer-policy/#referrer-policy-origin"
msgstr ""

#: ../../../scrapy/spidermiddlewares/referer.py:docstring of scrapy.spidermiddlewares.referer.OriginPolicy:3
msgid "The \"origin\" policy specifies that only the ASCII serialization of the origin of the request client is sent as referrer information when making both same-origin requests and cross-origin requests from a particular request client."
msgstr ""

#: ../../../scrapy/spidermiddlewares/referer.py:docstring of scrapy.spidermiddlewares.referer.StrictOriginPolicy:1
msgid "https://www.w3.org/TR/referrer-policy/#referrer-policy-strict-origin"
msgstr ""

#: ../../../scrapy/spidermiddlewares/referer.py:docstring of scrapy.spidermiddlewares.referer.StrictOriginPolicy:3
msgid "The \"strict-origin\" policy sends the ASCII serialization of the origin of the request client when making requests: - from a TLS-protected environment settings object to a potentially trustworthy URL, and - from non-TLS-protected environment settings objects to any origin."
msgstr ""

#: ../../../scrapy/spidermiddlewares/referer.py:docstring of scrapy.spidermiddlewares.referer.StrictOriginPolicy:8
msgid "Requests from TLS-protected request clients to non- potentially trustworthy URLs, on the other hand, will contain no referrer information. A Referer HTTP header will not be sent."
msgstr ""

#: ../../../scrapy/spidermiddlewares/referer.py:docstring of scrapy.spidermiddlewares.referer.OriginWhenCrossOriginPolicy:1
msgid "https://www.w3.org/TR/referrer-policy/#referrer-policy-origin-when-cross-origin"
msgstr ""

#: ../../../scrapy/spidermiddlewares/referer.py:docstring of scrapy.spidermiddlewares.referer.OriginWhenCrossOriginPolicy:3
msgid "The \"origin-when-cross-origin\" policy specifies that a full URL, stripped for use as a referrer, is sent as referrer information when making same-origin requests from a particular request client, and only the ASCII serialization of the origin of the request client is sent as referrer information when making cross-origin requests from a particular request client."
msgstr ""

#: ../../../scrapy/spidermiddlewares/referer.py:docstring of scrapy.spidermiddlewares.referer.StrictOriginWhenCrossOriginPolicy:1
msgid "https://www.w3.org/TR/referrer-policy/#referrer-policy-strict-origin-when-cross-origin"
msgstr ""

#: ../../../scrapy/spidermiddlewares/referer.py:docstring of scrapy.spidermiddlewares.referer.StrictOriginWhenCrossOriginPolicy:3
msgid "The \"strict-origin-when-cross-origin\" policy specifies that a full URL, stripped for use as a referrer, is sent as referrer information when making same-origin requests from a particular request client, and only the ASCII serialization of the origin of the request client when making cross-origin requests:"
msgstr ""

#: ../../../scrapy/spidermiddlewares/referer.py:docstring of scrapy.spidermiddlewares.referer.StrictOriginWhenCrossOriginPolicy:9
msgid "from a TLS-protected environment settings object to a potentially trustworthy URL, and"
msgstr ""

#: ../../../scrapy/spidermiddlewares/referer.py:docstring of scrapy.spidermiddlewares.referer.StrictOriginWhenCrossOriginPolicy:10
msgid "from non-TLS-protected environment settings objects to any origin."
msgstr ""

#: ../../../scrapy/spidermiddlewares/referer.py:docstring of scrapy.spidermiddlewares.referer.StrictOriginWhenCrossOriginPolicy:12
msgid "Requests from TLS-protected clients to non- potentially trustworthy URLs, on the other hand, will contain no referrer information. A Referer HTTP header will not be sent."
msgstr ""

#: ../../../scrapy/spidermiddlewares/referer.py:docstring of scrapy.spidermiddlewares.referer.UnsafeUrlPolicy:1
msgid "https://www.w3.org/TR/referrer-policy/#referrer-policy-unsafe-url"
msgstr ""

#: ../../../scrapy/spidermiddlewares/referer.py:docstring of scrapy.spidermiddlewares.referer.UnsafeUrlPolicy:3
msgid "The \"unsafe-url\" policy specifies that a full URL, stripped for use as a referrer, is sent along with both cross-origin requests and same-origin requests made from a particular request client."
msgstr ""

#: ../../../scrapy/spidermiddlewares/referer.py:docstring of scrapy.spidermiddlewares.referer.UnsafeUrlPolicy:7
msgid "Note: The policy's name doesn't lie; it is unsafe. This policy will leak origins and paths from TLS-protected resources to insecure origins. Carefully consider the impact of setting such a policy for potentially sensitive documents."
msgstr ""

#: ../../topics/spider-middleware.rst:422
msgid "\"unsafe-url\" policy is NOT recommended."
msgstr ""

#: ../../topics/spider-middleware.rst:436
msgid "UrlLengthMiddleware"
msgstr ""

#: ../../topics/spider-middleware.rst:443
msgid "Filters out requests with URLs longer than URLLENGTH_LIMIT"
msgstr ""

#: ../../topics/spider-middleware.rst:445
msgid "The :class:`UrlLengthMiddleware` can be configured through the following settings (see the settings documentation for more info):"
msgstr ""

#: ../../topics/spider-middleware.rst:448
msgid ":setting:`URLLENGTH_LIMIT` - The maximum URL length to allow for crawled URLs."
msgstr ""

#: ../../topics/spiders.rst:7
msgid "Spiders are classes which define how a certain site (or a group of sites) will be scraped, including how to perform the crawl (i.e. follow links) and how to extract structured data from their pages (i.e. scraping items). In other words, Spiders are the place where you define the custom behaviour for crawling and parsing pages for a particular site (or, in some cases, a group of sites)."
msgstr ""

#: ../../topics/spiders.rst:13
msgid "For spiders, the scraping cycle goes through something like this:"
msgstr ""

#: ../../topics/spiders.rst:15
msgid "You start by generating the initial Requests to crawl the first URLs, and specify a callback function to be called with the response downloaded from those requests."
msgstr ""

#: ../../topics/spiders.rst:19
msgid "The first requests to perform are obtained by calling the :meth:`~scrapy.spiders.Spider.start_requests` method which (by default) generates :class:`~scrapy.http.Request` for the URLs specified in the :attr:`~scrapy.spiders.Spider.start_urls` and the :attr:`~scrapy.spiders.Spider.parse` method as callback function for the Requests."
msgstr ""

#: ../../topics/spiders.rst:26
msgid "In the callback function, you parse the response (web page) and return either dicts with extracted data, :class:`~scrapy.item.Item` objects, :class:`~scrapy.http.Request` objects, or an iterable of these objects. Those Requests will also contain a callback (maybe the same) and will then be downloaded by Scrapy and then their response handled by the specified callback."
msgstr ""

#: ../../topics/spiders.rst:33
msgid "In callback functions, you parse the page contents, typically using :ref:`topics-selectors` (but you can also use BeautifulSoup, lxml or whatever mechanism you prefer) and generate items with the parsed data."
msgstr ""

#: ../../topics/spiders.rst:37
msgid "Finally, the items returned from the spider will be typically persisted to a database (in some :ref:`Item Pipeline <topics-item-pipeline>`) or written to a file using :ref:`topics-feed-exports`."
msgstr ""

#: ../../topics/spiders.rst:41
msgid "Even though this cycle applies (more or less) to any kind of spider, there are different kinds of default spiders bundled into Scrapy for different purposes. We will talk about those types here."
msgstr ""

#: ../../topics/spiders.rst:51
msgid "scrapy.Spider"
msgstr ""

#: ../../topics/spiders.rst:55
msgid "This is the simplest spider, and the one from which every other spider must inherit (including spiders that come bundled with Scrapy, as well as spiders that you write yourself). It doesn't provide any special functionality. It just provides a default :meth:`start_requests` implementation which sends requests from the :attr:`start_urls` spider attribute and calls the spider's method ``parse`` for each of the resulting responses."
msgstr ""

#: ../../topics/spiders.rst:64
msgid "A string which defines the name for this spider. The spider name is how the spider is located (and instantiated) by Scrapy, so it must be unique. However, nothing prevents you from instantiating more than one instance of the same spider. This is the most important spider attribute and it's required."
msgstr ""

#: ../../topics/spiders.rst:70
msgid "If the spider scrapes a single domain, a common practice is to name the spider after the domain, with or without the `TLD`_. So, for example, a spider that crawls ``mywebsite.com`` would often be called ``mywebsite``."
msgstr ""

#: ../../topics/spiders.rst:77
msgid "An optional list of strings containing domains that this spider is allowed to crawl. Requests for URLs not belonging to the domain names specified in this list (or their subdomains) won't be followed if :class:`~scrapy.spidermiddlewares.offsite.OffsiteMiddleware` is enabled."
msgstr ""

#: ../../topics/spiders.rst:82
msgid "Let's say your target url is ``https://www.example.com/1.html``, then add ``'example.com'`` to the list."
msgstr ""

#: ../../topics/spiders.rst:87
msgid "A list of URLs where the spider will begin to crawl from, when no particular URLs are specified. So, the first pages downloaded will be those listed here. The subsequent :class:`~scrapy.http.Request` will be generated successively from data contained in the start URLs."
msgstr ""

#: ../../topics/spiders.rst:94
msgid "A dictionary of settings that will be overridden from the project wide configuration when running this spider. It must be defined as a class attribute since the settings are updated before instantiation."
msgstr ""

#: ../../topics/spiders.rst:103
msgid "This attribute is set by the :meth:`from_crawler` class method after initializating the class, and links to the :class:`~scrapy.crawler.Crawler` object to which this spider instance is bound."
msgstr ""

#: ../../topics/spiders.rst:108
msgid "Crawlers encapsulate a lot of components in the project for their single entry access (such as extensions, middlewares, signals managers, etc). See :ref:`topics-api-crawler` to know more about them."
msgstr ""

#: ../../topics/spiders.rst:114
msgid "Configuration for running this spider. This is a :class:`~scrapy.settings.Settings` instance, see the :ref:`topics-settings` topic for a detailed introduction on this subject."
msgstr ""

#: ../../topics/spiders.rst:120
msgid "Python logger created with the Spider's :attr:`name`. You can use it to send log messages through it as described on :ref:`topics-logging-from-spiders`."
msgstr ""

#: ../../topics/spiders.rst:126
msgid "This is the class method used by Scrapy to create your spiders."
msgstr ""

#: ../../topics/spiders.rst:128
msgid "You probably won't need to override this directly because the default implementation acts as a proxy to the :meth:`__init__` method, calling it with the given arguments ``args`` and named arguments ``kwargs``."
msgstr ""

#: ../../topics/spiders.rst:132
msgid "Nonetheless, this method sets the :attr:`crawler` and :attr:`settings` attributes in the new instance so they can be accessed later inside the spider's code."
msgstr ""

#: ../../topics/spiders.rst:136
msgid "crawler to which the spider will be bound"
msgstr ""

#: ../../topics/spiders.rst:139
msgid "arguments passed to the :meth:`__init__` method"
msgstr ""

#: ../../topics/spiders.rst:142
msgid "keyword arguments passed to the :meth:`__init__` method"
msgstr ""

#: ../../topics/spiders.rst:147
msgid "This method must return an iterable with the first Requests to crawl for this spider. It is called by Scrapy when the spider is opened for scraping. Scrapy calls it only once, so it is safe to implement :meth:`start_requests` as a generator."
msgstr ""

#: ../../topics/spiders.rst:152
msgid "The default implementation generates ``Request(url, dont_filter=True)`` for each url in :attr:`start_urls`."
msgstr ""

#: ../../topics/spiders.rst:155
msgid "If you want to change the Requests used to start scraping a domain, this is the method to override. For example, if you need to start by logging in using a POST request, you could do::"
msgstr ""

#: ../../topics/spiders.rst:174
msgid "This is the default callback used by Scrapy to process downloaded responses, when their requests don't specify a callback."
msgstr ""

#: ../../topics/spiders.rst:177
msgid "The ``parse`` method is in charge of processing the response and returning scraped data and/or more URLs to follow. Other Requests callbacks have the same requirements as the :class:`Spider` class."
msgstr ""

#: ../../topics/spiders.rst:181
msgid "This method, as well as any other Request callback, must return an iterable of :class:`~scrapy.http.Request` and/or dicts or :class:`~scrapy.item.Item` objects."
msgstr ""

#: ../../topics/spiders.rst:185
msgid "the response to parse"
msgstr ""

#: ../../topics/spiders.rst:190
msgid "Wrapper that sends a log message through the Spider's :attr:`logger`, kept for backward compatibility. For more information see :ref:`topics-logging-from-spiders`."
msgstr ""

#: ../../topics/spiders.rst:196
msgid "Called when the spider closes. This method provides a shortcut to signals.connect() for the :signal:`spider_closed` signal."
msgstr ""

#: ../../topics/spiders.rst:199
msgid "Let's see an example::"
msgstr ""

#: ../../topics/spiders.rst:216
msgid "Return multiple Requests and items from a single callback::"
msgstr ""

#: ../../topics/spiders.rst:236
msgid "Instead of :attr:`~.start_urls` you can use :meth:`~.start_requests` directly; to give data more structure you can use :ref:`topics-items`::"
msgstr ""

#: ../../topics/spiders.rst:261
msgid "Spider arguments"
msgstr ""

#: ../../topics/spiders.rst:263
msgid "Spiders can receive arguments that modify their behaviour. Some common uses for spider arguments are to define the start URLs or to restrict the crawl to certain sections of the site, but they can be used to configure any functionality of the spider."
msgstr ""

#: ../../topics/spiders.rst:268
msgid "Spider arguments are passed through the :command:`crawl` command using the ``-a`` option. For example::"
msgstr ""

#: ../../topics/spiders.rst:273
msgid "Spiders can access arguments in their `__init__` methods::"
msgstr ""

#: ../../topics/spiders.rst:285
msgid "The default `__init__` method will take any spider arguments and copy them to the spider as attributes. The above example can also be written as follows::"
msgstr ""

#: ../../topics/spiders.rst:297
msgid "Keep in mind that spider arguments are only strings. The spider will not do any parsing on its own. If you were to set the ``start_urls`` attribute from the command line, you would have to parse it on your own into a list using something like :func:`ast.literal_eval` or :func:`json.loads` and then set it as an attribute. Otherwise, you would cause iteration over a ``start_urls`` string (a very common python pitfall) resulting in each character being seen as a separate url."
msgstr ""

#: ../../topics/spiders.rst:307
msgid "A valid use case is to set the http auth credentials used by :class:`~scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware` or the user agent used by :class:`~scrapy.downloadermiddlewares.useragent.UserAgentMiddleware`::"
msgstr ""

#: ../../topics/spiders.rst:314
msgid "Spider arguments can also be passed through the Scrapyd ``schedule.json`` API. See `Scrapyd documentation`_."
msgstr ""

#: ../../topics/spiders.rst:320
msgid "Generic Spiders"
msgstr ""

#: ../../topics/spiders.rst:322
msgid "Scrapy comes with some useful generic spiders that you can use to subclass your spiders from. Their aim is to provide convenient functionality for a few common scraping cases, like following all links on a site based on certain rules, crawling from `Sitemaps`_, or parsing an XML/CSV feed."
msgstr ""

#: ../../topics/spiders.rst:327
msgid "For the examples used in the following spiders, we'll assume you have a project with a ``TestItem`` declared in a ``myproject.items`` module::"
msgstr ""

#: ../../topics/spiders.rst:341
msgid "CrawlSpider"
msgstr ""

#: ../../topics/spiders.rst:345
msgid "This is the most commonly used spider for crawling regular websites, as it provides a convenient mechanism for following links by defining a set of rules. It may not be the best suited for your particular web sites or project, but it's generic enough for several cases, so you can start from it and override it as needed for more custom functionality, or just implement your own spider."
msgstr ""

#: ../../topics/spiders.rst:351
msgid "Apart from the attributes inherited from Spider (that you must specify), this class supports a new attribute:"
msgstr ""

#: ../../topics/spiders.rst:356
msgid "Which is a list of one (or more) :class:`Rule` objects.  Each :class:`Rule` defines a certain behaviour for crawling the site. Rules objects are described below. If multiple rules match the same link, the first one will be used, according to the order they're defined in this attribute."
msgstr ""

#: ../../topics/spiders.rst:361
msgid "This spider also exposes an overrideable method:"
msgstr ""

#: ../../topics/spiders.rst:365
msgid "This method is called for the start_urls responses. It allows to parse the initial responses and must return either an :class:`~scrapy.item.Item` object, a :class:`~scrapy.http.Request` object, or an iterable containing any of them."
msgstr ""

#: ../../topics/spiders.rst:371
msgid "Crawling rules"
msgstr ""

#: ../../topics/spiders.rst:375
msgid "``link_extractor`` is a :ref:`Link Extractor <topics-link-extractors>` object which defines how links will be extracted from each crawled page. Each produced link will be used to generate a :class:`~scrapy.http.Request` object, which will contain the link's text in its ``meta`` dictionary (under the ``link_text`` key). If omitted, a default link extractor created with no arguments will be used, resulting in all links being extracted."
msgstr ""

#: ../../topics/spiders.rst:382
msgid "``callback`` is a callable or a string (in which case a method from the spider object with that name will be used) to be called for each link extracted with the specified link extractor. This callback receives a :class:`~scrapy.http.Response` as its first argument and must return either a single instance or an iterable of :class:`~scrapy.item.Item`, ``dict`` and/or :class:`~scrapy.http.Request` objects (or any subclass of them). As mentioned above, the received :class:`~scrapy.http.Response` object will contain the text of the link that produced the :class:`~scrapy.http.Request` in its ``meta`` dictionary (under the ``link_text`` key)"
msgstr ""

#: ../../topics/spiders.rst:391
msgid "When writing crawl spider rules, avoid using ``parse`` as callback, since the :class:`CrawlSpider` uses the ``parse`` method itself to implement its logic. So if you override the ``parse`` method, the crawl spider will no longer work."
msgstr ""

#: ../../topics/spiders.rst:396
msgid "``cb_kwargs`` is a dict containing the keyword arguments to be passed to the callback function."
msgstr ""

#: ../../topics/spiders.rst:399
msgid "``follow`` is a boolean which specifies if links should be followed from each response extracted with this rule. If ``callback`` is None ``follow`` defaults to ``True``, otherwise it defaults to ``False``."
msgstr ""

#: ../../topics/spiders.rst:403
msgid "``process_links`` is a callable, or a string (in which case a method from the spider object with that name will be used) which will be called for each list of links extracted from each response using the specified ``link_extractor``. This is mainly used for filtering purposes."
msgstr ""

#: ../../topics/spiders.rst:408
msgid "``process_request`` is a callable (or a string, in which case a method from the spider object with that name will be used) which will be called for every :class:`~scrapy.http.Request` extracted by this rule. This callable should take said request as first argument and the :class:`~scrapy.http.Response` from which the request originated as second argument. It must return a ``Request`` object or ``None`` (to filter out the request)."
msgstr ""

#: ../../topics/spiders.rst:415
msgid "``errback`` is a callable or a string (in which case a method from the spider object with that name will be used) to be called if any exception is raised while processing a request generated by the rule. It receives a :class:`Twisted Failure <twisted.python.failure.Failure>` instance as first parameter."
msgstr ""

#: ../../topics/spiders.rst:421
msgid "The *errback* parameter."
msgstr ""

#: ../../topics/spiders.rst:425
msgid "CrawlSpider example"
msgstr ""

#: ../../topics/spiders.rst:427
msgid "Let's now take a look at an example CrawlSpider with rules::"
msgstr ""

#: ../../topics/spiders.rst:457
msgid "This spider would start crawling example.com's home page, collecting category links, and item links, parsing the latter with the ``parse_item`` method. For each item response, some data will be extracted from the HTML using XPath, and an :class:`~scrapy.item.Item` will be filled with it."
msgstr ""

#: ../../topics/spiders.rst:463
msgid "XMLFeedSpider"
msgstr ""

#: ../../topics/spiders.rst:467
msgid "XMLFeedSpider is designed for parsing XML feeds by iterating through them by a certain node name.  The iterator can be chosen from: ``iternodes``, ``xml``, and ``html``.  It's recommended to use the ``iternodes`` iterator for performance reasons, since the ``xml`` and ``html`` iterators generate the whole DOM at once in order to parse it.  However, using ``html`` as the iterator may be useful when parsing XML with bad markup."
msgstr ""

#: ../../topics/spiders.rst:474
msgid "To set the iterator and the tag name, you must define the following class attributes:"
msgstr ""

#: ../../topics/spiders.rst:479
msgid "A string which defines the iterator to use. It can be either:"
msgstr ""

#: ../../topics/spiders.rst:481
msgid "``'iternodes'`` - a fast iterator based on regular expressions"
msgstr ""

#: ../../topics/spiders.rst:483
msgid "``'html'`` - an iterator which uses :class:`~scrapy.selector.Selector`. Keep in mind this uses DOM parsing and must load all DOM in memory which could be a problem for big feeds"
msgstr ""

#: ../../topics/spiders.rst:487
msgid "``'xml'`` - an iterator which uses :class:`~scrapy.selector.Selector`. Keep in mind this uses DOM parsing and must load all DOM in memory which could be a problem for big feeds"
msgstr ""

#: ../../topics/spiders.rst:491
msgid "It defaults to: ``'iternodes'``."
msgstr ""

#: ../../topics/spiders.rst:495
msgid "A string with the name of the node (or element) to iterate in. Example::"
msgstr ""

#: ../../topics/spiders.rst:501
msgid "A list of ``(prefix, uri)`` tuples which define the namespaces available in that document that will be processed with this spider. The ``prefix`` and ``uri`` will be used to automatically register namespaces using the :meth:`~scrapy.selector.Selector.register_namespace` method."
msgstr ""

#: ../../topics/spiders.rst:507
msgid "You can then specify nodes with namespaces in the :attr:`itertag` attribute."
msgstr ""

#: ../../topics/spiders.rst:518
msgid "Apart from these new attributes, this spider has the following overrideable methods too:"
msgstr ""

#: ../../topics/spiders.rst:523
msgid "A method that receives the response as soon as it arrives from the spider middleware, before the spider starts parsing it. It can be used to modify the response body before parsing it. This method receives a response and also returns a response (it could be the same or another one)."
msgstr ""

#: ../../topics/spiders.rst:530
msgid "This method is called for the nodes matching the provided tag name (``itertag``).  Receives the response and an :class:`~scrapy.selector.Selector` for each node.  Overriding this method is mandatory. Otherwise, you spider won't work.  This method must return either a :class:`~scrapy.item.Item` object, a :class:`~scrapy.http.Request` object, or an iterable containing any of them."
msgstr ""

#: ../../topics/spiders.rst:540
msgid "This method is called for each result (item or request) returned by the spider, and it's intended to perform any last time processing required before returning the results to the framework core, for example setting the item IDs. It receives a list of results and the response which originated those results. It must return a list of results (Items or Requests)."
msgstr ""

#: ../../topics/spiders.rst:548
msgid "XMLFeedSpider example"
msgstr ""

#: ../../topics/spiders.rst:550
msgid "These spiders are pretty easy to use, let's have a look at one example::"
msgstr ""

#: ../../topics/spiders.rst:571
msgid "Basically what we did up there was to create a spider that downloads a feed from the given ``start_urls``, and then iterates through each of its ``item`` tags, prints them out, and stores some random data in an :class:`~scrapy.item.Item`."
msgstr ""

#: ../../topics/spiders.rst:576
msgid "CSVFeedSpider"
msgstr ""

#: ../../topics/spiders.rst:580
msgid "This spider is very similar to the XMLFeedSpider, except that it iterates over rows, instead of nodes. The method that gets called in each iteration is :meth:`parse_row`."
msgstr ""

#: ../../topics/spiders.rst:586
msgid "A string with the separator character for each field in the CSV file Defaults to ``','`` (comma)."
msgstr ""

#: ../../topics/spiders.rst:591
msgid "A string with the enclosure character for each field in the CSV file Defaults to ``'\"'`` (quotation mark)."
msgstr ""

#: ../../topics/spiders.rst:596
msgid "A list of the column names in the CSV file."
msgstr ""

#: ../../topics/spiders.rst:600
msgid "Receives a response and a dict (representing each row) with a key for each provided (or detected) header of the CSV file.  This spider also gives the opportunity to override ``adapt_response`` and ``process_results`` methods for pre- and post-processing purposes."
msgstr ""

#: ../../topics/spiders.rst:606
msgid "CSVFeedSpider example"
msgstr ""

#: ../../topics/spiders.rst:608
msgid "Let's see an example similar to the previous one, but using a :class:`CSVFeedSpider`::"
msgstr ""

#: ../../topics/spiders.rst:633
msgid "SitemapSpider"
msgstr ""

#: ../../topics/spiders.rst:637
msgid "SitemapSpider allows you to crawl a site by discovering the URLs using `Sitemaps`_."
msgstr ""

#: ../../topics/spiders.rst:640
msgid "It supports nested sitemaps and discovering sitemap urls from `robots.txt`_."
msgstr ""

#: ../../topics/spiders.rst:645
msgid "A list of urls pointing to the sitemaps whose urls you want to crawl."
msgstr ""

#: ../../topics/spiders.rst:647
msgid "You can also point to a `robots.txt`_ and it will be parsed to extract sitemap urls from it."
msgstr ""

#: ../../topics/spiders.rst:652
msgid "A list of tuples ``(regex, callback)`` where:"
msgstr ""

#: ../../topics/spiders.rst:654
msgid "``regex`` is a regular expression to match urls extracted from sitemaps. ``regex`` can be either a str or a compiled regex object."
msgstr ""

#: ../../topics/spiders.rst:657
msgid "callback is the callback to use for processing the urls that match the regular expression. ``callback`` can be a string (indicating the name of a spider method) or a callable."
msgstr ""

#: ../../topics/spiders.rst:665
msgid "Rules are applied in order, and only the first one that matches will be used."
msgstr ""

#: ../../topics/spiders.rst:668
msgid "If you omit this attribute, all urls found in sitemaps will be processed with the ``parse`` callback."
msgstr ""

#: ../../topics/spiders.rst:673
msgid "A list of regexes of sitemap that should be followed. This is only for sites that use `Sitemap index files`_ that point to other sitemap files."
msgstr ""

#: ../../topics/spiders.rst:677
msgid "By default, all sitemaps are followed."
msgstr ""

#: ../../topics/spiders.rst:681
msgid "Specifies if alternate links for one ``url`` should be followed. These are links for the same website in another language passed within the same ``url`` block."
msgstr ""

#: ../../topics/spiders.rst:692
msgid "With ``sitemap_alternate_links`` set, this would retrieve both URLs. With ``sitemap_alternate_links`` disabled, only ``http://example.com/`` would be retrieved."
msgstr ""

#: ../../topics/spiders.rst:696
msgid "Default is ``sitemap_alternate_links`` disabled."
msgstr ""

#: ../../topics/spiders.rst:700
msgid "This is a filter function that could be overridden to select sitemap entries based on their attributes."
msgstr ""

#: ../../topics/spiders.rst:710
msgid "We can define a ``sitemap_filter`` function to filter ``entries`` by date::"
msgstr ""

#: ../../topics/spiders.rst:726
msgid "This would retrieve only ``entries`` modified on 2005 and the following years."
msgstr ""

#: ../../topics/spiders.rst:729
msgid "Entries are dict objects extracted from the sitemap document. Usually, the key is the tag name and the value is the text inside it."
msgstr ""

#: ../../topics/spiders.rst:732
msgid "It's important to notice that:"
msgstr ""

#: ../../topics/spiders.rst:734
msgid "as the loc attribute is required, entries without this tag are discarded"
msgstr ""

#: ../../topics/spiders.rst:735
msgid "alternate links are stored in a list with the key ``alternate`` (see ``sitemap_alternate_links``)"
msgstr ""

#: ../../topics/spiders.rst:737
msgid "namespaces are removed, so lxml tags named as ``{namespace}tagname`` become only ``tagname``"
msgstr ""

#: ../../topics/spiders.rst:739
msgid "If you omit this method, all entries found in sitemaps will be processed, observing other attributes and their settings."
msgstr ""

#: ../../topics/spiders.rst:744
msgid "SitemapSpider examples"
msgstr ""

#: ../../topics/spiders.rst:746
msgid "Simplest example: process all urls discovered through sitemaps using the ``parse`` callback::"
msgstr ""

#: ../../topics/spiders.rst:757
msgid "Process some urls with certain callback and other urls with a different callback::"
msgstr ""

#: ../../topics/spiders.rst:775
msgid "Follow sitemaps defined in the `robots.txt`_ file and only follow sitemaps whose url contains ``/sitemap_shop``::"
msgstr ""

#: ../../topics/spiders.rst:790
msgid "Combine SitemapSpider with other sources of urls::"
msgstr ""

#: ../../topics/stats.rst:5
msgid "Stats Collection"
msgstr ""

#: ../../topics/stats.rst:7
msgid "Scrapy provides a convenient facility for collecting stats in the form of key/values, where values are often counters. The facility is called the Stats Collector, and can be accessed through the :attr:`~scrapy.crawler.Crawler.stats` attribute of the :ref:`topics-api-crawler`, as illustrated by the examples in the :ref:`topics-stats-usecases` section below."
msgstr ""

#: ../../topics/stats.rst:13
msgid "However, the Stats Collector is always available, so you can always import it in your module and use its API (to increment or set new stat keys), regardless of whether the stats collection is enabled or not. If it's disabled, the API will still work but it won't collect anything. This is aimed at simplifying the stats collector usage: you should spend no more than one line of code for collecting stats in your spider, Scrapy extension, or whatever code you're using the Stats Collector from."
msgstr ""

#: ../../topics/stats.rst:21
msgid "Another feature of the Stats Collector is that it's very efficient (when enabled) and extremely efficient (almost unnoticeable) when disabled."
msgstr ""

#: ../../topics/stats.rst:24
msgid "The Stats Collector keeps a stats table per open spider which is automatically opened when the spider is opened, and closed when the spider is closed."
msgstr ""

#: ../../topics/stats.rst:30
msgid "Common Stats Collector uses"
msgstr ""

#: ../../topics/stats.rst:32
msgid "Access the stats collector through the :attr:`~scrapy.crawler.Crawler.stats` attribute. Here is an example of an extension that access stats::"
msgstr ""

#: ../../topics/stats.rst:44
msgid "Set stat value::"
msgstr ""

#: ../../topics/stats.rst:48
msgid "Increment stat value::"
msgstr ""

#: ../../topics/stats.rst:52
msgid "Set stat value only if greater than previous::"
msgstr ""

#: ../../topics/stats.rst:56
msgid "Set stat value only if lower than previous::"
msgstr ""

#: ../../topics/stats.rst:60
msgid "Get stat value:"
msgstr ""

#: ../../topics/stats.rst:65
msgid "Get all stats:"
msgstr ""

#: ../../topics/stats.rst:71
msgid "Available Stats Collectors"
msgstr ""

#: ../../topics/stats.rst:73
msgid "Besides the basic :class:`StatsCollector` there are other Stats Collectors available in Scrapy which extend the basic Stats Collector. You can select which Stats Collector to use through the :setting:`STATS_CLASS` setting. The default Stats Collector used is the :class:`MemoryStatsCollector`."
msgstr ""

#: ../../topics/stats.rst:81
msgid "MemoryStatsCollector"
msgstr ""

#: ../../topics/stats.rst:85
msgid "A simple stats collector that keeps the stats of the last scraping run (for each spider) in memory, after they're closed. The stats can be accessed through the :attr:`spider_stats` attribute, which is a dict keyed by spider domain name."
msgstr ""

#: ../../topics/stats.rst:90
msgid "This is the default Stats Collector used in Scrapy."
msgstr ""

#: ../../topics/stats.rst:94
msgid "A dict of dicts (keyed by spider name) containing the stats of the last scraping run for each spider."
msgstr ""

#: ../../topics/stats.rst:98
msgid "DummyStatsCollector"
msgstr ""

#: ../../topics/stats.rst:102
msgid "A Stats collector which does nothing but is very efficient (because it does nothing). This stats collector can be set via the :setting:`STATS_CLASS` setting, to disable stats collect in order to improve performance. However, the performance penalty of stats collection is usually marginal compared to other Scrapy workload like parsing pages."
msgstr ""

#: ../../topics/telnetconsole.rst:7
msgid "Telnet Console"
msgstr ""

#: ../../topics/telnetconsole.rst:9
msgid "Scrapy comes with a built-in telnet console for inspecting and controlling a Scrapy running process. The telnet console is just a regular python shell running inside the Scrapy process, so you can do literally anything from it."
msgstr ""

#: ../../topics/telnetconsole.rst:13
msgid "The telnet console is a :ref:`built-in Scrapy extension <topics-extensions-ref>` which comes enabled by default, but you can also disable it if you want. For more information about the extension itself see :ref:`topics-extensions-ref-telnetconsole`."
msgstr ""

#: ../../topics/telnetconsole.rst:19
msgid "It is not secure to use telnet console via public networks, as telnet doesn't provide any transport-layer security. Having username/password authentication doesn't change that."
msgstr ""

#: ../../topics/telnetconsole.rst:23
msgid "Intended usage is connecting to a running Scrapy spider locally (spider process and telnet client are on the same machine) or over a secure connection (VPN, SSH tunnel). Please avoid using telnet console over insecure connections, or disable it completely using :setting:`TELNETCONSOLE_ENABLED` option."
msgstr ""

#: ../../topics/telnetconsole.rst:32
msgid "How to access the telnet console"
msgstr ""

#: ../../topics/telnetconsole.rst:34
msgid "The telnet console listens in the TCP port defined in the :setting:`TELNETCONSOLE_PORT` setting, which defaults to ``6023``. To access the console you need to type::"
msgstr ""

#: ../../topics/telnetconsole.rst:46
msgid "By default Username is ``scrapy`` and Password is autogenerated. The autogenerated Password can be seen on Scrapy logs like the example below::"
msgstr ""

#: ../../topics/telnetconsole.rst:51
msgid "Default Username and Password can be overridden by the settings :setting:`TELNETCONSOLE_USERNAME` and :setting:`TELNETCONSOLE_PASSWORD`."
msgstr ""

#: ../../topics/telnetconsole.rst:55
msgid "Username and password provide only a limited protection, as telnet is not using secure transport - by default traffic is not encrypted even if username and password are set."
msgstr ""

#: ../../topics/telnetconsole.rst:59
msgid "You need the telnet program which comes installed by default in Windows, and most Linux distros."
msgstr ""

#: ../../topics/telnetconsole.rst:63
msgid "Available variables in the telnet console"
msgstr ""

#: ../../topics/telnetconsole.rst:65
msgid "The telnet console is like a regular Python shell running inside the Scrapy process, so you can do anything from it including importing new modules, etc."
msgstr ""

#: ../../topics/telnetconsole.rst:68
msgid "However, the telnet console comes with some default variables defined for convenience:"
msgstr ""

#: ../../topics/telnetconsole.rst:72
msgid "Shortcut"
msgstr ""

#: ../../topics/telnetconsole.rst:72
msgid "Description"
msgstr ""

#: ../../topics/telnetconsole.rst:74
msgid "``crawler``"
msgstr ""

#: ../../topics/telnetconsole.rst:74
msgid "the Scrapy Crawler (:class:`scrapy.crawler.Crawler` object)"
msgstr ""

#: ../../topics/telnetconsole.rst:76
msgid "``engine``"
msgstr ""

#: ../../topics/telnetconsole.rst:76
msgid "Crawler.engine attribute"
msgstr ""

#: ../../topics/telnetconsole.rst:78
msgid "``spider``"
msgstr ""

#: ../../topics/telnetconsole.rst:78
msgid "the active spider"
msgstr ""

#: ../../topics/telnetconsole.rst:80
msgid "``slot``"
msgstr ""

#: ../../topics/telnetconsole.rst:80
msgid "the engine slot"
msgstr ""

#: ../../topics/telnetconsole.rst:82
msgid "``extensions``"
msgstr ""

#: ../../topics/telnetconsole.rst:82
msgid "the Extension Manager (Crawler.extensions attribute)"
msgstr ""

#: ../../topics/telnetconsole.rst:84
msgid "``stats``"
msgstr ""

#: ../../topics/telnetconsole.rst:84
msgid "the Stats Collector (Crawler.stats attribute)"
msgstr ""

#: ../../topics/telnetconsole.rst:86
msgid "``settings``"
msgstr ""

#: ../../topics/telnetconsole.rst:86
msgid "the Scrapy settings object (Crawler.settings attribute)"
msgstr ""

#: ../../topics/telnetconsole.rst:88
msgid "``est``"
msgstr ""

#: ../../topics/telnetconsole.rst:88
msgid "print a report of the engine status"
msgstr ""

#: ../../topics/telnetconsole.rst:90
msgid "``prefs``"
msgstr ""

#: ../../topics/telnetconsole.rst:90
#: ../../topics/telnetconsole.rst:94
msgid "for memory debugging (see :ref:`topics-leaks`)"
msgstr ""

#: ../../topics/telnetconsole.rst:92
msgid "``p``"
msgstr ""

#: ../../topics/telnetconsole.rst:92
msgid "a shortcut to the :func:`pprint.pprint` function"
msgstr ""

#: ../../topics/telnetconsole.rst:94
msgid "``hpy``"
msgstr ""

#: ../../topics/telnetconsole.rst:98
msgid "Telnet console usage examples"
msgstr ""

#: ../../topics/telnetconsole.rst:100
msgid "Here are some example tasks you can do with the telnet console:"
msgstr ""

#: ../../topics/telnetconsole.rst:103
msgid "View engine status"
msgstr ""

#: ../../topics/telnetconsole.rst:105
msgid "You can use the ``est()`` method of the Scrapy engine to quickly show its state using the telnet console::"
msgstr ""

#: ../../topics/telnetconsole.rst:130
msgid "Pause, resume and stop the Scrapy engine"
msgstr ""

#: ../../topics/telnetconsole.rst:132
msgid "To pause::"
msgstr ""

#: ../../topics/telnetconsole.rst:138
msgid "To resume::"
msgstr ""

#: ../../topics/telnetconsole.rst:144
msgid "To stop::"
msgstr ""

#: ../../topics/telnetconsole.rst:151
msgid "Telnet Console signals"
msgstr ""

#: ../../topics/telnetconsole.rst:156
msgid "Sent just before the telnet console is opened. You can hook up to this signal to add, remove or update the variables that will be available in the telnet local namespace. In order to do that, you need to update the ``telnet_vars`` dict in your handler."
msgstr ""

#: ../../topics/telnetconsole.rst:161
msgid "the dict of telnet variables"
msgstr ""

#: ../../topics/telnetconsole.rst:165
msgid "Telnet settings"
msgstr ""

#: ../../topics/telnetconsole.rst:167
msgid "These are the settings that control the telnet console's behaviour:"
msgstr ""

#: ../../topics/telnetconsole.rst:172
msgid "TELNETCONSOLE_PORT"
msgstr ""

#: ../../topics/telnetconsole.rst:174
msgid "Default: ``[6023, 6073]``"
msgstr ""

#: ../../topics/telnetconsole.rst:176
msgid "The port range to use for the telnet console. If set to ``None`` or ``0``, a dynamically assigned port is used."
msgstr ""

#: ../../topics/telnetconsole.rst:183
msgid "TELNETCONSOLE_HOST"
msgstr ""

#: ../../topics/telnetconsole.rst:185
msgid "Default: ``'127.0.0.1'``"
msgstr ""

#: ../../topics/telnetconsole.rst:187
msgid "The interface the telnet console should listen on"
msgstr ""

#: ../../topics/telnetconsole.rst:193
msgid "TELNETCONSOLE_USERNAME"
msgstr ""

#: ../../topics/telnetconsole.rst:195
msgid "Default: ``'scrapy'``"
msgstr ""

#: ../../topics/telnetconsole.rst:197
msgid "The username used for the telnet console"
msgstr ""

#: ../../topics/telnetconsole.rst:203
msgid "TELNETCONSOLE_PASSWORD"
msgstr ""

#: ../../topics/telnetconsole.rst:207
msgid "The password used for the telnet console, default behaviour is to have it autogenerated"
msgstr ""

#: ../../topics/webservice.rst:5
msgid "Web Service"
msgstr ""

#: ../../topics/webservice.rst:7
msgid "webservice has been moved into a separate project."
msgstr ""

#: ../../topics/webservice.rst:11
msgid "https://github.com/scrapy-plugins/scrapy-jsonrpc"
msgstr ""
