# SOME DESCRIPTIVE TITLE.
# Copyright (C) 2008â€“2020, Scrapy developers
# This file is distributed under the same license as the Scrapy package.
# FIRST AUTHOR <EMAIL@ADDRESS>, 2020.
#
#, fuzzy
msgid ""
msgstr ""
"Project-Id-Version: Scrapy 2.0\n"
"Report-Msgid-Bugs-To: \n"
"POT-Creation-Date: 2020-03-03 20:13+0100\n"
"PO-Revision-Date: YEAR-MO-DA HO:MI+ZONE\n"
"Last-Translator: FULL NAME <EMAIL@ADDRESS>\n"
"Language-Team: LANGUAGE <LL@li.org>\n"
"MIME-Version: 1.0\n"
"Content-Type: text/plain; charset=utf-8\n"
"Content-Transfer-Encoding: 8bit\n"
"Generated-By: Babel 2.8.0\n"

#: ../../topics/spiders.rst:5
msgid "Spiders"
msgstr ""

#: ../../topics/spiders.rst:7
msgid ""
"Spiders are classes which define how a certain site (or a group of sites)"
" will be scraped, including how to perform the crawl (i.e. follow links) "
"and how to extract structured data from their pages (i.e. scraping "
"items). In other words, Spiders are the place where you define the custom"
" behaviour for crawling and parsing pages for a particular site (or, in "
"some cases, a group of sites)."
msgstr ""

#: ../../topics/spiders.rst:13
msgid "For spiders, the scraping cycle goes through something like this:"
msgstr ""

#: ../../topics/spiders.rst:15
msgid ""
"You start by generating the initial Requests to crawl the first URLs, and"
" specify a callback function to be called with the response downloaded "
"from those requests."
msgstr ""

#: ../../topics/spiders.rst:19
msgid ""
"The first requests to perform are obtained by calling the "
":meth:`~scrapy.spiders.Spider.start_requests` method which (by default) "
"generates :class:`~scrapy.http.Request` for the URLs specified in the "
":attr:`~scrapy.spiders.Spider.start_urls` and the "
":attr:`~scrapy.spiders.Spider.parse` method as callback function for the "
"Requests."
msgstr ""

#: ../../topics/spiders.rst:26
msgid ""
"In the callback function, you parse the response (web page) and return "
"either dicts with extracted data, :class:`~scrapy.item.Item` objects, "
":class:`~scrapy.http.Request` objects, or an iterable of these objects. "
"Those Requests will also contain a callback (maybe the same) and will "
"then be downloaded by Scrapy and then their response handled by the "
"specified callback."
msgstr ""

#: ../../topics/spiders.rst:33
msgid ""
"In callback functions, you parse the page contents, typically using :ref"
":`topics-selectors` (but you can also use BeautifulSoup, lxml or whatever"
" mechanism you prefer) and generate items with the parsed data."
msgstr ""

#: ../../topics/spiders.rst:37
msgid ""
"Finally, the items returned from the spider will be typically persisted "
"to a database (in some :ref:`Item Pipeline <topics-item-pipeline>`) or "
"written to a file using :ref:`topics-feed-exports`."
msgstr ""

#: ../../topics/spiders.rst:41
msgid ""
"Even though this cycle applies (more or less) to any kind of spider, "
"there are different kinds of default spiders bundled into Scrapy for "
"different purposes. We will talk about those types here."
msgstr ""

#: ../../topics/spiders.rst:51
msgid "scrapy.Spider"
msgstr ""

#: ../../topics/spiders.rst:55
msgid ""
"This is the simplest spider, and the one from which every other spider "
"must inherit (including spiders that come bundled with Scrapy, as well as"
" spiders that you write yourself). It doesn't provide any special "
"functionality. It just provides a default :meth:`start_requests` "
"implementation which sends requests from the :attr:`start_urls` spider "
"attribute and calls the spider's method ``parse`` for each of the "
"resulting responses."
msgstr ""

#: ../../topics/spiders.rst:64
msgid ""
"A string which defines the name for this spider. The spider name is how "
"the spider is located (and instantiated) by Scrapy, so it must be unique."
" However, nothing prevents you from instantiating more than one instance "
"of the same spider. This is the most important spider attribute and it's "
"required."
msgstr ""

#: ../../topics/spiders.rst:70
msgid ""
"If the spider scrapes a single domain, a common practice is to name the "
"spider after the domain, with or without the `TLD`_. So, for example, a "
"spider that crawls ``mywebsite.com`` would often be called ``mywebsite``."
msgstr ""

#: ../../topics/spiders.rst:77
msgid ""
"An optional list of strings containing domains that this spider is "
"allowed to crawl. Requests for URLs not belonging to the domain names "
"specified in this list (or their subdomains) won't be followed if "
":class:`~scrapy.spidermiddlewares.offsite.OffsiteMiddleware` is enabled."
msgstr ""

#: ../../topics/spiders.rst:82
msgid ""
"Let's say your target url is ``https://www.example.com/1.html``, then add"
" ``'example.com'`` to the list."
msgstr ""

#: ../../topics/spiders.rst:87
msgid ""
"A list of URLs where the spider will begin to crawl from, when no "
"particular URLs are specified. So, the first pages downloaded will be "
"those listed here. The subsequent :class:`~scrapy.http.Request` will be "
"generated successively from data contained in the start URLs."
msgstr ""

#: ../../topics/spiders.rst:94
msgid ""
"A dictionary of settings that will be overridden from the project wide "
"configuration when running this spider. It must be defined as a class "
"attribute since the settings are updated before instantiation."
msgstr ""

#: ../../topics/spiders.rst:98
msgid "For a list of available built-in settings see: :ref:`topics-settings-ref`."
msgstr ""

#: ../../topics/spiders.rst:103
msgid ""
"This attribute is set by the :meth:`from_crawler` class method after "
"initializating the class, and links to the "
":class:`~scrapy.crawler.Crawler` object to which this spider instance is "
"bound."
msgstr ""

#: ../../topics/spiders.rst:108
msgid ""
"Crawlers encapsulate a lot of components in the project for their single "
"entry access (such as extensions, middlewares, signals managers, etc). "
"See :ref:`topics-api-crawler` to know more about them."
msgstr ""

#: ../../topics/spiders.rst:114
msgid ""
"Configuration for running this spider. This is a "
":class:`~scrapy.settings.Settings` instance, see the :ref:`topics-"
"settings` topic for a detailed introduction on this subject."
msgstr ""

#: ../../topics/spiders.rst:120
msgid ""
"Python logger created with the Spider's :attr:`name`. You can use it to "
"send log messages through it as described on :ref:`topics-logging-from-"
"spiders`."
msgstr ""

#: ../../topics/spiders.rst:126
msgid "This is the class method used by Scrapy to create your spiders."
msgstr ""

#: ../../topics/spiders.rst:128
msgid ""
"You probably won't need to override this directly because the default "
"implementation acts as a proxy to the :meth:`__init__` method, calling it"
" with the given arguments ``args`` and named arguments ``kwargs``."
msgstr ""

#: ../../topics/spiders.rst:132
msgid ""
"Nonetheless, this method sets the :attr:`crawler` and :attr:`settings` "
"attributes in the new instance so they can be accessed later inside the "
"spider's code."
msgstr ""

#: ../../topics/spiders.rst
msgid "Parameters"
msgstr ""

#: ../../topics/spiders.rst:136
msgid "crawler to which the spider will be bound"
msgstr ""

#: ../../topics/spiders.rst:139
msgid "arguments passed to the :meth:`__init__` method"
msgstr ""

#: ../../topics/spiders.rst:142
msgid "keyword arguments passed to the :meth:`__init__` method"
msgstr ""

#: ../../topics/spiders.rst:147
msgid ""
"This method must return an iterable with the first Requests to crawl for "
"this spider. It is called by Scrapy when the spider is opened for "
"scraping. Scrapy calls it only once, so it is safe to implement "
":meth:`start_requests` as a generator."
msgstr ""

#: ../../topics/spiders.rst:152
msgid ""
"The default implementation generates ``Request(url, dont_filter=True)`` "
"for each url in :attr:`start_urls`."
msgstr ""

#: ../../topics/spiders.rst:155
msgid ""
"If you want to change the Requests used to start scraping a domain, this "
"is the method to override. For example, if you need to start by logging "
"in using a POST request, you could do::"
msgstr ""

#: ../../topics/spiders.rst:174
msgid ""
"This is the default callback used by Scrapy to process downloaded "
"responses, when their requests don't specify a callback."
msgstr ""

#: ../../topics/spiders.rst:177
msgid ""
"The ``parse`` method is in charge of processing the response and "
"returning scraped data and/or more URLs to follow. Other Requests "
"callbacks have the same requirements as the :class:`Spider` class."
msgstr ""

#: ../../topics/spiders.rst:181
msgid ""
"This method, as well as any other Request callback, must return an "
"iterable of :class:`~scrapy.http.Request` and/or dicts or "
":class:`~scrapy.item.Item` objects."
msgstr ""

#: ../../topics/spiders.rst:185
msgid "the response to parse"
msgstr ""

#: ../../topics/spiders.rst:190
msgid ""
"Wrapper that sends a log message through the Spider's :attr:`logger`, "
"kept for backward compatibility. For more information see :ref:`topics-"
"logging-from-spiders`."
msgstr ""

#: ../../topics/spiders.rst:196
msgid ""
"Called when the spider closes. This method provides a shortcut to "
"signals.connect() for the :signal:`spider_closed` signal."
msgstr ""

#: ../../topics/spiders.rst:199
msgid "Let's see an example::"
msgstr ""

#: ../../topics/spiders.rst:216
msgid "Return multiple Requests and items from a single callback::"
msgstr ""

#: ../../topics/spiders.rst:236
msgid ""
"Instead of :attr:`~.start_urls` you can use :meth:`~.start_requests` "
"directly; to give data more structure you can use :ref:`topics-items`::"
msgstr ""

#: ../../topics/spiders.rst:261
msgid "Spider arguments"
msgstr ""

#: ../../topics/spiders.rst:263
msgid ""
"Spiders can receive arguments that modify their behaviour. Some common "
"uses for spider arguments are to define the start URLs or to restrict the"
" crawl to certain sections of the site, but they can be used to configure"
" any functionality of the spider."
msgstr ""

#: ../../topics/spiders.rst:268
msgid ""
"Spider arguments are passed through the :command:`crawl` command using "
"the ``-a`` option. For example::"
msgstr ""

#: ../../topics/spiders.rst:273
msgid "Spiders can access arguments in their `__init__` methods::"
msgstr ""

#: ../../topics/spiders.rst:285
msgid ""
"The default `__init__` method will take any spider arguments and copy "
"them to the spider as attributes. The above example can also be written "
"as follows::"
msgstr ""

#: ../../topics/spiders.rst:297
msgid ""
"Keep in mind that spider arguments are only strings. The spider will not "
"do any parsing on its own. If you were to set the ``start_urls`` "
"attribute from the command line, you would have to parse it on your own "
"into a list using something like `ast.literal_eval "
"<https://docs.python.org/3/library/ast.html#ast.literal_eval>`_ or "
"`json.loads <https://docs.python.org/3/library/json.html#json.loads>`_ "
"and then set it as an attribute. Otherwise, you would cause iteration "
"over a ``start_urls`` string (a very common python pitfall) resulting in "
"each character being seen as a separate url."
msgstr ""

#: ../../topics/spiders.rst:309
msgid ""
"A valid use case is to set the http auth credentials used by "
":class:`~scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware` or the"
" user agent used by "
":class:`~scrapy.downloadermiddlewares.useragent.UserAgentMiddleware`::"
msgstr ""

#: ../../topics/spiders.rst:316
msgid ""
"Spider arguments can also be passed through the Scrapyd ``schedule.json``"
" API. See `Scrapyd documentation`_."
msgstr ""

#: ../../topics/spiders.rst:322
msgid "Generic Spiders"
msgstr ""

#: ../../topics/spiders.rst:324
msgid ""
"Scrapy comes with some useful generic spiders that you can use to "
"subclass your spiders from. Their aim is to provide convenient "
"functionality for a few common scraping cases, like following all links "
"on a site based on certain rules, crawling from `Sitemaps`_, or parsing "
"an XML/CSV feed."
msgstr ""

#: ../../topics/spiders.rst:329
msgid ""
"For the examples used in the following spiders, we'll assume you have a "
"project with a ``TestItem`` declared in a ``myproject.items`` module::"
msgstr ""

#: ../../topics/spiders.rst:343
msgid "CrawlSpider"
msgstr ""

#: ../../topics/spiders.rst:347
msgid ""
"This is the most commonly used spider for crawling regular websites, as "
"it provides a convenient mechanism for following links by defining a set "
"of rules. It may not be the best suited for your particular web sites or "
"project, but it's generic enough for several cases, so you can start from"
" it and override it as needed for more custom functionality, or just "
"implement your own spider."
msgstr ""

#: ../../topics/spiders.rst:353
msgid ""
"Apart from the attributes inherited from Spider (that you must specify), "
"this class supports a new attribute:"
msgstr ""

#: ../../topics/spiders.rst:358
msgid ""
"Which is a list of one (or more) :class:`Rule` objects.  Each "
":class:`Rule` defines a certain behaviour for crawling the site. Rules "
"objects are described below. If multiple rules match the same link, the "
"first one will be used, according to the order they're defined in this "
"attribute."
msgstr ""

#: ../../topics/spiders.rst:363
msgid "This spider also exposes an overrideable method:"
msgstr ""

#: ../../topics/spiders.rst:367
msgid ""
"This method is called for the start_urls responses. It allows to parse "
"the initial responses and must return either an "
":class:`~scrapy.item.Item` object, a :class:`~scrapy.http.Request` "
"object, or an iterable containing any of them."
msgstr ""

#: ../../topics/spiders.rst:373
msgid "Crawling rules"
msgstr ""

#: ../../topics/spiders.rst:377
msgid ""
"``link_extractor`` is a :ref:`Link Extractor <topics-link-extractors>` "
"object which defines how links will be extracted from each crawled page. "
"Each produced link will be used to generate a "
":class:`~scrapy.http.Request` object, which will contain the link's text "
"in its ``meta`` dictionary (under the ``link_text`` key). If omitted, a "
"default link extractor created with no arguments will be used, resulting "
"in all links being extracted."
msgstr ""

#: ../../topics/spiders.rst:384
msgid ""
"``callback`` is a callable or a string (in which case a method from the "
"spider object with that name will be used) to be called for each link "
"extracted with the specified link extractor. This callback receives a "
":class:`~scrapy.http.Response` as its first argument and must return "
"either a single instance or an iterable of :class:`~scrapy.item.Item`, "
"``dict`` and/or :class:`~scrapy.http.Request` objects (or any subclass of"
" them). As mentioned above, the received :class:`~scrapy.http.Response` "
"object will contain the text of the link that produced the "
":class:`~scrapy.http.Request` in its ``meta`` dictionary (under the "
"``link_text`` key)"
msgstr ""

#: ../../topics/spiders.rst:393
msgid ""
"When writing crawl spider rules, avoid using ``parse`` as callback, since"
" the :class:`CrawlSpider` uses the ``parse`` method itself to implement "
"its logic. So if you override the ``parse`` method, the crawl spider will"
" no longer work."
msgstr ""

#: ../../topics/spiders.rst:398
msgid ""
"``cb_kwargs`` is a dict containing the keyword arguments to be passed to "
"the callback function."
msgstr ""

#: ../../topics/spiders.rst:401
msgid ""
"``follow`` is a boolean which specifies if links should be followed from "
"each response extracted with this rule. If ``callback`` is None "
"``follow`` defaults to ``True``, otherwise it defaults to ``False``."
msgstr ""

#: ../../topics/spiders.rst:405
msgid ""
"``process_links`` is a callable, or a string (in which case a method from"
" the spider object with that name will be used) which will be called for "
"each list of links extracted from each response using the specified "
"``link_extractor``. This is mainly used for filtering purposes."
msgstr ""

#: ../../topics/spiders.rst:410
msgid ""
"``process_request`` is a callable (or a string, in which case a method "
"from the spider object with that name will be used) which will be called "
"for every :class:`~scrapy.http.Request` extracted by this rule. This "
"callable should take said request as first argument and the "
":class:`~scrapy.http.Response` from which the request originated as "
"second argument. It must return a ``Request`` object or ``None`` (to "
"filter out the request)."
msgstr ""

#: ../../topics/spiders.rst:417
msgid ""
"``errback`` is a callable or a string (in which case a method from the "
"spider object with that name will be used) to be called if any exception "
"is raised while processing a request generated by the rule. It receives a"
" :class:`Twisted Failure <twisted.python.failure.Failure>` instance as "
"first parameter."
msgstr ""

#: ../../topics/spiders.rst:423
msgid "The *errback* parameter."
msgstr ""

#: ../../topics/spiders.rst:427
msgid "CrawlSpider example"
msgstr ""

#: ../../topics/spiders.rst:429
msgid "Let's now take a look at an example CrawlSpider with rules::"
msgstr ""

#: ../../topics/spiders.rst:459
msgid ""
"This spider would start crawling example.com's home page, collecting "
"category links, and item links, parsing the latter with the "
"``parse_item`` method. For each item response, some data will be "
"extracted from the HTML using XPath, and an :class:`~scrapy.item.Item` "
"will be filled with it."
msgstr ""

#: ../../topics/spiders.rst:465
msgid "XMLFeedSpider"
msgstr ""

#: ../../topics/spiders.rst:469
msgid ""
"XMLFeedSpider is designed for parsing XML feeds by iterating through them"
" by a certain node name.  The iterator can be chosen from: ``iternodes``,"
" ``xml``, and ``html``.  It's recommended to use the ``iternodes`` "
"iterator for performance reasons, since the ``xml`` and ``html`` "
"iterators generate the whole DOM at once in order to parse it.  However, "
"using ``html`` as the iterator may be useful when parsing XML with bad "
"markup."
msgstr ""

#: ../../topics/spiders.rst:476
msgid ""
"To set the iterator and the tag name, you must define the following class"
" attributes:"
msgstr ""

#: ../../topics/spiders.rst:481
msgid "A string which defines the iterator to use. It can be either:"
msgstr ""

#: ../../topics/spiders.rst:483
msgid "``'iternodes'`` - a fast iterator based on regular expressions"
msgstr ""

#: ../../topics/spiders.rst:485
msgid ""
"``'html'`` - an iterator which uses :class:`~scrapy.selector.Selector`. "
"Keep in mind this uses DOM parsing and must load all DOM in memory which "
"could be a problem for big feeds"
msgstr ""

#: ../../topics/spiders.rst:489
msgid ""
"``'xml'`` - an iterator which uses :class:`~scrapy.selector.Selector`. "
"Keep in mind this uses DOM parsing and must load all DOM in memory which "
"could be a problem for big feeds"
msgstr ""

#: ../../topics/spiders.rst:493
msgid "It defaults to: ``'iternodes'``."
msgstr ""

#: ../../topics/spiders.rst:497
msgid "A string with the name of the node (or element) to iterate in. Example::"
msgstr ""

#: ../../topics/spiders.rst:503
msgid ""
"A list of ``(prefix, uri)`` tuples which define the namespaces available "
"in that document that will be processed with this spider. The ``prefix`` "
"and ``uri`` will be used to automatically register namespaces using the "
":meth:`~scrapy.selector.Selector.register_namespace` method."
msgstr ""

#: ../../topics/spiders.rst:509
msgid ""
"You can then specify nodes with namespaces in the :attr:`itertag` "
"attribute."
msgstr ""

#: ../../topics/spiders.rst:512
msgid "Example::"
msgstr ""

#: ../../topics/spiders.rst:520
msgid ""
"Apart from these new attributes, this spider has the following "
"overrideable methods too:"
msgstr ""

#: ../../topics/spiders.rst:525
msgid ""
"A method that receives the response as soon as it arrives from the spider"
" middleware, before the spider starts parsing it. It can be used to "
"modify the response body before parsing it. This method receives a "
"response and also returns a response (it could be the same or another "
"one)."
msgstr ""

#: ../../topics/spiders.rst:532
msgid ""
"This method is called for the nodes matching the provided tag name "
"(``itertag``).  Receives the response and an "
":class:`~scrapy.selector.Selector` for each node.  Overriding this method"
" is mandatory. Otherwise, you spider won't work.  This method must return"
" either a :class:`~scrapy.item.Item` object, a "
":class:`~scrapy.http.Request` object, or an iterable containing any of "
"them."
msgstr ""

#: ../../topics/spiders.rst:542
msgid ""
"This method is called for each result (item or request) returned by the "
"spider, and it's intended to perform any last time processing required "
"before returning the results to the framework core, for example setting "
"the item IDs. It receives a list of results and the response which "
"originated those results. It must return a list of results (Items or "
"Requests)."
msgstr ""

#: ../../topics/spiders.rst:550
msgid "XMLFeedSpider example"
msgstr ""

#: ../../topics/spiders.rst:552
msgid "These spiders are pretty easy to use, let's have a look at one example::"
msgstr ""

#: ../../topics/spiders.rst:573
msgid ""
"Basically what we did up there was to create a spider that downloads a "
"feed from the given ``start_urls``, and then iterates through each of its"
" ``item`` tags, prints them out, and stores some random data in an "
":class:`~scrapy.item.Item`."
msgstr ""

#: ../../topics/spiders.rst:578
msgid "CSVFeedSpider"
msgstr ""

#: ../../topics/spiders.rst:582
msgid ""
"This spider is very similar to the XMLFeedSpider, except that it iterates"
" over rows, instead of nodes. The method that gets called in each "
"iteration is :meth:`parse_row`."
msgstr ""

#: ../../topics/spiders.rst:588
msgid ""
"A string with the separator character for each field in the CSV file "
"Defaults to ``','`` (comma)."
msgstr ""

#: ../../topics/spiders.rst:593
msgid ""
"A string with the enclosure character for each field in the CSV file "
"Defaults to ``'\"'`` (quotation mark)."
msgstr ""

#: ../../topics/spiders.rst:598
msgid "A list of the column names in the CSV file."
msgstr ""

#: ../../topics/spiders.rst:602
msgid ""
"Receives a response and a dict (representing each row) with a key for "
"each provided (or detected) header of the CSV file.  This spider also "
"gives the opportunity to override ``adapt_response`` and "
"``process_results`` methods for pre- and post-processing purposes."
msgstr ""

#: ../../topics/spiders.rst:608
msgid "CSVFeedSpider example"
msgstr ""

#: ../../topics/spiders.rst:610
msgid ""
"Let's see an example similar to the previous one, but using a "
":class:`CSVFeedSpider`::"
msgstr ""

#: ../../topics/spiders.rst:635
msgid "SitemapSpider"
msgstr ""

#: ../../topics/spiders.rst:639
msgid ""
"SitemapSpider allows you to crawl a site by discovering the URLs using "
"`Sitemaps`_."
msgstr ""

#: ../../topics/spiders.rst:642
msgid ""
"It supports nested sitemaps and discovering sitemap urls from "
"`robots.txt`_."
msgstr ""

#: ../../topics/spiders.rst:647
msgid "A list of urls pointing to the sitemaps whose urls you want to crawl."
msgstr ""

#: ../../topics/spiders.rst:649
msgid ""
"You can also point to a `robots.txt`_ and it will be parsed to extract "
"sitemap urls from it."
msgstr ""

#: ../../topics/spiders.rst:654
msgid "A list of tuples ``(regex, callback)`` where:"
msgstr ""

#: ../../topics/spiders.rst:656
msgid ""
"``regex`` is a regular expression to match urls extracted from sitemaps. "
"``regex`` can be either a str or a compiled regex object."
msgstr ""

#: ../../topics/spiders.rst:659
msgid ""
"callback is the callback to use for processing the urls that match the "
"regular expression. ``callback`` can be a string (indicating the name of "
"a spider method) or a callable."
msgstr ""

#: ../../topics/spiders.rst:663 ../../topics/spiders.rst:687
#: ../../topics/spiders.rst:705
msgid "For example::"
msgstr ""

#: ../../topics/spiders.rst:667
msgid ""
"Rules are applied in order, and only the first one that matches will be "
"used."
msgstr ""

#: ../../topics/spiders.rst:670
msgid ""
"If you omit this attribute, all urls found in sitemaps will be processed "
"with the ``parse`` callback."
msgstr ""

#: ../../topics/spiders.rst:675
msgid ""
"A list of regexes of sitemap that should be followed. This is only for "
"sites that use `Sitemap index files`_ that point to other sitemap files."
msgstr ""

#: ../../topics/spiders.rst:679
msgid "By default, all sitemaps are followed."
msgstr ""

#: ../../topics/spiders.rst:683
msgid ""
"Specifies if alternate links for one ``url`` should be followed. These "
"are links for the same website in another language passed within the same"
" ``url`` block."
msgstr ""

#: ../../topics/spiders.rst:694
msgid ""
"With ``sitemap_alternate_links`` set, this would retrieve both URLs. With"
" ``sitemap_alternate_links`` disabled, only ``http://example.com/`` would"
" be retrieved."
msgstr ""

#: ../../topics/spiders.rst:698
msgid "Default is ``sitemap_alternate_links`` disabled."
msgstr ""

#: ../../topics/spiders.rst:702
msgid ""
"This is a filter function that could be overridden to select sitemap "
"entries based on their attributes."
msgstr ""

#: ../../topics/spiders.rst:712
msgid ""
"We can define a ``sitemap_filter`` function to filter ``entries`` by "
"date::"
msgstr ""

#: ../../topics/spiders.rst:728
msgid ""
"This would retrieve only ``entries`` modified on 2005 and the following "
"years."
msgstr ""

#: ../../topics/spiders.rst:731
msgid ""
"Entries are dict objects extracted from the sitemap document. Usually, "
"the key is the tag name and the value is the text inside it."
msgstr ""

#: ../../topics/spiders.rst:734
msgid "It's important to notice that:"
msgstr ""

#: ../../topics/spiders.rst:736
msgid "as the loc attribute is required, entries without this tag are discarded"
msgstr ""

#: ../../topics/spiders.rst:737
msgid ""
"alternate links are stored in a list with the key ``alternate`` (see "
"``sitemap_alternate_links``)"
msgstr ""

#: ../../topics/spiders.rst:739
msgid ""
"namespaces are removed, so lxml tags named as ``{namespace}tagname`` "
"become only ``tagname``"
msgstr ""

#: ../../topics/spiders.rst:741
msgid ""
"If you omit this method, all entries found in sitemaps will be processed,"
" observing other attributes and their settings."
msgstr ""

#: ../../topics/spiders.rst:746
msgid "SitemapSpider examples"
msgstr ""

#: ../../topics/spiders.rst:748
msgid ""
"Simplest example: process all urls discovered through sitemaps using the "
"``parse`` callback::"
msgstr ""

#: ../../topics/spiders.rst:759
msgid ""
"Process some urls with certain callback and other urls with a different "
"callback::"
msgstr ""

#: ../../topics/spiders.rst:777
msgid ""
"Follow sitemaps defined in the `robots.txt`_ file and only follow "
"sitemaps whose url contains ``/sitemap_shop``::"
msgstr ""

#: ../../topics/spiders.rst:792
msgid "Combine SitemapSpider with other sources of urls::"
msgstr ""

