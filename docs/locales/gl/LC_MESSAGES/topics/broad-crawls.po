# SOME DESCRIPTIVE TITLE.
# Copyright (C) 2008–2020, Scrapy developers
# This file is distributed under the same license as the Scrapy package.
# FIRST AUTHOR <EMAIL@ADDRESS>, 2020.
#
#, fuzzy
msgid ""
msgstr ""
"Project-Id-Version: Scrapy 2.0\n"
"Report-Msgid-Bugs-To: \n"
"POT-Creation-Date: 2020-03-03 20:13+0100\n"
"PO-Revision-Date: YEAR-MO-DA HO:MI+ZONE\n"
"Last-Translator: FULL NAME <EMAIL@ADDRESS>\n"
"Language-Team: LANGUAGE <LL@li.org>\n"
"MIME-Version: 1.0\n"
"Content-Type: text/plain; charset=utf-8\n"
"Content-Transfer-Encoding: 8bit\n"
"Generated-By: Babel 2.8.0\n"

#: ../../topics/broad-crawls.rst:5
msgid "Broad Crawls"
msgstr ""

#: ../../topics/broad-crawls.rst:7
msgid ""
"Scrapy defaults are optimized for crawling specific sites. These sites "
"are often handled by a single Scrapy spider, although this is not "
"necessary or required (for example, there are generic spiders that handle"
" any given site thrown at them)."
msgstr ""

#: ../../topics/broad-crawls.rst:12
msgid ""
"In addition to this \"focused crawl\", there is another common type of "
"crawling which covers a large (potentially unlimited) number of domains, "
"and is only limited by time or other arbitrary constraint, rather than "
"stopping when the domain was crawled to completion or when there are no "
"more requests to perform. These are called \"broad crawls\" and is the "
"typical crawlers employed by search engines."
msgstr ""

#: ../../topics/broad-crawls.rst:19
msgid "These are some common properties often found in broad crawls:"
msgstr ""

#: ../../topics/broad-crawls.rst:21
msgid ""
"they crawl many domains (often, unbounded) instead of a specific set of "
"sites"
msgstr ""

#: ../../topics/broad-crawls.rst:23
msgid ""
"they don't necessarily crawl domains to completion, because it would be "
"impractical (or impossible) to do so, and instead limit the crawl by time"
" or number of pages crawled"
msgstr ""

#: ../../topics/broad-crawls.rst:27
msgid ""
"they are simpler in logic (as opposed to very complex spiders with many "
"extraction rules) because data is often post-processed in a separate "
"stage"
msgstr ""

#: ../../topics/broad-crawls.rst:30
msgid ""
"they crawl many domains concurrently, which allows them to achieve faster"
" crawl speeds by not being limited by any particular site constraint "
"(each site is crawled slowly to respect politeness, but many sites are "
"crawled in parallel)"
msgstr ""

#: ../../topics/broad-crawls.rst:35
msgid ""
"As said above, Scrapy default settings are optimized for focused crawls, "
"not broad crawls. However, due to its asynchronous architecture, Scrapy "
"is very well suited for performing fast broad crawls. This page "
"summarizes some things you need to keep in mind when using Scrapy for "
"doing broad crawls, along with concrete suggestions of Scrapy settings to"
" tune in order to achieve an efficient broad crawl."
msgstr ""

#: ../../topics/broad-crawls.rst:45
msgid "Use the right :setting:`SCHEDULER_PRIORITY_QUEUE`"
msgstr ""

#: ../../topics/broad-crawls.rst:47
msgid ""
"Scrapy’s default scheduler priority queue is "
"``'scrapy.pqueues.ScrapyPriorityQueue'``. It works best during single-"
"domain crawl. It does not work well with crawling many different domains "
"in parallel"
msgstr ""

#: ../../topics/broad-crawls.rst:51
msgid "To apply the recommended priority queue use::"
msgstr ""

#: ../../topics/broad-crawls.rst:58
msgid "Increase concurrency"
msgstr ""

#: ../../topics/broad-crawls.rst:60
msgid ""
"Concurrency is the number of requests that are processed in parallel. "
"There is a global limit (:setting:`CONCURRENT_REQUESTS`) and an "
"additional limit that can be set either per domain "
"(:setting:`CONCURRENT_REQUESTS_PER_DOMAIN`) or per IP "
"(:setting:`CONCURRENT_REQUESTS_PER_IP`)."
msgstr ""

#: ../../topics/broad-crawls.rst:65
msgid ""
"The scheduler priority queue :ref:`recommended for broad crawls <broad-"
"crawls-scheduler-priority-queue>` does not support "
":setting:`CONCURRENT_REQUESTS_PER_IP`."
msgstr ""

#: ../../topics/broad-crawls.rst:69
msgid ""
"The default global concurrency limit in Scrapy is not suitable for "
"crawling many different domains in parallel, so you will want to increase"
" it. How much to increase it will depend on how much CPU and memory you "
"crawler will have available."
msgstr ""

#: ../../topics/broad-crawls.rst:74
msgid "A good starting point is ``100``::"
msgstr ""

#: ../../topics/broad-crawls.rst:78
msgid ""
"But the best way to find out is by doing some trials and identifying at "
"what concurrency your Scrapy process gets CPU bounded. For optimum "
"performance, you should pick a concurrency where CPU usage is at 80-90%."
msgstr ""

#: ../../topics/broad-crawls.rst:82
msgid ""
"Increasing concurrency also increases memory usage. If memory usage is a "
"concern, you might need to lower your global concurrency limit "
"accordingly."
msgstr ""

#: ../../topics/broad-crawls.rst:87
msgid "Increase Twisted IO thread pool maximum size"
msgstr ""

#: ../../topics/broad-crawls.rst:89
msgid ""
"Currently Scrapy does DNS resolution in a blocking way with usage of "
"thread pool. With higher concurrency levels the crawling could be slow or"
" even fail hitting DNS resolver timeouts. Possible solution to increase "
"the number of threads handling DNS queries. The DNS queue will be "
"processed faster speeding up establishing of connection and crawling "
"overall."
msgstr ""

#: ../../topics/broad-crawls.rst:95
msgid "To increase maximum thread pool size use::"
msgstr ""

#: ../../topics/broad-crawls.rst:100
msgid "Setup your own DNS"
msgstr ""

#: ../../topics/broad-crawls.rst:102
msgid ""
"If you have multiple crawling processes and single central DNS, it can "
"act like DoS attack on the DNS server resulting to slow down of entire "
"network or even blocking your machines. To avoid this setup your own DNS "
"server with local cache and upstream to some large DNS like OpenDNS or "
"Verizon."
msgstr ""

#: ../../topics/broad-crawls.rst:108
msgid "Reduce log level"
msgstr ""

#: ../../topics/broad-crawls.rst:110
msgid ""
"When doing broad crawls you are often only interested in the crawl rates "
"you get and any errors found. These stats are reported by Scrapy when "
"using the ``INFO`` log level. In order to save CPU (and log storage "
"requirements) you should not use ``DEBUG`` log level when preforming "
"large broad crawls in production. Using ``DEBUG`` level when developing "
"your (broad) crawler may be fine though."
msgstr ""

#: ../../topics/broad-crawls.rst:117
msgid "To set the log level use::"
msgstr ""

#: ../../topics/broad-crawls.rst:122
msgid "Disable cookies"
msgstr ""

#: ../../topics/broad-crawls.rst:124
msgid ""
"Disable cookies unless you *really* need. Cookies are often not needed "
"when doing broad crawls (search engine crawlers ignore them), and they "
"improve performance by saving some CPU cycles and reducing the memory "
"footprint of your Scrapy crawler."
msgstr ""

#: ../../topics/broad-crawls.rst:129
msgid "To disable cookies use::"
msgstr ""

#: ../../topics/broad-crawls.rst:134
msgid "Disable retries"
msgstr ""

#: ../../topics/broad-crawls.rst:136
msgid ""
"Retrying failed HTTP requests can slow down the crawls substantially, "
"specially when sites causes are very slow (or fail) to respond, thus "
"causing a timeout error which gets retried many times, unnecessarily, "
"preventing crawler capacity to be reused for other domains."
msgstr ""

#: ../../topics/broad-crawls.rst:141
msgid "To disable retries use::"
msgstr ""

#: ../../topics/broad-crawls.rst:146
msgid "Reduce download timeout"
msgstr ""

#: ../../topics/broad-crawls.rst:148
msgid ""
"Unless you are crawling from a very slow connection (which shouldn't be "
"the case for broad crawls) reduce the download timeout so that stuck "
"requests are discarded quickly and free up capacity to process the next "
"ones."
msgstr ""

#: ../../topics/broad-crawls.rst:152
msgid "To reduce the download timeout use::"
msgstr ""

#: ../../topics/broad-crawls.rst:157
msgid "Disable redirects"
msgstr ""

#: ../../topics/broad-crawls.rst:159
msgid ""
"Consider disabling redirects, unless you are interested in following "
"them. When doing broad crawls it's common to save redirects and resolve "
"them when revisiting the site at a later crawl. This also help to keep "
"the number of request constant per crawl batch, otherwise redirect loops "
"may cause the crawler to dedicate too many resources on any specific "
"domain."
msgstr ""

#: ../../topics/broad-crawls.rst:165
msgid "To disable redirects use::"
msgstr ""

#: ../../topics/broad-crawls.rst:170
msgid "Enable crawling of \"Ajax Crawlable Pages\""
msgstr ""

#: ../../topics/broad-crawls.rst:172
msgid ""
"Some pages (up to 1%, based on empirical data from year 2013) declare "
"themselves as `ajax crawlable`_. This means they provide plain HTML "
"version of content that is usually available only via AJAX. Pages can "
"indicate it in two ways:"
msgstr ""

#: ../../topics/broad-crawls.rst:177
msgid "by using ``#!`` in URL - this is the default way;"
msgstr ""

#: ../../topics/broad-crawls.rst:178
msgid ""
"by using a special meta tag - this way is used on \"main\", \"index\" "
"website pages."
msgstr ""

#: ../../topics/broad-crawls.rst:181
msgid ""
"Scrapy handles (1) automatically; to handle (2) enable "
":ref:`AjaxCrawlMiddleware <ajaxcrawl-middleware>`::"
msgstr ""

#: ../../topics/broad-crawls.rst:186
msgid ""
"When doing broad crawls it's common to crawl a lot of \"index\" web "
"pages; AjaxCrawlMiddleware helps to crawl them correctly. It is turned "
"OFF by default because it has some performance overhead, and enabling it "
"for focused crawls doesn't make much sense."
msgstr ""

#: ../../topics/broad-crawls.rst:196
msgid "Crawl in BFO order"
msgstr ""

#: ../../topics/broad-crawls.rst:198
msgid ":ref:`Scrapy crawls in DFO order by default <faq-bfo-dfo>`."
msgstr ""

#: ../../topics/broad-crawls.rst:200
msgid ""
"In broad crawls, however, page crawling tends to be faster than page "
"processing. As a result, unprocessed early requests stay in memory until "
"the final depth is reached, which can significantly increase memory "
"usage."
msgstr ""

#: ../../topics/broad-crawls.rst:204
msgid ":ref:`Crawl in BFO order <faq-bfo-dfo>` instead to save memory."
msgstr ""

#: ../../topics/broad-crawls.rst:208
msgid "Be mindful of memory leaks"
msgstr ""

#: ../../topics/broad-crawls.rst:210
msgid ""
"If your broad crawl shows a high memory usage, in addition to "
":ref:`crawling in BFO order <broad-crawls-bfo>` and :ref:`lowering "
"concurrency <broad-crawls-concurrency>` you should :ref:`debug your "
"memory leaks <topics-leaks>`."
msgstr ""

#: ../../topics/broad-crawls.rst:217
msgid "Install a specific Twisted reactor"
msgstr ""

#: ../../topics/broad-crawls.rst:219
msgid ""
"If the crawl is exceeding the system's capabilities, you might want to "
"try installing a specific Twisted reactor, via the "
":setting:`TWISTED_REACTOR` setting."
msgstr ""

