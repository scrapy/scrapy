# This spider was autogenerated by `scrapy genspider` command
#
# For more information on how to write Scrapy spiders and how to use Selectors,
# see http://scrapy.readthedocs.org/en/latest/topics/spiders.html
# and http://scrapy.readthedocs.org/en/latest/topics/selectors.html
#
from scrapy.spider import Spider

# You may find these imports quite handy
#from scrapy.selector import Selector
#from scrapy.http import Request
#import urlparse

# also import your Item classes at some point
#from myproject.items import MyItem, MyProduct


class $classname(Spider):
    name = "$name"

    # Requests made to a domain outside this list will be filtered.
    # To allow all domains, leave emtpy or comment it (Warning: not recommended!)
    allowed_domains = ["$domain"]

    # initial requests are generated using this sequence of URLs
    start_urls = (
        'http://www.$domain/',
        )

    # the "parse" method is a Spider's default callback.
    # Here, it receives responses for URLs in the start_urls tuple above
    def parse(self, response):

        # remove this eventually
        pass

        # --------------------------------------------------------------
        # suppose you need to extract all H2 headings:
        # you first instantiate a Selector with the received response:
        # --------------------------------------------------------------
        #selector = Selector(Response)

        # --------------------------------------------------------------
        # then you apply some XPath or CSS selector:
        # --------------------------------------------------------------
        #headings = selector.xpath('//h2')

        # --------------------------------------------------------------
        # suppose you're interested in the text content of each heading:
        # --------------------------------------------------------------
        #for heading in headings:
        #    item = MyItem()
        #    item["value"] = heading.xpath('./text()').extract()[0]
        #    yield item

        # --------------------------------------------------------------
        # If you want to launch requests to fetch other pages,
        # you can yield Requests:
        # --------------------------------------------------------------
        #for link_href in selector.css('a.detail::attr(href)').extract()
        #    yield Request(url=urlparse.urljoin(response.url, link_href),
        #                  callback=self.parse_product)

    #def parse_product(self, response):
    #    selector = Selector(Response)
    #
    #    #--------------------------------------------------------------
    #    # you can either return a list of items or yield item one by one
    #    #--------------------------------------------------------------
    #    product_items = []
    #    for product in selector.css('div.product'):
    #        product_item = MyProduct()
    #        product_item["title"] = product.xpath('./span[@class="title"]/text()').extract()[0]
    #        product_item["image_urls"] = product.xpath('.//img/@src').extract()
    #        product_items.append(product_item)
    #    return product_items
